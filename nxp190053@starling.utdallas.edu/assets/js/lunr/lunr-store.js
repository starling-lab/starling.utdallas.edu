var store = [{
        "title": "Getting Started",
        "excerpt":"Background   Prerequisites:     programming experience is strongly recommended   basic shell experience (moving between folders, viewing files)   an Apple/Linux machine (Windows coming soon)   At its core, this is a brief “Inductive Logic Programming” (ILP) tutorial. However, we are learning probabilistic (weighted) clauses. Following any standard logic learner, we assume that the data is structured i.e., consists of objects and relationships between the objects. The goal is to induce a theory (regression trees with logical variables in them) from a set of examples (positive and negative as separate files), facts (values of the attributes and/or relationships) and some background knowledge, all in predicate logic format. More on the background knowledge later. Like a typical ILP learner, ours is a discriminative learner and hence requires both positive and negative examples.   Like many things in computer science, usually the best way to learn something is by doing it.   Download the data, a copy of the jar file, and the AUC jar and follow along!      unzip Father.zip   mv v1.0.jar Father/BoostSRL.jar   mv auc.jar Father/auc.jar   cd Father   Table of Contents     Setup as Tables      Suppose we have data about some people and some of their relationships. (we’ll refer to this information as “facts,” more on that later)                  Name       Sex       Child       Sibling                       James Potter       Male       [Harry Potter]       -                 Lily Potter       Female       [Harry Potter]       -                 Harry Potter       Male       -       -                 Arthur Weasley       Male       [Ron, Fred, Ginny]       -                 Molly Weasley       Female       [Ron, Fred, Ginny]       -                 Ron       Male       -       [Fred, Ginny]                 Fred       Male       -       [Ron, Ginny]                 Ginny       Female       -       [Ron, Fred]           Note that the Child and Sibling are multi-arity relations and hence can have sets of values instead of a single value.      Assume that the goal is to learn father(X)?. This means that the goal is to learn logic rules for the father of any given domain object X (persons in this case), given the information in the tables, i.e. given that you know the names of the domain objects, their sex, their children and their siblings.   Table of Contents     From Tables to First-order Predicate Logic Notation   Once we have the high-level idea of what these relationships look like, the next step is to convert this into predicate logic format. This is the standard format for most Prolog-based systems but based on feedback from the users of the previous version, we provide a more detailed tutorial on its construction. However, we recommend that a script be used for this automatic construction.   A key difference to standard Prolog systems is that our system makes a restrictive assumption - no function symbols. We only handle predicate (boolean) symbols for ease of learning. However, note that a n-valued function can be represented using n different binary predicates with a mutual exclusive constraint.   A few things to consider during the transition:     ‘Name’ is an identifier.   ‘Sex’ is assumed to be either male or female, so we can simplify by making it a True/False value.   childof, siblingof, and fatherof are binary relations, i.e., they encode the relation between two people (e.g. childof(mrbennet,jane), denotes that Jane is the parent of Mr. Bennet)   The target we want to learn is father(x,y). To learn this rule, the algorithm learns a decision tree that most effectively splits the positive and negative examples. This example is fairly small, and hence a few trees should suffice. However, for more complex problems we need more trees to learn a robust model. In most of our experiments, we use at least 20 trees.      Positive Examples      father(harrypotter,jamespotter).   father(ginnyweasley,arthurweasley).   father(ronweasley,arthurweasley).   father(fredweasley,arthurweasley).   ...      Negative Examples      father(harrypotter,mollyweasley).   father(georgeweasley,jamespotter).   father(harrypotter,arthurweasley).   father(harrypotter,lilypotter).   father(ginnyweasley,harrypotter).   father(mollyweasley,arthurweasley).   father(fredweasley,georgeweasley).   father(georgeweasley,fredweasley).   father(harrypotter,ronweasley).   father(georgeweasley,harrypotter).   father(mollyweasley,lilypotter).   ...      Facts      male(jamespotter).   male(harrypotter).   male(arthurweasley).   male(ronweasley).   male(fredweasley).   male(georgeweasley).   siblingof(ronweasley,fredweasley).   siblingof(ronweasley,georgeweasley).   siblingof(ronweasley,ginnyweasley).   siblingof(fredweasley,ronweasley).   siblingof(fredweasley,georgeweasley).   siblingof(fredweasley,ginnyweasley).   siblingof(georgeweasley,ronweasley).   siblingof(georgeweasley,fredweasley).   siblingof(georgeweasley,ginnyweasley).   siblingof(ginnyweasley,ronweasley).   siblingof(ginnyweasley,fredweasley).   siblingof(ginnyweasley,georgeweasley).   childof(jamespotter,harrypotter).   childof(lilypotter,harrypotter).   childof(arthurweasley,ronweasley).   childof(mollyweasley,ronweasley).   childof(arthurweasley,fredweasley).   childof(mollyweasley,fredweasley).   childof(arthurweasley,georgeweasley).   childof(mollyweasley,georgeweasley).   childof(arthurweasley,ginnyweasley).   childof(mollyweasley,ginnyweasley).   ...   Table of Contents     Training a Model   There’s one more piece we need: the background.      background      // Parameters   setParam: maxTreeDepth=3.   setParam: nodeSize=1.   setParam: numOfClauses=8.   // Modes   mode: male(+name).   mode: childof(+name,+name).   mode: siblingof(+name,-name).   mode: father(+name,+name).   Enter the directory and run the jar file to train the model.   java -jar BoostSRL.jar -l -combine -train train/ -target father -trees 10   First it finds childof(B,A):   “fathers are parents”      Next, if the person is male, they are also more likely to be a father:   “fathers are male”      Each tree builds on the error of the previous tree. By combining the trees the model learned, we can see the decision the model finds most accurate.      This tree shows that a male father E is the parent of child D. Alternatively, the person is also the father to his child’s siblings.   Table of Contents     Testing a Model   Technically we’re querying a model for an answer here, but we’ll call it the testing phase.      We have a new set of data, and we want to know about the father(x) relationship:                  Name       Sex       Child       Sibling       Father                       Mr. Bennet       Male       [Elizabeth, Jane]       -       ?                 Mrs. Bennet       Female       [Elizabeth, Jane]       -       ?                 Elizabeth       Female       -       [Jane]       ?                 Jane       Female       [Elizabeth]       -       ?                 Mr. Lucas       Male       [Charlotte]       -       ?                 Mrs. Lucas       Female       [Charlotte]       -       ?                 Charlotte       Female       -       -       ?              Outline the facts and “positive examples” to test:      Facts      male(mrbennet).   male(mrlucas).   male(darcy).   childof(mrbennet,elizabeth).   childof(mrsbennet,elizabeth).   childof(mrbennet,jane).   childof(mrsbennet,jane).   childof(mrlucas,charlotte).   childof(mrslucas,charlotte).   siblingof(jane,elizabeth).   siblingof(elizabeth,jane).      Positive Examples    In this case, we’ll only add the examples we want to test.     father(elizabeth,mrbennet).   father(jane,mrbennet).   father(charlotte,mrlucas).   father(charlotte,mrsbennet).   father(jane,mrlucas).   father(mrsbennet,mrbennet).   father(jane,elizabeth).      Run the testing command from the Father directory:   java -jar BoostSRL.jar -i -model train/models -test test/ -target father -aucJarPath . -trees 10      Enter the test directory and look at the results:      cd test   less results_father.db   The results will look something like this:     father(elizabeth, mrbennet) 0.8469089683281722   father(jane, mrbennet) 0.8469089683281722   father(charlotte, mrlucas) 0.8469089683281722   father(charlotte, mrsbennet) 0.03200025741806517   father(jane, mrlucas) 0.06111079239086714   father(mrsbennet, mrbennet) 0.06111079239086714   father(jane, elizabeth) 0.03200025741806517      Results in a nicer format:                  Person A       Person B       Probability of B being the father of A                       Elizabeth       Mr. Bennet       84.691%                 Jane       Mr. Bennet       84.691%                 Charlotte       Mr. Lucas       84.691%                 Charlotte       Mrs. Bennet       3.200%                 Jane       Mr. Lucas       6.111%                 Mrs. Bennet       Mr. Bennet       6.111%                 Jane       Elizabeth       3.200%           Table of Contents       ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/getting-started/",
        "teaser": null
      },{
        "title": "File Structure",
        "excerpt":"Basic File Structure   Files that BoostSRL operates on are stored in a folder with three things:      background.txt : Modes   train/ folder :            train_bk.txt : Pointer to the background file.       train_facts.txt : Facts       train_pos.txt : Positive examples       train_neg.txt : Negative examples           test/ folder :            test_bk.txt : Pointer to the background file.       test_facts.txt : Facts       test_pos.txt : Positive examples       test_neg.txt : Negative examples           Example:      File structure for the Cora dataset, notice that the background is called “cora_bk.txt” in this example.   This is okay if train_bk.txt and test_bk.txt both point correctly with: import: \"../cora_bk.txt\".   Table of Contents     FAQ      What does it mean to have positive and negative examples during testing? Isn’t the point of testing that I do not know these labels ahead of time?   Think from a classic machine learning perspective. We divide data into training and test sets, learn from training set, hide the labels on the test set, and try to predict what is hidden. Positive and negative examples during testing are hidden, and the goal is to predict which are positive and which are negative based on the learned model and the facts.   If you want to use the model to perform inference on data you do not know the labels for, add your data to either test_pos.txt or test_neg.txt. The regression values in the results_(target).txt can be roughly interpreted as “What is the probability of this example being true/false?”, respectfully.   Table of Contents     Advanced File Structure   After training/testing, more files and folders will appear. This advanced guide explains what each of them are, including the contents of the models, dotFiles, bRDNs, and WILLtheories directories.   Not all of these will necessarily appear, for example: the CombinedTrees(target).dot only appears when the -combine flag is set.   1 :Data/ 2 :├── background.txt 3 :├── BoostSRL.jar 4 :├── test 5 :│   ├── query_(target).db 6 :│   ├── results_(target).db 7 :│   ├── test_bk.txt 8 :│   ├── test_facts.txt 9 :│   ├── test_infer_dribble.txt 10:│   ├── test_neg.txt 11:│   └── test_pos.txt 12:└── train 13:    ├── models 14:    │   ├── bRDNs 15:    │   │   ├── dotFiles 16:    │   │   │   ├── CombinedTrees(target).dot 17:    │   │   │   ├── rdn.dot 18:    │   │   │   ├── WILLTreeFor_(target)0.dot 19:    │   │   │   ├── ... 20:    │   │   │   └── WILLTreeFor_(target)9.dot 21:    │   │   ├── (target).model 22:    │   │   ├── (target)_testsetStats_pos_neg_Lits1Trees10Skew2.txt 23:    │   │   ├── old_(target).model 24:    │   │   ├── predictions_pos_neg_Lits1Trees10Skew2.csv 25:    │   │   └── Trees 26:    │   │       ├── CombinedTreesTreeFile(target).tree 27:    │   │       ├── (target)Tree0.tree 28:    │   │       ├── ... 29:    │   │       └── (target)Tree9.tree 30:    │   └── WILLtheories 31:    │       ├── (target)_learnedWILLregressionTrees.txt 32:    │       └── old_(target)_learnedWILLregressionTrees.txt 33:    ├── schema.db 34:    ├── train_bk.txt 35:    ├── train_facts.txt 36:    ├── train_gleaner.txt 37:    ├── train_learn_dribble.txt 38:    ├── train_neg.txt 39:    └── train_pos.txt        Data/: Directory that contains the data we train/test on.   background.txt: modes file used to guide the search space.   BoostSRL.jar: If you’re using a jar file, you’ll usually keep it at the root of the data directory.   test/: Directory containing testing data.   query_(target).db:   results_(target).db: Results of running inference (testing) on the data.   test_bk.txt: Pointer to the background.txt   test_facts.txt: Predicates described in the background.txt   test_infer_dribble.txt:   test_neg.txt: Negative testing examples.   test_pos.txt: Positive testing examples.   train/: Directory containing training data.   models/:   bRDNs/:   dotFiles/:   CombinedTrees(target).dot: Combined Tree of the target, results from using the -combine flag.   rdn.dot:   WILLTreeFor_(target)0.dot: First of the boosted trees.   ...: For each tree, there will be an associated file named WILLTreeFor_(target)#.dot   WILLTreeFor_(target)9.dot: Last of the boosted trees, equal to one less than the number of trees learned.   (target).model:   (target)_testsetStats_pos_neg_Lits1Trees10Skew2.txt:   old_(target).model:   predictions_pos_neg_Lits1Trees10Skew2.csv:   Trees/:   Combined TreesTreeFile(target).tree   (target)Tree0.tree:   ...: For each tree, there will be an associated file named (target)Tree#.tree   (target)Tree9.tree:   WILLtheories/:   (target)_learnedWILLregressionTrees.txt:   old_(target)_learnedWILLregressionTrees.txt:   schema.db:   train_bk.txt: Pointer to the background.txt   train_facts.txt: Predicates described in the background.txt   train_gleaner.txt:   train_learn_dribble.txt:   train_neg.txt: Negative training examples.   train_pos.txt: Positive training examples.   Table of Contents     ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/file-structure/",
        "teaser": null
      },{
        "title": "Basic Parameters",
        "excerpt":"Note that this is for learning an RDN. The basic version boosts a single conditional relational probability distribution.   Simple Usage:      java -cp BoostSRL.jar edu.wisc.cs.Boosting.RDN.RunBoostedRDN [Args]   java -jar BoostSRL.jar [Args]   Arguments [Args]:      -l : enable training (learning).   -i : enable testing (inference).   -noBoost : disable Boosting (i.e., learns a single relational regression tree).   -train &lt;Training directory&gt; : Path to the training directory in predicate logic format.   -test &lt;Testing directory&gt; : Path to the testing directory in predicate logic format format.        -model &lt;Model directory&gt; : Path to the directory with the stored models [or where they will be stored].       Default location: “Training directory”/models       -target &lt;target predicates&gt; : Comma separated list of predicates to be learned/inferred.        -trees &lt;Number of trees&gt; : Number of Boosting trees.       Default: 20. Ignored if -noBoost is set.            -step &lt;Step Length&gt; : Default step length for functional gradient.       Default: 1. Ignored if lineSearch is set in advanced parameters.       -modelSuffix &lt;suffix&gt; : All the trees/models are saved with this suffix appended to the file names.   -aucJarPath &lt;path to auc.jar&gt; : If this is not set, AUC values are not computed.        -testNegPosRatio &lt;Negative/Positive ratio&gt; : Ratio of negatives to positive for testing.       Default: 2. Set to -1 to disable sampling.       Table of Contents     Sample Calls:   Try to follow along with what each of these are doing:   From the Getting Started tutorial:           java -jar BoostSRL.jar -l -combine -train train/ -target father -trees 10            java -jar BoostSRL.jar -i -model train/models -test test/ -target father -trees 10 -aucJarPath &lt;Path to auc.jar&gt;       (If auc.jar does not exist in the source code folder, download from http://mark.goadrich.com/programs/AUC/)   From the Boston Housing Dataset (notice the different classpath):      java -cp BoostSRL.jar edu.wisc.cs.will.Boosting.Regression.RunBoostedRegressionTrees -reg -l -train train/ -target medv -trees 20   From the CiteSeer Dataset:      java -jar BoostSRL.jar -l -train train/ -target infield_fauthor,infield_ftitle,infield_fvenue -trees 5   Table of Contents     ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/basic-parameters/",
        "teaser": null
      },{
        "title": "Advanced Parameters",
        "excerpt":"Few Advanced Parameters are set in the background file while few can be included as command line arguments.   Advanced Arguments      -lc : print learning curve wth varying number of trees.   Parameters in Background file   Include the following lines in the background file (folder_bk.txt) to set the parameters:   maxTreeDepth   Set the maximum number of nodes from root to leaf (height) in the tree.   setParam: maxTreeDepth=3.   nodeSize   Set the maximum number of literals in the node. Default value is 2.   setParam: nodeSize=2.   numOfClauses   Set the maximum number of clauses in the tree (i.e. maximum number of leaves). Default value is 100.   setParam: numOfClauses=8.   numOfCycles   Set the maximum number of times the code should loop to learn clauses. Similar to numOfClauses but the counter increases even when no clause is learned. Default value is 100.   setParam: numOfCycles=8.   recursion   Allow reusing the literal from head of the clause in the body of the clause. Default is false.   setParam: recursion=true.   lineSearch   Performs line search for deciding step length for functional gradient instead of using the fixed step length provided as -step in basic parameters. Default value is false.   setParam: lineSearch=true.   loadAllLibraries   Prevent loading of all the existing libraries: arithmeticInLogic, comparisonInLogic, differentInLogic, listsInLogic by setting it to false. Individual libraries can then be loaded using importLibrary parameters. Default value is true.   setParam: loadAllLibraries = false.   loadAllBasicModes   Prevent loading of all the basic modes: modes_arithmeticInLogic, modes_comparisonInLogic, modes_differentInLogic, modes_listsInLogic by setting it to false. This might use a lot of cycles, so use with care. Default value is true   setParam: loadAllBasicModes = false.   minLCTrees   Set the minimum number of trees used for printing learning curves. Used only when -lc is set. Default value is 20   setParam: minLCTrees=5.   incrLCTrees   Set the number of trees to be increased every step while printing learning curve. Used only when -lc is set. Default value is 2   setParam: incrLCTrees=5.     treeDepth   Deprecated. Use maxTreeDepth instead   resampleNegs   Deprecated.   Advanced settings   Warm Start   RDN Boost supports warm start, which allows you to add more trees to an already fitted model. To warm start learning, rename the existing &lt;target_predicate&gt;.model file in the model directory to &lt;target_predicate&gt;.model.ckpt and use the learn command as before.  ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/advanced-parameters/",
        "teaser": null
      },{
        "title": "Basic Modes Guide",
        "excerpt":"Contact us if your particular mode file does not work.   Modes are used to restrict/guide the search space and are a powerful tool in getting relational algorithms such as BoostSRL to work. If your algorithm does not learn anything useful, then the first debug point would be the modes (in the background.txt file).   The modes of our system follow the Aleph modes definitions, the key difference being that we use the form:   mode: predicateName(ModeTypeArg1,...).     By Definition   The modes supported by our system are +, -, and # (following the definitions of Aleph). Variables are associated with one (or more) of them.   Consider the famous ‘trains’ testbed of Michalski:   mode: eastbound(+train). mode: short(+car). ... mode: shape(+car, #shape). mode: has_car(+train, -car).      +v    indicates that variable v must have already been mentioned in the current rule. For instance, let us assume that the goal is to learn if the train is eastbound (i.e., eastbound(train) is the target)). The search algorithm first considers only the predicates that has a modeType +train in them. If no predicates has the modeType +train, the algorithm terminates. In this case, it will add has_car as it has the modeType +train.   -v    indicates that a new variable v can be introduced into the clause (essentially an existential). Continuing the above example, has_car can be added because it has both +train and -car. -car allows for a new variable to be added and +train will allow for the algorithm to consider this predicate. It should be mentioned that if the mode definition had been has_car(+train,+car), it would be ignored by the search. Though train has been declared earlier (in the target), car was not defined earlier and this predicate will be ignored. Therefore, to add a clause has_car(X,Y) -&gt; eastbound(X) it is essential that the car is of type -.   #v    indicates that v is of type constant. This is the simplest of all the modes since the variables are grounded and their specific values are searched over.   Finally, if one observes closely, has_car(X,Y) -&gt; eastbound(X) is not informative in that all the trains will have at least one car. Hence, the algorithm can never learn this clause. To enable learning of this clause, one has to increase the lookahead of the search algorithm. This can be achieved in two ways:           Setting a parameter. nodeSize=2 will allow for two predicates to be considered at the same time. So it is possible to learn has_car(X,Y) ^ big(Y) -&gt; eastbound(X).            Defining a bridger. The bridger predicate will not be scored during search but will serve to introduce new objects in the search space (in this case, a car). This is defined as follows:       bridger: has_car/2.   The number after the / sign indicates the number of parameters of that predicate.   Table of Contents     By Example   Here are basic modes for some of the small datasets, the full modes can be found on their associated page.      Toy Cancer Dataset   useStdLogicVariables: true. setParam: treeDepth=4. setParam: nodeSize=2. setParam: numOfClauses=8. mode: friends(+Person, -Person). mode: friends(-Person, +Person). mode: smokes(+Person). mode: cancer(+Person). bridger: friends/2.      Father Dataset (from the “Getting Started” guide)   //Parameters setParam: maxTreeDepth=3. setParam: nodeSize=1. setParam: numOfClauses=8. //Modes mode: male(+name). mode: childof(+name,+name). mode: siblingof(+name,-name). mode: father(+name,+name).   Table of Contents     ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/basic-modes/",
        "teaser": null
      },{
        "title": "Advanced Modes Guide",
        "excerpt":"For a more advanced tutorial on modes refer to the presentation: Advanced-Modes-Guide  ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/advanced-modes/",
        "teaser": null
      },{
        "title": "Relational Dependency Networks (RDNs)",
        "excerpt":"Relational Dependency Networks (RDNs) are graphical models that extend dependency networks to relational domains where the joint probability distribution over the variables is approximated as a product of conditional distributions. This higher expressivity, however, comes at the expense of of a more complex model-selection problem: an unbounded number of relational abstraction levels might need to be explored.   Whereas current learning approaches for RDNs learn a single probability tree per random variable, RDN-Boost learns a series of relational function-approximation problems using gradient-based boosting In doing so, one can easily induce highly complex features over several iterations and in turn estimate quickly a very expressive model.   By default, if multiple target predicates are provided, the code learns a relational dependency network.   The software provided here can also learn a single relational probability tree with -noBoost flag though the main contribution is the functional gradient boosting of RDNs.   For more details on learning RDN please refer to      Sriraam Natarajan, Tushar Khot, Kristian Kersting, Bernd Gutmann and Jude Shavlik. Gradient-based Boosting for Statistical Relational Learning: The Relational Dependency Network Case, Special issue of Machine Learning Journal (MLJ), Volume 86, Number 1, 25-56, 2012.   ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/rdn-boost/",
        "teaser": null
      },{
        "title": "Markov Logic Networks (MLNs)",
        "excerpt":"Overview   Markov Logic Networks (Richardson and Domingos 2006) are arguably one of the most popular relational probabilistic models. They lifted undirected Markov networks to relational setting. Hence, by design, they are intractable.   In our paper, we convert the problem of learning a MLN to learning a series of RDNs. In essence, this is very similar to our RDN learning with one key difference. In RDNs, when grounding a clause, we use existential semantics i.e., if the clause has at least one grounding, then we treat that clause as true. However, for MLNs, we count the number of groundings of that clause. For more details, please refer to the paper below.   We have also implemented an approximate, fast counting method for MLNs. See the section on Approximate counting to set the appropriate flags.   -mln : Set this flag, if you want to learn MLNs instead of RDNs   -mlnClause : Set this flag, if you want to learn MLNs via clausal representation. If not set, the tree representation will be used.   -numMLNClause : If -mlnclause is set, set the number of clauses learned during each gradient step.   -mlnClauseLen : If -mlnclause is set, set the length of the clauses learned during each gradient step.   Paper:      Tushar Khot, Sriraam Natarajan, Kristian Kersting, Jude Shavlik.Learning Markov Logic Networks via Functional Gradient Boosting. In ICDM 2011.   ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/mln-boost/",
        "teaser": null
      },{
        "title": "Regression",
        "excerpt":"Overview   Regression is a technique in statistics and machine learning that attempts to find the relation between one (or more) dependent variable(s) and an independent variable. Imagine graphing features as points on a coordinate plane, then fitting a line to show how the variables influence one another. The benefit of using relational regression is the ability to easily handle high-dimensional attributes without necessarily projecting them into the same space. Furthermore, we can learn the relationship between examples and control whether they are dependent or independent on one another.   The type of regression implemented here is “Least Square Error Tree Boosting (LSTree Boosting),” which learns a series of boosted regression trees and aims to minimize the error of the previous tree.   Loss function: L(y,F(x)) = (y - F(x))^2      Table of Contents     Preparing Data for Regression   Refer to the Boston Housing Dataset page for more information.      Facts   BoostSRL regression cannot explicitly handle continuous features. In order to input facts into BoostSRL, we need to discretize the continuous features using either precomputes or tools like Weka. For example: we discretize the Boston dataset using equal frequency 10-bins on these features: Crim, Indus, Nox, Rm, Age, Dis, Ptratio, B, and Lstat.   crim(id143,8). zn(id257,90). indus(id124,8). chas(id12,0). nox(id314,1). rm(id131,2). age(id25,7). dis(id198,5). rad(id197,2). tax(id202,348). ptratio(id295,1). b(id50,1). lstat(id150,5). ...(etc.)      Positive Examples   The target predicate medv(exampleid,targetvalue) should be wrapped inside the predicate regressionExample().   For example: the target predicate for the Boston Dataset is medv(exampleid,targetvalue), so the target example will be regressionExample(medv(exampleid),targetvalue).   regressionExample(medv(id60),19.6). regressionExample(medv(id70),20.9). regressionExample(medv(id63),22.2). regressionExample(medv(id207),24.4). regressionExample(medv(id153),15.3). ...(etc.)      Negative Examples   There are no negative examples in regression. Because of this, an empty file named train_neg.txt should be provided to show that there are none.      Table of Contents     Usage Commands           Learning:       java -cp BoostSRL.jar edu.wisc.cs.will.Boosting.Regression.RunBoostedRegressionTrees -reg -l -train train/ -target medv -trees 20            Inference:       java -cp BoostSRL.jar edu.wisc.cs.will.Boosting.Regression.RunBoostedRegressionTrees -i -test test/ -target medv -model train/models/ -trees 20       Table of Contents     ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/regression/",
        "teaser": null
      },{
        "title": "Relational One Class Classification",
        "excerpt":"Parameters      -occ: to enable the one class classification.   -l: to enable learning.   -train: path to train folder.   -target: name of the target predicate.   java -jar boostsrl.jar -occ -l -train /path/to/train/folder -target targetpredicate   Note:   train_neg.txt file needs to be empty  ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/one-class-classification/",
        "teaser": null
      },{
        "title": "Cost-sensitive Statistical Relational Learning",
        "excerpt":"Overview   Cost-sensitive Statistical Relational learning (CSSRL) fulfills the cost-sensitive learning for both standard machine learning domains as well as the statistical relational learning problems.   You may want to consider trying this package if you have one of the following problems:           You have structured data where class-imbalance is a prominent problem for statistical relational learning or standard data sets which are class imbalanced by nature, such as medical diagnosis, text classification, and detection of oil spills or financial fraud.            You have practical concerns on the false prediction costs based on the domain requirements. For example, the medical diagnosis where the cost for false negative prediction is much more than that of the false positive prediction, or recommender systems where the cost for false positive prediction needs to be decreased in order to avoid losing users by sending out numerous inappropriate recommendations.            You need to put different weights on positive and negative examples due to the data properties. For example, when you know there are considerable amount of noisy samples in negative class and you do not want the classifier boundary to be dominated by those outliers.       If you need more details about this approach, please refer the paper on this topic:      Shuo Yang, Tushar Khot, Kristian Kersting, Gautam Kunapuli, Kris Hauser and Sriraam Natarajan, Learning from Imbalanced Data in Relational Domains: A Soft Margin Approach, International Conference on Data Mining (ICDM), 2014.      CSSRL Parameters   CSSRL allows you to incorporate the domain knowledge on different weights of positive samples and negative samples by explicit tuning the trade-off between false positive rate and false negative rate. All you need to do is set the parameters alpha and beta which should be set positive values if harsher penalty is needed for the corresponding class, negative values if more tolerance is needed for the corresponding class and zero if there is no special requirements on that class.           -softm should be set to activate the CSSRL learning.            -alpha should be set to assign the weight on false negative examples.            -beta should be set to assign the weight on false positive examples.       Table of Contents     ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/cost-sensitive-srl/",
        "teaser": null
      },{
        "title": "Learning with Advice",
        "excerpt":"Overview   Knowledge-based Probabilistic Logic Learning (KBPLL) learns a model using training data and expert advice, extending Relational Functional Gradient Boosting (RFGB).   Knowledge and advice are interchangeable in this context, and both are specified by label preferences. Label preferences are a set that are more preferred (and therefore more likely) than another set. A conjunction of literals in logic specifies a set of examples to which the label preferences should apply.   For example:      For the clause:   Patient(x) &amp;&amp; BloodPressure(x, HIGH) &amp;&amp; Cholesterol(x, HIGH)      The preferred label is:   {Heart Attack}      The non-preferred label is:   {Stroke, Cancer}   This advice says that patients with high blood pressure and high cholesterol are more likely to have a heart attack and less likely to have cancer or a stroke.   The key notion behind RFGB is to calculate the current error (gradient) in the model for every example and fit a tree to that error. By adding that tree to the current model, the algorithm is pushed toward correcting its error. In order to incorporate expert knowledge, we introduce an advice gradient that pushes preferred labels toward 1 and non-preferred labels toward 0.   Table of Contents     Advice   Advice preferences are specified in a single file, and each piece of advice is specified in three lines.      The first line contains the preferred label(s), preferred label(s), preferred label(s)   The second line contains the non-preferred label(s), non-preferred label(s)   The final line contains the path to the file that specifies advice clauses. These clauses define the examples where the advice should apply.   The advice file could contain multiple pieces of advice, each read in three line chunks.   Example:   heartattack stroke,cancer C:\\path\\to\\advice\\file.txt   In the binary case, heartattack and !heartattack can be used as the preferred/non-preferred labels.   The file defining the clauses must use the following format:   4.0 target(Arg1,...,ArgN) :- predicate1(...) ^ predicate2(...) ^ ... predicateN(...). -4.0 target(Arg1,...,ArgN) :- !.   The file must define a clause for every possible example.     Clauses lead by a positive value (use 4.0) define examples where the advice will apply.   Clauses lead by a negative value (use -4.0) define examples where the advice will not apply.   The cut (!) is used in the last clause to define all possible examples not covered by the previous clause.   Advice is compiled into a single value for every example. This is written to the “gradients_target.adv_grad” file in the training directory. It is useful to check this file to ensure that there are non-zero values.   Table of Contents     KBPLL Parameters   Two key parameters: -adviceFile and -adviceWt           -adviceFile should be set as the path to the file that contains the advice. Specifying a value for this parameter should ensure that KBPLL runs correctly.            -adviceWt controls the trade-off between the gradient with respect to the training data and the gradient with respect to the advice. 0.5 is the default value, but this parameters only needs to be specified if you want to change it.       Table of Contents     Download   Here is a basic dataset to help with understanding advice and its associated file setup:      Download: Toy-Advice.zip (27.5 KB)      md5sum:     7ad05a6cfa3b9cde55a96669cb94ed15       sha256sum:     df60f5399a41ff8ef38f136366a55e4886d0af8705bfce55b761710a1f3848c7       advice.txt:  ha !ha adviceFile.txt   adviceFile.txt:  -4.0 ha(A) :- chol(A, 4). -4.0 ha(A) :- chol(A, 3). 4.0 ha(A) :- !.   Train with advice using:   java -jar v1-0.jar -l -train train/ -target ha -adviceFile \"advice.txt\" -trees 3   Test with:   java -jar v1-0.jar -i -model train/models/ -test test/ -target ha -trees 3   Table of Contents     References      Odom, P.; Khot, T.; Porter, R.; and Natarajan, S. 2015. Knowledge-based Probabilistic Logic Learning. In AAAI.   Odom, P.; Bangera, V.; Khot, T.; Page, D.; and Natarajan, S. 2015. Extracting Adverse Drug Events from Text using Human Advice. In AIME.   Table of Contents     ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/advice/",
        "teaser": null
      },{
        "title": "Approximate Counting",
        "excerpt":"Overview   Notice: v1.0 of BoostSRL.jar does not include Approximate Counting, but it is available in the source.   MLN-Boost (Khot et al. 2011) requires counting over satisfied groundings of First-Order Logic formulas (could be partially grounded) given the facts/evidence (ground atoms that are true in the world). This counting operation may occur multiple times both for computing probabilities while learning and during inference.   The vanilla implementation of the software performs this counting in a brute force manner via combinatorial search which is #P-complete. Hence, scaling to large evidence is a significant concern. (Das et. al. 2016) proposed an approach (FACT algorithm) to compute these counts in an approximate fashion via graph databases in order to elevate efficiency without sacrificing performance. Approximate Counting has been integrated into the software for scalability to large datasets.   Table of Contents     Parameters   Approximate counting can be enabled for MLN-Boost by adding a flag to the command-line arguments. There are several other minor additional points that one needs to be aware of while running MLN-Boost with approximate counting.      -approxCount   -mln   Example:      java -cp BoostSRL.jar edu.wisc.cs.will.Boosting.MLN.RunBoostedMLN -l -train train/ -target (target) -trees 10 -mln -approxCount   Table of Contents     Limitations   There are certain limitations to using the approximate counting module (as discussed in the original paper).      Arity: Presently, the approximation module supports only unary and binary predicates. If that data has predicates of arity greater than two we advise against using approximate counting.   Delta-size vs. Performance: MLN-Boost works well with approximate counting for data sets of reasonable to large size (&gt; 100 facts). For small datasets it will run without errors but the efficiency gain may not be significant and performance may be worse than the original.   Table of Contents     References      Khot, T.; Natarajan, S.; Kersting, K.; and Shavlik, J. 2011. Learning markov logic networks via functional gradient boosting. In ICDM.   Das, M.; Wu, Y.; Khot, T.; Kersting, K.; and Natarajan, S. 2016. Scaling Lifted Probabilistic Inference and Learning Via Graph Databases. In SIAM International Conference on Data Mining (SDM).   Table of Contents     ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/approximate-counting/",
        "teaser": null
      },{
        "title": "Discretization of Continuous-Valued Attributes",
        "excerpt":"As RDN Boost model accepts discrete values in its input predicate logic form, a new discretization functionality has been added as follows;      In the mode file, user has to specify the predicate and its arguments to discretize along with the number of bins desired. The discretization uses equal frequency distribution. An example from background file is mentioned below:   For the following predicate hdl(+Patient,#Time, #hdlValue).   We want to discretize the 2nd and 3rd argument and into 2 and 3 bins respectively. disc: hdl([2,3],[2,3]).   Another example  For a predicate  a1c(+Patient,+a1cValue).   We want to discretize the 1rst argument and into 4 bins. disc: a1c([2],[4]).   Table of Contents     ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/discretization/",
        "teaser": null
      },{
        "title": "Lifted Relational Random Walks",
        "excerpt":"Overview   “Lifted Relational Random Walks” has been integrated into BoostSRL to obtain random walks in relational domains. Relational data schema can often be represented as a lifted graph where the nodes represent entity types and edges represent relations between two entities. A random walk on such graph may result in exploration of some interesting structure present in the relational schema. For example, the following random walk can be interpreted as a person taught a course that has a courseLevel of levelid:   Consider the random walk: personid -&gt; taught -&gt; courseid -&gt; courseLevel -&gt; levelid   This random walk can be converted into clausal form as: taught(personid,courseid) ^ courseLevel(courseid,levelid)   Table of Contents   Parameters   Random Walks can be obtained by running the following command in BoostSRL:   java -cp edu.iu.cs.RelationalRandomWalks.RunRelationalRandomWalks -rw -train \"./facts.txt\" -startentity \"personid\" -endentity \"personid\" -maxRWlen 6   As shown above the following flags need to be set:           -rw: Perform lifted relational random walks.            -startentity: Set the entity type from which the random walk should always start (e.g. personid in the above example).            -endentity: Set the entity type at which the random walk should always end, (e.g. levelid in above example).            -maxRWlen: Set the maximum length (number of relations) of any random walks.            -train: Set the path to schema file.       Table of Contents   Input File   The input file (‘facts.txt’) will consist of the schema of the relational dataset. An example of schema to be input to the system is shown as follows:   courseLevel(courseid,levelid)|NoBF student(personid,tudentype)|NoBF professor(personid,rofessortype)|NoBF inPhase(personid,haseid)|NoBF yearsInProgram(personid,yearid)|NoBF hasPosition(personid,ositiontype)|NoBF B_taughtBy(courseid,personid)|NoTwin|NoBB   Table of Contents   Setting Flags in Input File   As can be seen from the above examples, some flags can be set in schema file after vertical bars (|) for each relation. For more information on importance of these flags, please refer to [1]. This code supports the following flags: NoTwin: This code allows an inverse relation for every relation present in schema file, which is represented by putting an underscore (_) character in front relation. For e.g. inverse of courseLevel(coursid,levelid) will be represented as _courseLevel(levelid, courseid) such that courseLevel and _courseLevel are two distinct relations. Setting NoTwin disallows the inverse of a relation to be present in random walks.           NoTwin: Disallow the inverse of a relation from being present in random walks.            NoBB: Restrict an inverse relation to immediately follow itself in random walk.            NoFF: Restrict a non-inverse relation to immediately follow itself in random walk.            NoFB: Restrict an inverse relation to immediately follow its ‘non-inverse’ counterpart in random walk.            NoBF: Restrict a non-inverse relation to immediately follow its inverse counterpart in random walk.       Caution: these flags are case sensitive. So set them carefully.   Output File   The output will be stored in ‘RWRPredicates.txt’ file in the same folder as input file.   References      Ni Lao and William W. Cohen, “Relational Retrieval Using a Combination of Path-Constrained Random Walks”, ECML 2011.   Table of Contents     ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/lifted-relational-random-walks/",
        "teaser": null
      },{
        "title": "Grounded Relational Random Walks",
        "excerpt":"Overview   We obtain valued structure of relational data by randomly walking on a lifted relational graph whose nodes represent different object types and edges denote relations between them. We can obtain first order clauses from these relational random walks; where each random walk forms the body of the clause and object types at the end points of random walks form the arguments of a target predicate.   This idea is now extended to grounded random walks from the given lifted random walks that are expressed in clausal form by unifying them with the fact base. In the following tutorial, we explain how to obtain grounded random walks from the lifted random walks by building on inference in MLN-Boost models.   Table of Contents     Formulating Lifted Random Walks as Clauses   Suppose we are provided with a lifted random walk as follows:      We are inferring the target relation advisedBy(personid1,personid2) by converting it into a clausal form that is acceptable to MLN-Boost inference.   ( advisedBy(A,B,0) :- SamePerson(A, C), isa(C, D), _isa(D,E), tempAdvisedBy(E, B), !).   Convert the lifted random walks in clausal form above and save them in ./train/models/bRDNs/Trees/advisedByTree0.tree   The name of the output file (advisedByTree0.tree) should always be targetPredicate name followed by Tree0.tree. The third argument of the advisedBy predicate is the regression value of MLN-Boost. It can be set to any real number for executing grounded random walks.   Refer to our tutorial on Lifted Relational Random Walks to generate the lifted random walks.   Table of Contents     Facts File   /test/test_facts.txt   The facts file should contain all of the facts that would be used to ground the lifted random walks. It should not contain the inverted facts (those starting with underscore: _) as they are generated internally at runtime.   Background File   /test/test_bk.txt      Background (bk file) should contain modes set for all predicates and all modes should be set to +. No modes should be set for inverted predicates.   Random walk constraints should be set for each predicate by utilizing the keyword randomwalkconstraint. This constraint will be used to generate the inverse predicates (e.g. _is(designation,personid).) for facts at runtime. These constraints should be set the same way as they were set during generation of lifted random walks. Refer to our tutorial on Lifted Random Walks for more details on setting these constraints.   Negative Examples File   /test/test_neg.txt   The negative examples file can be left empty. Otherwise, the code is capable of generating random walks for negative examples too.   Target Predicate File   Create a model file named targetpredicate.model (in our example: advisedBy.model). This is required for the internal workings of MLN-Boost inference. This file should contain the following:   1 targetpredicatename [1.0] -1.8 targetpredicatename   Store this file in ./train/models/bRDNs/advisedBy.model   Table of Contents     Usage Commands   java -cp edu.wisc.cs.will.GroundRelationalRandomWalks.RunGroundRelationalRandomWalks -grw -mln -i -test \".\\test\" -target advisedBy -trees 1 -model \".\\train\\models\"   The output grounded random walks are stored inside ./test/OutputRW.txt Kindly ignore all the AUC-ROC values and AUC-PR values, they are generated as part of MLN-Boost execution.   Getting Started   Here is a small sample dataset to get started: IMDB-relationalrandomwalk   References           Tushar Khot, Sriraam Natarajan, Kristian Kersting, Jude Shavlik, “Learning Markov Logic Networks via Functional Gradient Boosting,” In ICDM 2011.            Ni Lao and William W. Cohen, “Relational Retrieval Using a Combination of Path-Constrained Random Walks,” ECML 2011.       Table of Contents     ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/grounded-relational-random-walks/",
        "teaser": null
      },{
        "title": "Natural Language Processing",
        "excerpt":"Overview   Natural language processing is generally a hard task for methods which ignore the relations between sentences and words, and require careful feature construction to work effectively. We focus on preserving these relations while using BoostSRL as the learning and inference engine.     Financial NLP      “Financial NLP” is an information extraction project for finding three features in SEC Form S-1 documents, notably the number of primary shares, secondary shares, and overallotments. This project is split into two parts: the information extraction package and the terminal interface to make it easier to interact with. Furthermore, it is a testbed for providing advice to the algorithms through markov logic networks.   Table of Contents     References and Further Reading      Sriraam Natarajan, Ameet Soni, Anurag Wazalwar, Dileep Viswanathan, and Kristian Kersting, Deep Distant Supervision: Learning Statistical Relational Models for Weak Supervision in Natural Language Extraction, Morik Festschrift, LNAI 9580 2016.   Sriraam Natarajan, Vishal Bangera, Tushar Khot, Jose Picado, Anurag Wazalwar, Vitor Santos Costa, David Page, and Michael Caldwell, Markov Logic Networks for Adverse Drug Event Extraction from Text, Knowledge and Information Systems (KAIS), 2016.   Ameet Soni, Dileep Viswanathan, Jude Shavlik, and Sriraam Natarajan, Learning Relational Dependency Networks for Relation Extraction, Internation Conference on Inductive Logic Programming (ILP), 2016.   Table of Contents     ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/wiki/natural-language-processing/",
        "teaser": null
      },{
        "title": "Toy Father",
        "excerpt":"Overview   This toy dataset is used in the “Getting Started” section of the BoostSRL Wiki. The goal is to predict who is the father of who.   Target: father   The facts contain labels for male, siblingof, and childof.      train_pos    father(harrypotter,jamespotter). father(ginnyweasley,arthurweasley). father(ronweasley,arthurweasley). father(fredweasley,arthurweasley). ...      train_neg    father(harrypotter,mollyweasley). father(georgeweasley,jamespotter). father(harrypotter,arthurweasley). father(harrypotter,lilypotter). father(ginnyweasley,harrypotter). father(mollyweasley,arthurweasley). father(fredweasley,georgeweasley). father(georgeweasley,fredweasley). father(harrypotter,ronweasley). father(georgeweasley,harrypotter). father(mollyweasley,lilypotter). ...      train_facts    male(jamespotter). male(harrypotter). male(arthurweasley). male(ronweasley). male(fredweasley). male(georgeweasley). siblingof(ronweasley,fredweasley). siblingof(ronweasley,georgeweasley). siblingof(ronweasley,ginnyweasley). siblingof(georgeweasley,ginnyweasley). siblingof(ginnyweasley,ronweasley). siblingof(ginnyweasley,fredweasley). siblingof(ginnyweasley,georgeweasley). childof(jamespotter,harrypotter). childof(lilypotter,harrypotter). childof(arthurweasley,ronweasley). childof(mollyweasley,georgeweasley). childof(arthurweasley,ginnyweasley). childof(mollyweasley,ginnyweasley). ...     Download   Download: Toy-Father.zip (4.3 KB)      md5sum:     a637cae7ba78997a0d0bb372d1edaf5e       sha256sum:     75a45707975977daa7358e4678dd3eaf97293c6d98910e474c133593adb1cfd7         Setup:           After downloading, unzip Toy-Father.zip       unzip Toy-Father.zip            If you’re using a jar file, move it into the Toy-Father directory:       mv v1.0.jar Father/BoostSRL.jar   mv auc.jar Father/auc.jar            Learning:       java -jar BoostSRL.jar -l -train train/ -target father -trees 10            Inference:       java -jar BoostSRL.jar -i -model train/models -test test/ -target father -aucJarPath . -trees 10     Modes   //Parameters setParam: maxTreeDepth=3. setParam: nodeSize=1. setParam: numOfClauses=8. //Modes mode: male(+name). mode: childof(+name,+name). mode: siblingof(+name,-name). mode: father(+name,+name).  ","categories": [],
        "tags": [],
        "url": "/datasets/toy-father/",
        "teaser": null
      },{
        "title": "Toy Cancer",
        "excerpt":"Overview   This is referred to as a “Toy Dataset” because of its small size and the fact that it uses made-up data. However, it is meant to show that the probability of someone having cancer increases if they smoke or have friends who smoke.   Target: cancer   The facts contain information on two labels: friends, smokes.      train_facts    friends(Alice, Bob). friends(Alice, Fred). friends(Chuck, Bob). friends(Chuck, Fred). friends(Dan, Bob). friends(Earl, Bob). friends(Bob, Alice). friends(Fred, Alice). friends(Bob, Chuck). friends(Fred, Chuck). friends(Bob, Dan). friends(Bob, Earl). smokes(Alice). smokes(Chuck). smokes(Bob).      train_pos    cancer(Alice). cancer(Bob). cancer(Chuck). cancer(Fred).      train_neg    cancer(Dan). cancer(Earl).     Download   Download: Toy-Cancer.zip (2.83 KB)      md5sum:     fa15b64583f9b1abc7fd78b93025792d       sha256sum:     618d9283caa5459711b01d7b535aa1e91c8c98945ed4085248368a373ce880c2         Setup           After downloading, unzip Toy-Cancer.zip       unzip Toy-Cancer.zip            If you’re using a jar file, move it into the Toy-Cancer directory:       mv (BoostSRL jar file) Toy-Cancer/   mv (auc jar file) Toy-Cancer/            Learning:       java -jar BoostSRL.jar -l -train train/ -target cancer -trees 10            Inference:       java -jar BoostSRL.jar -i -test test/ -model train/models/ -target cancer -aucJarPath . -trees 10     Modes   useStdLogicVariables: true. setParam: treeDepth=4. setParam: nodeSize=2. setParam: numOfClauses=8. mode: friends(+Person, -Person). mode: friends(-Person, +Person). mode: smokes(+Person). mode: cancer(+Person). bridger: friends/2. //precompute1: num_of_smoking_friends(x, n) :-    friends(x, y), // grounding x first    countUniqueBindings((friends(x,z)^smokes(z)), n). mode: num_of_smoking_friends(+Person, #Number).  ","categories": [],
        "tags": [],
        "url": "/datasets/toy-cancer/",
        "teaser": null
      },{
        "title": "IMDB: Internet Movie Database",
        "excerpt":"Overview   Internet Movie Database (IMDB) is an online database of movies, television shows, the actors/actresses that star in them, and the people that make them.   Target:      female_gender     Download   Download: IMDB.zip (29.6 KB)      md5sum:     70f85ae348531b3103008d4e6acfa379       sha256sum:     fe636348146f9f0ef0c2a081515abf543e3db9bdab1b441575955095c41e4c4f         Setup           After downloading, unzip IMDB.zip       unzip IMDB.zip            If you’re using a jar file, move it into the IMDB directory:       mv (BoostSRL jar file) IMDB/   mv (auc jar file) IMDB/            Learning:       java -jar BoostSRL.jar -l -train train/ -target female_gender -trees 10            Inference:       java -jar BoostSRL.jar -i -test test/ -model train/models/ -aucJarPath . -target female_gender -trees 10     Modes   setParam: treeDepth=3. setParam: nodeSize=3. setParam: numOfClauses=8. setParam: numOfCycles=8. setParam: recursion=true. setParam: lineSearch=true. setParam: resampleNegs=true. mode: actor(+person). mode: recursive_actor(`person). mode: recursive_movie(+movie, `person). mode: recursive_movie(`movie, +person). mode: movie(+movie, +person). mode: movie(+movie, -person). mode: movie(-movie, +person). mode: female_gender(+person). mode: recursive_female_gender(`person). mode: genre(+person, +genre). mode: genre(+person, #genre). mode: genre(+person, -genre). mode: genre(-person, +genre). mode: recursive_genre(+person, `genre). mode: recursive_genre(`person, +genre). mode: workedunder(+person, +person). mode: workedunder(+person, -person). mode: workedunder(-person, +person). mode: recursive_workedunder(+person, `person). mode: recursive_workedunder(`person, +person). okIfUnknown: recursive_actor/1. okIfUnknown: recursive_movie/2. okIfUnknown: recursive_female_gender/1. okIfUnknown: recursive_genre/2. okIfUnknown: recursive_workedunder/2. okIfUnknown: movie_2_1_genre/3. okIfUnknown: workedunder_2_1_genre/3. okIfUnknown: workedunder_1_1_gender/2. okIfUnknown: movie_2_1_gender/2. usePrologVariables: true. mode: movie_2_1_genre(+movie, +genre, #count). mode: movie_2_1_genre(+movie, #genre, #count). mode: movie_2_1_gender(+movie, #count). mode: workedunder_2_1_genre(+person, +genre, #count). mode: workedunder_2_1_genre(+person, #genre, #count). mode: workedunder_1_1_gender(+person, #count). mode: movie_1(+person, #count). mode: movie_2(+movie, #count). mode: workedunder_1(+person, #count). mode: workedunder_2(+person, #count). precompute: movie_2_1_genre(M, G, C) :- movie(M,P1), genre(P1,G), all(P, movie(M, P)^genre(P,G), AllP), C is length(AllP). precompute: movie_2_1_gender(M, C) :- movie(M,P1), female_gender(P1), all(P, movie(M, P)^female_gender(P), AllP), C is length(AllP). precompute: workedunder_2_1_genre(M, G, C) :- workedunder(M,P1), genre(P1,G), all(P, workedunder(M, P)^genre(P,G), AllP), C is length(AllP). precompute: workedunder_1_1_gender(M, C) :- workedunder(P1,M), female_gender(P1), all(P, workedunder(P, M)^female_gender(P), AllP), C is length(AllP). precompute: movie_1(P,C) :- movie(M,P), all(M, movie(M,P), AllM), C is length(AllM). precompute: movie_2(M,C) :- movie(M,P), all(P, movie(M,P), AllM), C is length(AllM). precompute: workedunder_1(D, C) :- workedunder(A,D), all(A, workedunder(A,D), AllA), C is length(AllA). precompute: workedunder_2(A, C) :- workedunder(A,D), all(D, workedunder(A,D), AllD), C is length(AllD).  ","categories": [],
        "tags": [],
        "url": "/datasets/imdb/",
        "teaser": null
      },{
        "title": "Cora: Citation Matching",
        "excerpt":"Overview   Cora is a dataset based on citations in scientific papers. The original dataset is available on the Alchemy website, this version contains the necessary background and train/test folders.   There are four possible targets:      sameauthor   samebib   sametitle   samevenue   The facts contain information on six labels: author, haswordauthor, haswordtitle, haswordvenue, title, venue.     Download   Download: Cora.zip (489 KB)      md5sum:     905ad622d003da817c00f9835f36dc2f       sha256sum:     0a8b1f138d6827344c840bc3e3cccbe1eb40f2c102f929a05f4bc8f96ebbdae7         Setup           After downloading, unzip Cora.zip       unzip Cora.zip            If you’re using a jar file, move it into the Cora directory:       mv (BoostSRL jar file) Cora/   mv (auc jar file) Cora/            Learning:       java -jar BoostSRL.jar -l -train train/ -target sameauthor -trees 10            Inference:       java -jar BoostSRL.jar -i -test test/ -model train/models/ -aucJarPath . -target sameauthor -trees 10     Modes   usePrologVariables: false. setParam: maxTreeDepth=3. setParam: nodeSize=2. setParam: numOfClauses=4. setParam: numOfCycles=4. setParam: minLCTrees=5; setParam: incrLCTrees=5; setParam: loadAllBasicModes=false. setParam: loadAllLibraries=false. queryPred: samebib/2. queryPred: sametitle/2. queryPred: samevenue/2. queryPred: sameauthor/2. mode: author(+paper, -auth). mode: haswordauthor(+auth, -word). mode: haswordtitle(+title, -word). mode: haswordvenue(+venue, -word). mode: title(+paper, -title). mode: venue(+paper, -venue). mode: author(-paper, +auth). mode: haswordauthor(-auth, +word). mode: haswordtitle(-title, +word). mode: haswordvenue(-venue, +word). mode: title(-paper, +title). mode: venue(-paper, +venue). mode: samebib(+paper, +paper). mode: sametitle(+title, +title). mode: samevenue(+venue, +venue). mode: sameauthor(+auth, +auth). mode: recursive_samebib(+paper, `paper). mode: recursive_sametitle(+title, `title). mode: recursive_samevenue(+venue, `venue). mode: recursive_sameauthor(+auth, `auth). mode: recursive_samebib(`paper, +paper). mode: recursive_sametitle(`title, +title). mode: recursive_samevenue(`venue, +venue). mode: recursive_sameauthor(`auth, +auth). mode: samebib(+paper, -paper). mode: sametitle(+title, -title). mode: samevenue(+venue, -venue). mode: sameauthor(+auth, -auth). mode: samebib(-paper, +paper). mode: sametitle(-title, +title). mode: samevenue(-venue, +venue). mode: sameauthor(-auth, +auth). usePrologVariables: true. okIfUnknown: recursive_sametitle/2. okIfUnknown: recursive_samebib/2. okIfUnknown: recursive_samevenue/2. okIfUnknown: recursive_sameauthor/2.  ","categories": [],
        "tags": [],
        "url": "/datasets/cora/",
        "teaser": null
      },{
        "title": "UW-CSE: Advised-By Relationships",
        "excerpt":"Overview   From the UW-CSE Alchemy Page.      “This data set consists of information about the University of Washington Department of Computer Science and Engineering. The data has been anonymized to comply with the University of Washington’s privacy guidelines.”    As usual, the version here is a .zip with the necessary background and train/test folders.   Target: advisedby   The facts contain information on fourteen labels: courselevel, hasposition, inphase, professor, projectmember, publication, samecourse, sameperson, sameproject, student, ta, taughtby, tempadvisedby, yearsinprogram.     Download   Download: UW-CSE.zip (257 KB)      md5sum:     5e8217ebdb835ff8b6ff94eb3880d96b       sha256sum:     f16be492805bdac95cded02a3a3e590c29a68145f5ea59eb4180c300fb23b7e2         Setup           After downloading, unzip UW-CSE.zip       unzip UW-CSE.zip            If you’re using a jar file, move it into the UW-CSE directory:       mv (BoostSRL jar file) UW-CSE/   mv (auc jar file) UW-CSE/            Learning:       java -jar BoostSRL.jar -l -train train/ -target advisedby -trees 10            Inference:       java -jar BoostSRL.jar -i -test test/ -model train/models/ -aucJarPath . -target advisedby -trees 10     Modes   setParam: loadAllLibraries = false. setParam: treeDepth=3. setParam: nodeSize=1. setParam: numOfClauses=8. setParam: numOfCycles=8. importLibrary:  listsInLogic. queryPred: advisedby/2. mode: professor(+Person). mode: student(+Person). mode: publication(+Title, -Person). mode: publication(-Title, +Person). mode: taughtby(+Course, +Person, -Quarter). mode: taughtby(+Course, -Person, +Quarter). mode: taughtby(-Course, +Person, -Quarter). mode: courselevel(+Course, +Level). mode: courselevel(+Course, #Level). mode: hasposition(+Person, +Position!1). mode: hasposition(+Person, #Position). mode: multiclass_hasposition(+Person). okIfUnknown: multiclass_hasposition/1. mode: projectmember(+Project, -Person). mode: projectmember(-Project, +Person). range: Position={faculty_affiliate,faculty,faculty_adjunct,faculty_emeritus}. range: Phase={pre_quals,post_generals,post_quals}. mode: position(+Position). mode: phase(+Phase). position(faculty_affiliate). position(faculty). position(faculty_adjunct). position(faculty_emeritus). phase(pre_quals). phase(post_generals). phase(post_quals). mode: advisedby(+Person, +Person). mode: inphase(+Person, +Phase!1). mode: inphase(+Person, #Phase). mode: multiclass_inphase(+Person). okIfUnknown: multiclass_inphase/1. mode: tempadvisedby(-Person, +Person). mode: tempadvisedby(+Person, -Person). mode: yearsinprogram(+Person, #Integer). mode: ta(+Course, -Person, +Quarter). mode: ta(+Course, +Person, -Quarter). mode: ta(-Course, +Person, -Quarter). mode: sameperson(+Person, +Person). mode: samecourse(+Course, +Course). mode: sameproject(+Project, +Project). mode: have_more_than_n_pubs(+Person, #PThresh). mode: have_more_than_n_common_pubs(+Person, -Person, #PThresh). mode: have_more_than_n_common_pubs(-Person, +Person, #PThresh). mode: count_taughtby(+Person, -PThresh). mode: count_publications(+Person, -PThresh). mode: count_common_pubs(+Person, -Person, -PThresh). mode: count_common_pubs(-Person, +Person, -PThresh). usePrologVariables: true. precompute: commonpub(Title, P1,P2) :- publication(Title, P1), publication(Title, P2),P1\\==P2. precompute: commonta(C,Q,P1,P2) :- ta(C,P2,Q), taughtby(C,P1,Q). precompute1: count_taughtby(Person,N) :- taughtby(SomeC, Person, SomeQ), all([Course, Quarter], taughtby(Course, Person, Quarter), AllCourses), N is length(AllCourses). precompute1: count_publications(Person,N) :- publication(Somet, Person), all(Title, publication(Title, Person), AllTitles), N is length(AllTitles). precompute1: count_common_pubs(P1,P2,N) :- commonpub(Somet, P1,P2), all(Title, commonpub(Title, P1,P2), AllTitles),  N is length(AllTitles). precompute2: have_more_than_n_pubs(A,N) :- \t        count_publications(A,N2), \t\tmember(N,[1, 3, 5, 7, 9,11,13,15]), \t\t        N2 &gt; N. precompute2: have_more_than_n_common_pubs(A1,A2,N) :- \t        count_common_pubs(A1,A2,N2), \t\tmember(N,[1, 3, 5, 7, 9,11,13,15]), \t\t        N2 &gt; N.  ","categories": [],
        "tags": [],
        "url": "/datasets/uwcse/",
        "teaser": null
      },{
        "title": "WebKB",
        "excerpt":"Overview   The Alchemy WebKB Dataset was adapted from a dataset by the same name from Mark Craven’s website (from the University of Wisconsin-Madison). WebKB consists of web pages and hyperlinks “from four computer science departments: Cornell University, The University of Texas, The University of Washington, and The University of Wisconsin.”   This version contains the necessary background and train/test folders.   Target: faculty   The facts contain information on five labels: courseprof, courseta, project, sameperson, student.     Download   Download: WebKB.zip (41.1 KB)      md5sum:     977e62fca51bfa7fe9c27bdf8af5d478       sha256sum:     7b36e85cc99483a98c68fc868ba9890398339eaca20b48b80e4b56d16ddc1522         Setup           After downloading, unzip WebKB.zip       unzip WebKB.zip            If you’re using a jar file, move it into the WebKB directory:       mv (BoostSRL jar file) WebKB/   mv (auc jar file) WebKB/            Learning:       java -jar BoostSRL.jar -l -train train/ -target faculty -trees 10            Inference:       java -jar BoostSRL.jar -i -test test/ -model train/models/ -aucJarPath . -target faculty -trees 10         Modes   setParam: loadAllLibraries = false. setParam: treeDepth=3. setParam: nodeSize=3. setParam: numOfClauses=8. mode:courseprof(-Course, +Person). mode:courseprof(+Course, -Person). mode: courseta(+Course, -Person). mode: courseta(-Course, +Person). mode:faculty(+Person). mode:project(-Proj, +Person). mode:project(+Proj, -Person). mode:sameperson(-Person, +Person). mode:student(+Person).  ","categories": [],
        "tags": [],
        "url": "/datasets/webkb/",
        "teaser": null
      },{
        "title": "CiteSeer: Citation Matching",
        "excerpt":"Overview   “CiteSeer” is a relational dataset of publication citations for Alchemy, the original dataset is available on their website. This version has modifications to work with BoostSRL; including the associated background, train/test folders, and the positives/negatives/facts.   Three targets are possible:      infield_fauthor   infield_ftitle   infield_fvenue     Download   Download: CiteSeer.zip (1.62 MB)      md5sum:     e606e6f3fbe12f62cb5261285b39209c       sha256sum:     f5f6dd960a09d98e80cb2dcb735463dbc7dc5aaf2676f98d938be7df6edd2200         Setup           After downloading, unzip CiteSeer.zip       unzip CiteSeer.zip            If you’re using a jar file, move it into the CiteSeer directory:       mv (BoostSRL jar file) CiteSeer/   mv (auc jar file) CiteSeer/            Learning:  Note: Learning may take a long amount of time on all three targets.     java -jar BoostSRL.jar -l -train train/ -trees 10 -target infield_fauthor,infield_ftitle,infield_fvenue             Inference:       java -jar BoostSRL.jar -i -test test/ -model train/models/ -aucJarPath . -target infield_fauthor,infield_ftitle,infield_fvenue -trees 10     Modes   // Parameters usePrologVariables: true. setParam: treeDepth=4. setParam: nodeSize=2. setParam: numOfClauses=8. setParam: numOfCycles=8. // Modes &amp; Bridgers mode: center(+bib, +pos). mode: center(+bib, -pos). mode: firstin(+bib, +pos). mode: firstin(+bib, -pos). mode: firstnonauthortitletkn(+bib, +pos). mode: firstnonauthortitletkn(+bib, -pos). mode: followby(+bib, +pos, #token). mode: hascomma(+bib, +pos). mode: hascomma(+bib, -pos). mode: haspunc(+bib, +pos). mode: haspunc(+bib, -pos). mode: infield_ftitle(+bibpos). mode: infield_fauthor(+bibpos). mode: infield_fvenue(+bibpos). mode: isalphachar(+token). mode: isdate(+token). mode: isdigit(+token). mode: lastinitial(+bib, +pos). mode: lastinitial(+bib, -pos). mode: lessthan(+pos, -pos). mode: lessthan(-pos, +pos). mode: next(+pos, -pos). mode: next(-pos, +pos). bridger: next/2. mode: nextbibpos(+bibpos, -bibpos). mode: nextbibpos(-bibpos, +bibpos). nextbibpos(BP1,BP2) :- isbibpos(BP1, B,P1), isbibpos(BP2,B,P2), next(P1,P2). mode: isbibpos(+bibpos, -bib, -pos). mode: isbibpos(+bibpos, +bib, -pos). mode: isbibpos(+bibpos, -bib, +pos). mode: isbibpos(-bibpos, +bib, +pos). bridger: isbibpos/3. mode: token(+token, +pos, +bib). mode: token(+token, -pos, +bib). mode: token(-token, +pos, +bib).  ","categories": [],
        "tags": [],
        "url": "/datasets/citeseer/",
        "teaser": null
      },{
        "title": "Boston Housing Prices: Regression",
        "excerpt":"Overview   This dataset concerns housing values in Boston suburbs. It’s based on the “Boston Housing Dataset” from University of California, Irvine, which in turn was taken from the StatLib library maintained at Carnegie Mellon University.   The target is medv: median value of owner-occupied homes in terms of thousands of dollars ($1000s).   Features:      crim: per-capita crime rate by town.   zn: proportion of residential land zoned for lots over 25,000 sq.ft.   indus: proportion of non-retail business acres per town.   chas: Charles River dummy variable (=1 if tract bounds river; 0 otherwise)   nox: nitric oxides concentration (parts per 10 million)   rm: average number of rooms per dwelling.   age: proportion of owner-occupied units built prior to 1940.   dis: weighted distances to five Boston employment centres.   rad: index of accessibility to radial highways.   tax: full-value property-tax rate per $10,000.   ptratio: pupil-teacher ratio by town.   b: 1000(Bk-0.63)^2 where Bk is the proportion of black people by town.   lsat: percent lower status of the population.   medv: median value of owner-occupied homes in terms of thousands of dollars ($1000s).     Download   Download: Boston-Housing.zip (19 KB)      md5sum:     5306de665616e7d76e98ea8d98ffd4b2       sha256sum:     e029e7695a87910c26861180d77c95db306ccbc01c0decfacedbf91e38c077c5         Setup           After downloading, unzip Boston-Housing.zip       unzip Boston-Housing.zip            If you’re using a jar file, move it into the Boston-Housing directory:  mv (BoostSRL jar file) Boston-Housing/   mv (auc jar file) Boston-Housing/            For learning/inference, full explanations are available on the “Regression Tutorial”. Commands are also listed below.          Learning:   java -cp BoostSRL.jar edu.wisc.cs.will.Boosting.Regression.RunBoostedRegressionTrees -reg -l -train train/ -target medv -trees 20      Inference:   java -cp BoostSRL.jar edu.wisc.cs.will.Boosting.Regression.RunBoostedRegressionTrees -i -test test/ -aucJarPath . -target medv -model train/models/ -trees 20     Modes   Notice that since the values have been discretized, we can treat the values as constants and therefore we can use an octothorpe (#) in the modes file.   mode: crim(+id,#varsrim). mode: zn(+id,#varzn). mode: indus(+id,#varindus). mode: chas(+id,#varchas). mode: nox(+id,#varnox). mode: rm(+id,#varrm). mode: age(+id,#varage). mode: dis(+id,#vardis). mode: rad(+id,#varrad). mode: tax(+id,#vartax). mode: ptratio(+id,#varptrat). mode: b(+id,#varb). mode: lstat(+id,#varlstat). mode: medv(+id).  ","categories": [],
        "tags": [],
        "url": "/datasets/boston-housing/",
        "teaser": null
      },{
        "title": "Drug-Drug Interactions",
        "excerpt":"Overview   The data consists of 5 similarity matrices used for the problem of drug-drug interaction prediction. These 5 similarity matrices are:     Reachability: Concisely, this similarity measure defines how many paths exist between two drugs in a directed graph of known chemical reactions between drugs and enzymes, drugs and transporters and drugs and targets. The directed graph is obtained by applying the path ranking algorithm [1] on a knowledge graph constructed from the ADMET features of the drugs, which are obtained from the DrugBank database. The directed graph consists of parameterized guided random walks which are sequences of relations with shared arguments, where the arguments are entity classes starting and ending in the drug entity. Thus, there can be multiple paths that exist between two drugs based on the various random walks and the reachability similarity matrix consists of the counts i.e. the number of paths that exist between two drugs. The other 4 similarity matrices are derived from SMILES strings which are textual representations of the molecular structure of the drugs.   Molecular Feature Similarity: compares the chemical properties of two drugs using 19 features extracted from their SMILES strings. While extracting these features, it was made sure that they affect the chemical reactiveness of a molecule. Some of these features include, number of valence electrons, number of aromatic rings and number of hydrogen donors and receptors. The similarity measure is constructed by using the Jaccardian distance between the calculated features.   SMILES String Similarity: is the textual similarity between the SMILES strings calculated using edit distance.   Molecular Fingerprint similarity: Fingerprints are bit-string representations of the molecular structure of the drugs. A similarity score between the fingerprints of each pair of drugs is generated using the python rdkit[2] package.   MACCS fingerprint similarity: are 166 bit structural key descriptors in which each bit is associated with a SMARTS[3] pattern.   All the similarity matrices are symmetric in nature and the data consists of 196 drugs (thus 196x196 sized matrices). Since every drug can react with all the other drugs except itself, 196 drugs correspond to (196*195)/2 = 19110 possible interactions. Since the data is symmetric, only the upper(lower) half of the similarity matrices should be used.     Download   Download: DDI_data_196_drugs.zip (232 KB) The dataset consists of the 5 similarity measures, the true labels of the drug pairs and the name of the drugs involved.      md5sum:     884b32402f8a8519209b8bdc0e305c9e       sha256sum:     c6c7f9e51674af870e63be157eacc7b54d1c954acfc7d30a179f7a54c85fa51e         References:   [1] Lao, Ni, and William W. Cohen. “Relational retrieval using a combination of path-constrained random walks.” Machine learning 2010   [2] http://www.rdkit.org/   [3] http://www.daylight.com/dayhtml/doc/theory/theory.smarts.html  ","categories": [],
        "tags": [],
        "url": "/datasets/ddi/",
        "teaser": null
      },{
        "title": "Drug-Drug Interactions (Relational)",
        "excerpt":"Drug-Drug Interactions (Relational)   A relational dataset consisting of relations between various drugs and enzymes, transporters and targets. Prediction task is whether two drugs interact.   Download   Download: DDI.zip (72.8 KB)      md5sum:     c67446a109e599fbd8ad7cce146ab85f       sha256sum:     e3113dcb215f22ba0fb7544220f22c4626cdc37986fd130279a94d53572c57a6      ","categories": [],
        "tags": [],
        "url": "/datasets/ddi-relational/",
        "teaser": null
      },{
        "title": "Financial NLP",
        "excerpt":"Financial NLP   A relational dataset consisting of relations obtained by extracting information from SEC Form S-1 documents. Prediction task is whether a word is present in a sentence.   Download   Download: FNLP.zip (72.8 KB)      md5sum:     a8a603fa6e6ce61ec808739b9e830d58       sha256sum:     a941dd86abf97518405cc8815a2df5fe9db8730d325acc1b6adf9bb316ab302b      ","categories": [],
        "tags": [],
        "url": "/datasets/fnlp/",
        "teaser": null
      },{
        "title": "NELL Sports",
        "excerpt":"NELL Sports   A relational dataset consisting of relations generated by the Never Ending Language Learner (NELL) consisting of information about players and teams. Prediction task is whether a team plays a particular sport.   Download   Download: NELL.zip (178.9 KB)      md5sum:     5eb6b3606051275b79280f4b16ede796       sha256sum:     c0d0a93a1cb7456037375a6a8c890b8906486d9329c78c5b46f124a70bbde44b      ","categories": [],
        "tags": [],
        "url": "/datasets/nells/",
        "teaser": null
      },{
        "title": "ICML Co-authors",
        "excerpt":"ICML Co-authors   A relational dataset consisting of relations between a person and his/her affiliation, location, institute type and research topic. It is obtained by mining publication data from ICML 2018. Prediction task is whether 2 persons are co-authors.   Download   Download: ICML.zip (178.9 KB)      md5sum:     08d8634e332304e62f4c184967412e5a       sha256sum:     90b3cf4542c549b291137298968f2f3426ce10938edab0321108cd5530d951c9      ","categories": [],
        "tags": [],
        "url": "/datasets/coauthor/",
        "teaser": null
      },{
        "title": "RRBM Paper Accepted at ILP",
        "excerpt":"Navdeep’s paper on Relational Restricted Boltzmann Machines using Probabilistic Random Walks has been accepted to ILP. Congratulations to her and all coauthors!      Navdeep Kaur, Gautam Kunapuli, Tushar Khot, William Cohen, Kristian Kersting and Sriraam Natarajan, “Relational Restricted Boltzmann Machines: A Probabilistic Logic Learning Approach”, Inductive Logic Programming (ILP) 2017.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2017-06-09-rrbm-paper-accepted/",
        "teaser": null
      },{
        "title": "Two Health Informatics Papers Accepted",
        "excerpt":"Two Health Informatics Papers were accepted: “Identifying Parkinson’s Patients” at AI in Medicine, and “Boosting for Postpartum Depression Prediction” at CHASE 2017. Congratulations to everyone!           Devendra Singh Dhami, Ameet Soni, David Page, Sriraam Natarajan, “Identifying Parkinson’s Patients: A Functional Gradient Boosting Approach”, Artificial Intelligence in Medicine (AIME) (2017).            Sriraam Natarajan, Annu Prabhakar, Nandini Ramanan, Anna Bagilone, Katie Siek, and Kay Connelly, “Boosting for Postpartum Depression Prediction”, IEEE Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE), 2017.      ","categories": ["news"],
        "tags": [],
        "url": "/news/2017-09-19-chase-papers-accepted/",
        "teaser": null
      },{
        "title": "Alexander's Paper Accepted at Knowledge Capture 2017",
        "excerpt":"Our paper on using entity-relational diagrams to set background knowledge and provide user advice for our boosting algorithm has been accepted at Knowledge Capture Conference 2017!   Alexander L. Hayes, Mayukh Das, Phillip Odom, and Sriraam Natarajan. 2017, User Friendly Automatic Construction of Background Knowledge: Mode Construction from ER Diagrams, Knowledge Capture Conference (K-CAP ‘17).  ","categories": ["news"],
        "tags": [],
        "url": "/news/2017-09-19-erd-knowledge-capture-accepted/",
        "teaser": null
      },{
        "title": "Two Papers Accepted at BIBM",
        "excerpt":"Two papers were accepted at BIBM, one on predicting the number of cardiovascular procedures (Shuo) and the other on predicting multiple procedures jointly (Nandini). Congratulations to Shuo Yang, Nandini Ramanan, and all coauthors!           Shuo Yang, Fabian Hadiji, Kristian Kersting, Shaun Grannis and Sriraam Natarajan, Modeling Heart Procedures from EHRs: An Application of Exponential Families, IEEE International Conference on Bioinformatics and Biomedicine (IEEE BIBM), 2017.            Nandini Ramanan, Shuo Yang, Shaun Grannis, and Sriraam Natarajan, Discriminative Boosted Bayes Networks for Learning Multiple Cardiovascular Procedures , IEEE International Conference on Bioinformatics and Biomedicine (IEEE BIBM),2017.      ","categories": ["news"],
        "tags": [],
        "url": "/news/2017-09-29-bibm-submissions-accepted/",
        "teaser": null
      },{
        "title": "Dr. Gautam Kunapuli Joins StARLinG",
        "excerpt":"A warm welcome to Dr. Gautam Kunapuli! He has joined StARLinG Lab and The University of Texas at Dallas as a Research Associate Professor.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2017-10-01-welcome-dr-kunapuli/",
        "teaser": null
      },{
        "title": "Submission on Mixed Sum-Product Networks Accepted to AAAI",
        "excerpt":"“Mixed Sum-Product Networks: A Deep Architecture for Hybrid Domains” was accepted at AAAI!   Alejandro Molina, Antonio Vergari, Nicola Di Mauro, Floriana Esposito, Siraam Natarajan, and Kristian Kersting, Mixed Sum-Product Networks: A Deep Architecture for Hybrid Domains, In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2018.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2017-11-10-aaai-submission-accepted/",
        "teaser": null
      },{
        "title": "Statistical Relational Learning Tutorial at NIPS",
        "excerpt":"Sriraam Natarajan presented a tutorial on Statistical Relational AI with Luc De Raedt, Kristian Kersting and David Poole at NIPS 2017.   Update:   A video of the presentation is available on Facebook.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2017-12-04-nips-tutorial/",
        "teaser": null
      },{
        "title": "AAMAS'2018 Papers Accepted",
        "excerpt":"Great start to 2018, two papers from our students were accepted in International Conference on Autonomous Agents and Multiagent Systems (AAMAS).           Mayukh Das, Phillip Odom, Md. Rakibul Islam, Jana Doppa, Dan Roth, and Sriraam Natarajan, “Preference-Guided Planning: An Active Elicitation Approach.” International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2018.            Kaushik Roy, Phillip Odom, Kristian Kersting, Ron Parr, and Sriraam Natarajan, “Non-parametric Fitted Relational Value Iteration: Unifying Relational and Propositional Discrete Domains.” International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2018.       Congratulations to Mayukh Das, Kaushik Roy, and all coauthors! Copies will be available under Publications soon.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-01-25-papers-accepted-at-aamas/",
        "teaser": null
      },{
        "title": "Sriraam Natarajan Awarded with Amazon Faculty Award",
        "excerpt":"Professor Sriraam Natarajan is a recipient of the 2017 Amazon Research Awards for work on “Guiding Probabilistic Learning in Structured Domains with Crowd-Sourced Inputs: Treating Humans as More Than Mere Labelers.”   Recipients are available on the Amazon Research Awards page.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-02-22-amazon-faculty-award/",
        "teaser": null
      },{
        "title": "Submission on 'Active Feature Elicitation accepted to IJCAI",
        "excerpt":"“Whom Should I Perform the Lab Test on Next? An Active Feature Elicitation Approach” was accepted at International Joint Conference on Artificial Intelligence (IJCAI), 2018.   Sriraam Natarajan, Srijita Das, Nandini Ramanan, Gautam Kunapuli, Predrag Radivojac, Whom Should I Perform the Lab Test on Next? An Active Feature Elicitation Approach, International Joint Conference on Artificial Intelligence (IJCAI), 2018.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-06-07-active-feature-elicitation/",
        "teaser": null
      },{
        "title": "Submission on Kernel Learning from Heterogeneous Similarities accepted to CHASE",
        "excerpt":"“Drug-Drug Interaction Discovery: Kernel Learning from Heterogeneous Similarities” was accepted to CHASE, IEEE Conference on Connected Health: Applications, Systems and Engineering Technologies.   Devendra Singh Dhami, Gautam Kunapuli, Mayukh Das, David Page and Sriraam Natarajan, Drug-Drug Interaction Discovery: Kernel Learning from Heterogeneous Similarities, IEEE Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE) 2018.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-06-07-kernel-learning/",
        "teaser": null
      },{
        "title": "'Human-Guided Learning for Probabilistic Logic Models' published at Frontiers in Robotics and AI",
        "excerpt":"Advice-giving has been long explored in the artificial intelligence community to build robust learning algorithms when the data is noisy, incorrect or even insufficient. While logic based systems were effectively used in building expert systems, the role of the human has been restricted to being a “mere labeler” in recent times. We hypothesize and demonstrate that probabilistic logic can provide an effective and natural way for the expert to specify domain advice. Specifically, we consider different types of advice-giving in relational domains where noise could arise due to systematic errors or class-imbalance inherent in the domains. The advice is provided as logical statements or privileged features that are thenexplicitly considered by an iterative learning algorithm at every update. Our empirical evidence shows that human advice can effectively accelerate learning in noisy, structured domains where so far humans have been merely used as labelers or as designers of the (initial or final) structure of the model.   You can find more details in:     Odom, P., &amp; Natarajan, S., Human-Guided Learning for Probabilistic Logic Models, Frontiers in Robotics and AI (Front. Robot. AI) 2018.   Congratulations to Phillip Odom and Sriraam Natarajan!   ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-06-25-human-guided-learning/",
        "teaser": null
      },{
        "title": "Sriraam Natarajan to talk on 'Human Aware Statistical Relational AI' @ StarAI",
        "excerpt":"Catch Sriraam Natarajan talking about Human Aware Statistical Relational AI at StarAI workshop on 14th July, 2018.   StaRAI models combine the powerful formalisms of probability theory and first-order logic to handle uncertainty in large, complex problems. While they provide a very effective representation paradigm due to their succinctness and parameter sharing, efficient learning is a significant problem in these models. First, I will discuss state-of-the-art learning method based on boosting that is representation independent. Our results demonstrate that learning multiple weak models can lead to a dramatic improvement in accuracy and efficiency.   One of the key attractive properties of StaRAI models is that they use a rich representation for modeling the domain that potentially allows for seam-less human interaction. However, in current StaRAI research, the human is restricted to either being a mere labeler or being an oracle who provides the entire model. I will present our recent work that allows for more reasonable human interaction where the human input is taken as “advice” and the learning algorithm combines this advice with data. Finally, I will discuss our work on soliciting advice from humans as needed that allows for seamless interactions with the human expert.   Find more details about the timings on StarAI, 2018.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-07-11-talk-at-starai-2018/",
        "teaser": null
      },{
        "title": "Our proposal on Efficient Learning with Human-in-the-Loop in Structured, Noisy and Temporal Domains has been accepted by AFRL",
        "excerpt":"Our proposal on “Efficient Learning with Human-in-the-Loop in Structured, Noisy and Temporal Domains” has been accepted by Air Force Research Laboratory.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-08-02-afrl-grant/",
        "teaser": null
      },{
        "title": "Prof. Gautam Kunapuli is teaching CS6375: Machine Learning in Fall 2018",
        "excerpt":"Prof. Gautam Kunapuli is teaching CS6375: Machine Learning in Fall 2018. For more information please visit here  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-08-02-gautam-kunapuli-mlcourse-fall2018/",
        "teaser": null
      },{
        "title": "Prof. Sriraam Natarajan podcast on Statistical Relational Artificial Intelligence at TWiML&AI is now available",
        "excerpt":"Prof. Sriraam Natarajan podcast on Statistical Relational Artificial Intelligence at TWiML&amp;AI is now available   Statistical Relational Artificial Intelligence with Sriraam Natarajan - This Week in Machine Learning &amp; Artificial Intelligence Talk No 113 By Sam Charrington   Released Feb 23, 2018   In this episode, Sam Charrington speaks with Sriraam Natarajan, Associate Professor in the Department of Computer Science at UT Dallas. While at NIPS a few months back, Sriraam and Sam Charrington sat down to discuss his work on Statistical Relational Artificial Intelligence. StarAI is the combination of probabilistic &amp; statistical machine learning techniques with relational databases. They cover systems learning on top of relational databases and making predictions with relational data, with quite a few examples from the healthcare field. Sriraam and his collaborators have also developed BoostSRL, a gradient-boosting based approach to learning different types of statistical relational models. They briefly touch on this, along with other implementation approaches.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-08-02-podcast/",
        "teaser": null
      },{
        "title": "Submission on Boosting Relational Logistic Regression accepted to KR 2018",
        "excerpt":"Our collaborative work with David Poole’s group and Kristian Kersting on Boosting Relational Logistic Regression has been accepted to KR 2018.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-08-02-rlr-paper-kr/",
        "teaser": null
      },{
        "title": "Prof. Sriraam Natarajan is invited speaker @ US-Serbia & West Balkan Data Science Workshop, Serbia",
        "excerpt":"Prof. Sriraam Natarajan is invited speaker at US-Serbia &amp; West Balkan Data Science Workshop, Serbia on August 26-28, 2018.   ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-08-02-serbia-honoring/",
        "teaser": null
      },{
        "title": "Prof. Sriraam Natarajan is invited speaker @ ACAI 2018",
        "excerpt":"Catch Prof. Sriraam Natarajan teach Human Allied Statistical Relational AI at ACAI 2018 Summer School on Statistical Relational Artificial Intelligence on August 27-31, 2018 at Ferrara, Italy.   Statistical Relational AI (StaRAI) Models combine the powerful formalisms of probability theory and first-order logic to handle uncertainty in large, complex problems. While they provide a very effective representation paradigm due to their succinctness and parameter sharing, efficient learning is a significant problem in these models. First, I will discuss state-of-the-art learning methods based on boosting that is representation independent. Our results demonstrate that learning multiple weak models can lead to a dramatic improvement in accuracy and efficiency.   One of the key attractive properties of StaRAI models is that they use a rich representation for modeling the domain that potentially allows for seam-less human interaction. However, in current StaRAI research, the human is restricted to either being a mere labeler or being an oracle who provides the entire model. I will present the recent progress that allows for more reasonable human interaction where the human input is taken as “advice” and the learning algorithm combines this advice with data. Finally, I will discuss more recent work on soliciting advice from humans as needed that allows for seamless interactions with the human expert.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-08-02-talk-at-acai-2018/",
        "teaser": null
      },{
        "title": "Catch Prof. Sriraam Natarajan's video lectures on Human Allied AI from ACAI 2018 here",
        "excerpt":"Catch Prof. Sriraam Natarajan’s video lectures on Human Allied Statistical Relational AI from Summer School on Statistical Relational AI ACAI 2018 held at Ferrara, Italy here. Human Allied AI-Part1,Human Allied AI-Part 2   It is a 2 1/2 hour tutorial on Human Allied Statistical Relational AI starting with learning from data, to including human inputs as initial bias to finally closing the loop with seeking human inputs (richer than labels). It covers a broad spectrum of work done in the Starling lab and you can get an overview of our lab’’s work from these video lectures. You can also take a look at the other video lectures from this workshop here. Video Lectures at ACAI 2018  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-09-13-video-lectures-at-acai-2018/",
        "teaser": null
      },{
        "title": "Catch Prof. Sriraam Natarajan on live TV, CNN Serbia",
        "excerpt":"Catch Prof. Sriraam Natarajan on live TV, CNN Serbia here Interview on live TV, CNN, Serbia. He was an invited speaker for US-Serbia &amp; West Balkan Data Science Workshop, Serbia on August 26-28, 2018.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-09-18-cnn-serbia-video/",
        "teaser": null
      },{
        "title": "StARLinG YouTube Channel is Live",
        "excerpt":"Video lectures, talks, and tutorials by members of StARLinG Lab are on our StARLinG Lab Youtube Channel.   Like, share, comment, and subscribe!  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-09-18-new-youtube-channel/",
        "teaser": null
      },{
        "title": "NSF Eager Award for Sequential Decision Making",
        "excerpt":"We are pleased to announce that our NSF Eager proposal has been accepted:   Award Abstract #1836565: EAGER: Collaborative Research: A Unified Learnable Roadmap for Sequential Decision Making in Relatinoal Domains   This project seeks to develop new algorithms and data structures for learning and planning in situations where the environment is represented with a set of relations between objects. Relational representations capture interactions between objects in a succinct and easily interpretable representation.   Examples of domains that are well-suited to relational representations includes intelligent drones assisting soldiers, activities in a supply chain management, communication and friendship connections in a social network, and tracking individuals and activities in video.   Most recent advances in machine learning and planning, such as so-called “deep neural networks”, however, employ simple “flat” representations, where the state of the world is an uninterpreted string of bits. This project will make machine learning and planning methods easier to use and more robust by generalizing them so that they explicitly work with relational models and data.   The methods, theory, and data resulting from this proposal will impact the scientific community in several positive ways and will be made publicly available through an appropriate website. The research will be disseminated through refereed journals and conference proceedings and made available to researchers. Code for the proposed algorithms and descriptions of new benchmark problems will also be made publicly available. The investigators will work on organizing workshops and tutorials based on the challenges and findings arising from this project.   Many special purpose solutions have been developed to address small parts of these problems, but there are no general purpose tools that harness recent advances in machine learning to tackle this family of problems. This proposal seeks to develop such tools, drawing upon the investigators’ prior experience in learning relational regression trees and experience in value function approximation for reinforcement learning. In addition, this project seeks to build a bridge between recent advances in deep learning, which generally has not been compatible with relational representations, and recent advances in relational learning.   Read more on the National Science Foundation’s webpage.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-09-25-nsf-eager-proposal/",
        "teaser": null
      },{
        "title": "Devendra is attending CHASE 2018 in Washington D.C",
        "excerpt":"Dev is attending CHASE 2018 for his paper on Drug Drug interaction.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-09-26-Dev-attending-chase/",
        "teaser": null
      },{
        "title": "Prof. Sriraam Natarajan is the invited speaker at CoDS-COMAD 2019",
        "excerpt":"Prof. Sriraam Natarajan is the invited speaker at CoDS-COMAD 2019  to be held in Kolkata, India from 3rd to 5th Jan, 2019. It is a premier conference encompassing diverse areas like Databases, Data Analytics and Machine learning, hence, CONSIDER ATTENDING!!  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-10-02-invited-speaker-cods-comads/",
        "teaser": null
      },{
        "title": "We are delighted to announce that Prof. Sriraam Natarajan is one of the speciality cheif editors for frontiers",
        "excerpt":"Prof. Sriraam Natarajan is serving on the editorial board as one of the speciality chief editors in the area of Machine Learning and Artificial Intelligence for frontiers in Big Data.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-10-02-special-editor-frontier/",
        "teaser": null
      },{
        "title": "Prof. Natarajan is attending PROBPROG 2018 at MIT",
        "excerpt":"Prof. Sriraam Natarajan is attending PROBPROG 2018 held at MIT, Cambridge from Oct 4th to Oct 6th, 2018. Our lab has 2 papers accepted at PROBPROG. Check out the publications page for more details.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-10-05-attending-probprog-news/",
        "teaser": null
      },{
        "title": "Paper on approximate counting using hypergraph accepted to AAAI 2019. Congratulations Mayukh and Dev. ",
        "excerpt":"Paper on approximate counting using hypergraph accepted to AAAI 2019. This work allows for fast counting by transforming relational data to hypergraph and computing summary statistics. Code and paper will be available soon on our website. Congratulations Mayukh and Dev.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-11-02-AAAI-paper-acceptance-news/",
        "teaser": null
      },{
        "title": "Nandini presented her paper on Efficient structure learning for Relational Logistic Regression at KR 2018",
        "excerpt":"Nandini presented her paper and poster on Efficient structure learning for RLR at KR 2018 held in Tempe, Arizona. Nandini, Navdeep and Srijita attended KR 2018 and it had a great list of speakers. You can find the paper on the Publications page of our website.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-11-02-attended-KR-news/",
        "teaser": null
      },{
        "title": "Professor Natarajan is the director of THE CENTER FOR MACHINE LEARNING at UTD",
        "excerpt":"Professor Sriraam Natarajan is the director for THE CENTER FOR MACHINE LEARNING at UTD.  The purpose for this centre is to foster excellent research and development of machine learning algorithms motivated by challenges from real world domains ranging from precision health to natural language understanding, from biology to social network analysis. The core team consists of researchers whose expertise lie in relational models, probabilistic modeling, combinatorial optimization, active learning, logic-based learning, human-in-the-loop learning, reinforcement learning, supervised learning and data mining.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-12-04-Centre-of-ML-director/",
        "teaser": null
      },{
        "title": "Professor Natarajan gave a keynote at IIT Madras as distinguished faculty",
        "excerpt":"Professor Sriraam Natarajan was recognized as a Distinguished Faculty by Robert Bosch Centre for Data Science and AI at Indian Institute of Technology (IIT), Madras. He delivered a key note on Human Allied Artificial Intelligence.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2018-12-04-IIT-Chennai-Distinguished-faculty-news/",
        "teaser": null
      },{
        "title": "Nandini's paper on Causal learning for Post-Partum Depression accepted to AAAI 2019 Spring Symposium",
        "excerpt":"Nandini’s paper on Causal learning for Post-Partum Depression accepted to AAAI 2019 Spring Symposium on “Beyond Curve Fitting — Causation, Counterfactuals and Imagination-Based AI”. Nandini and Professor Natarajan are travelling to Stanford to present the poster and talk for the paper from March 25-27, 2019.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2019-03-25-AAAI19-Spring_Symposium-paper-accepted/",
        "teaser": null
      },{
        "title": "Professor Natarajan is the Program Chair for CODS-COMAD 2020",
        "excerpt":"Professor Natarajan is serving as one of the Program Chairs for CODS-COMAD 2020 to be held in Hyderabad from 5th to 7th January 2020. Consider submitting good papers.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2019-04-04-PC-Cods-Comad-2020/",
        "teaser": null
      },{
        "title": "Professor Natarajan recognized as distinguished SPC at IJCAI 2019",
        "excerpt":"Congratulations to Professor Natarajan for recognition as Distinguished SPC at IJCAI 2019 for his exceptional reviewing contribution.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2019-07-26-IJCAI-SPC/",
        "teaser": null
      },{
        "title": "Professor Natarajan will be a program co-chair for SDM 2020",
        "excerpt":"Professor Natarajan is serving as the program co-chair for SIAM Conference on Data Mining (SDM) 2020 with Professor Yan Liu. SDM will be co-located with the SIAM Conference on Mathematics of Data Science (MDS20), May 5-7, 2020.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2019-08-01-SDM-2020/",
        "teaser": null
      },{
        "title": "Two abstracts accepted at WiML, 2019",
        "excerpt":"Congratulations to all the authors on the acceptance of their abstracts at 14th Women in Machine Learning workshop co-located with NeurIPS.   Accepted Abstracts:      Navdeep Kaur, Gautam Kunapuli, Sriraam Natarajan, “Boosting Relational Restricted Boltzmann Machines”, WiML 2019.   Sriraam Natarajan, Srijita Das, Nandini Ramaman, Gautam Kunapuli, Predrag Radivojac, “Active Feature Elicitation”, WiML 2019.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2019-09-21-WiML-2019/",
        "teaser": null
      },{
        "title": "Dr. Das Defended his dissertation",
        "excerpt":"Dr. Mayukh Das defended his dissertation on Human-Allied AI successfully on October 14th.      Congratulations to Mayukh Das for well earned Ph.D.   Congratulations Dr. Natarajan!  ","categories": ["news"],
        "tags": [],
        "url": "/news/2019-10-16-Dr/",
        "teaser": null
      },{
        "title": "Dr. Gautam Kunapuli joins Verisk Analytics",
        "excerpt":"Congratulations to Dr. Gautam Kunapuli for joining Verisk Analytics. We wish you all the best for your new venture and will miss your presence.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2019-12-31-gautam-departs/",
        "teaser": null
      },{
        "title": "Paper on Knowledge-intensive gradient boosting accepted to AAAI 2020.",
        "excerpt":"Paper on Knowledge-intensive gradient boosting accepted to AAAI 2020. This work proposes an approach to leverage qualitative domain knowledge when boosting – for regions where data is noisy or absent. More details available here.   Link to paper  Link to code  ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-01-28-AAAI-paper-acceptance/",
        "teaser": null
      },{
        "title": "Six papers accepted to STARAI Workshop at AAAI 2020.",
        "excerpt":"Six paper accepted at Ninth International Workshop on Statistical Relational AI (Starai) organized at AAAI 2020. Congratulations to all the authors!   List of the papers below.      Chen, Y., &amp; Yang, Y., &amp; Natarajan, S., &amp; Ruozzi, N., Lifted Hybrid Variational Inference, Workshop on Statistical Relational AI (StarAI) 2020.   Skinner, M.A., &amp; Raman, L., &amp; Shah, N., &amp; Farhat, A., &amp; Natarajan, S., A preliminary approach for learning relational policies for the management of critically ill children, Workshop on Statistical Relational AI (StarAI) 2020.   Hayes, A.L., srlearn: A Python Library for Gradient-Boosted Statistical Relational Models, Workshop on Statistical Relational AI (StarAI) 2020.   Kaur, N., &amp; Kunapuli, G., &amp; Natarajan, S., Non-Parametric Learning of Lifted Restricted Boltzmann Machines, Workshop on Statistical Relational AI (StarAI) 2020.   Dhami, D.S., &amp; Yan, S., &amp; Kunapuli, G., &amp; Natarajan, S., Non-Parametric Learning of Gaifman Models, Workshop on Statistical Relational AI (StarAI) 2020.   Das, M., &amp; Ramanan, N., &amp; Doppa, J.R., &amp; Natarajan, S., One-Shot Induction of Generalized Logical Concepts via Human Guidance, Workshop on Statistical Relational AI (StarAI) 2020.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-01-29-starai-2020/",
        "teaser": null
      },{
        "title": "Professor Natarajan distinguished lecture at University of South Carolina",
        "excerpt":"Professor Sriraam Natarajan gave a distinguished lecture on “Human Allied Artificial Intelligence” at University of South Carolina.   ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-02-29-sriraam-usc/",
        "teaser": null
      },{
        "title": "Our work on Interactive Transfer Learning in Relational Domains got accepted at KUIN Springer Journal, 2020",
        "excerpt":"Our work on Interactive Transfer Learning in Relational Domains got accepted at KUIN Springer Journal, 2020. Refer to the Publication page for the paper. Congratulations to the authors!  ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-02-29-nandini-journal/",
        "teaser": null
      },{
        "title": "Congratulations to Dr. Dhami for successfully defending his thesis",
        "excerpt":"Dr. Devendra Singh Dhami defended his dissertation on “Learning effective machine learning methods for health care” on 30th March, 2020.   ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-04-18-ddhami-dissertation/",
        "teaser": null
      },{
        "title": "Paper on Lifted Variational Inference accepted to IJCAI 2020",
        "excerpt":"Paper on Lifted Variational Inference accepted to IJCAI 2020. Congratulations to all the authors.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-05-08-ijcai2020-acceped/",
        "teaser": null
      },{
        "title": "Video of Dr. Natarajan's distinguished lecture at University of South Carolina is now available on YouTube.",
        "excerpt":"Video of Dr. Natarajan’s distinguished lecture at University of South Carolina is now available on YouTube.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-05-23-usc-lecture-video/",
        "teaser": null
      },{
        "title": "StARLinG Lab YouTube Channel is Live",
        "excerpt":"Video lectures, talks, and tutorials by members of StARLinG Lab are on our StARLinG Lab UTD Youtube Channel.   Like, share, comment, and subscribe!  ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-06-24-new-youtube-channel/",
        "teaser": null
      },{
        "title": "Professor Sriraam Natarajan's invited talk at US-Serbia & West Balkan Data Science Workshop, Serbia 2018, is now available online",
        "excerpt":"Prof. Sriraam Natarajan’s invited talk at US-Serbia &amp; West Balkan Data Science Workshop, Serbia 2018, is now available online  ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-06-24-serbia-talk-video/",
        "teaser": null
      },{
        "title": "Congratulations to Dr. Das for successfully defending her thesis",
        "excerpt":"Dr. Srijita Das defended her dissertation on “Sample Efficient Cost-Aware Active Learning” on 31st August, 2020.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-09-30-sdas-dissertation/",
        "teaser": null
      },{
        "title": "Professor Natarajan was the keynote speaker at KiML 2020 workshop @ KDD 2020",
        "excerpt":"Professor Sriraam Natarajan gave a keynote talk at the KDD Workshop on Knowledge-infused Mining and Learning (KiML) 2020.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-09-15-KiML-keynote/",
        "teaser": null
      },{
        "title": "Congratulations to Devendra for winning the best student paper award at KiML 2020 co-located with KDD 2020 ",
        "excerpt":"Devendra’s paper on Knowledge Intensive Learning of GANs was awarded the best student paper award at KDD Workshop on Knowledge-infused Mining and Learning (KiML) 2020. Congratulations Dev!!  ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-09-15-kiml-best-paper/",
        "teaser": null
      },{
        "title": "Professor Natarajan's talk from RBCDSAI Summit is now available on youtube",
        "excerpt":"Watch Professor Natarajan’s talk from Indian Institute of Technology Madras Robert Bosch Centre for Data Science and Artificial Intelligence (RBCDSAI) summit here.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-09-15-talk-at-rbcdsai/",
        "teaser": null
      },{
        "title": "Paper on A Clustering based Selection Framework for Cost Aware and Test-time Feature Elicitation accepted to CoDS-COMAD 2021",
        "excerpt":"Paper on A Clustering based Selection Framework for Cost Aware and Test-time Feature Elicitation accepted to CoDS-COMAD 2021. Congratulations to all the authors.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-09-29-CoDSCOMAD-accepted1/",
        "teaser": null
      },{
        "title": "Paper on Human-Guided Learning of Column Networks: Knowledge Injection for Relational Deep Learning accepted to CoDS-COMAD 2021",
        "excerpt":"Paper on Human-Guided Learning of Column Networks: Knowledge Injection for Relational Deep Learning accepted to CoDS-COMAD 2021. Congratulations to all the authors.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-09-29-CoDSCOMAD-accepted2/",
        "teaser": null
      },{
        "title": "Congratulations to Dr. Ramanan for successfully defending her thesis",
        "excerpt":"Dr. Nandini Ramanan defended her dissertation on “Effective and Efficient Structure Learning of Graphical Models” on 5th October, 2020.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-10-05-nandini-dissertation/",
        "teaser": null
      },{
        "title": "Congratulations to Dr. Kaur for successfully defending her thesis",
        "excerpt":"Dr. Navdeep Kaur defended her dissertation on “Efficient Combination of Neural and Symbolic Learning for Relational Data” on 6th November, 2020.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2020-11-06-navdeep-dissertation/",
        "teaser": null
      },{
        "title": "Paper on Relational Boosted Bandits accepted to AAAI 2021",
        "excerpt":"Paper on Relational Boosted Bandits accepted to AAAI 2021. Congratulations to all the authors.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2021-02-04-AAAI-accepted/",
        "teaser": null
      },{
        "title": "Professor Natarajan's talk from RBCDSAI Distinguished Fellow Colloquium Series - Coffee Shop Banter is now available on youtube",
        "excerpt":"Watch Professor Natarajan and Professor Kersting discuss Symbolic and Deep Learning and what it means for the future of AI Robert Bosch Centre for Data Science and Artificial Intelligence (RBCDSAI) Distinguished Fellow Colloquium Series here.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2021-05-03-talk-at-RBCDSAI/",
        "teaser": null
      },{
        "title": "Two papers presented at Artificial Intelligence in Medicine (AIME) Conference",
        "excerpt":"Two papers are being presented at the 19th International Conference on Artificial Intelligence in Medicine AIME this week:      Devendra Singh Dhami, Siwen Yan, Gautam Kunapuli, David Page, Sriraam Natarajan. Predicting Drug-Drug Interactions from Heterogeneous Data: An Embedding Approach Springer Link   Athresh Karanam, Alexander L. Hayes, Harsha Kokel, David M. Haas, Predrag Radivojac, Sriraam Natarajan. A Probabilistic Approach to Extract Qualitative Knowledge for Early Prediction of Gestational Diabetes Springer Link   Qualitative Knowledge Extraction   Spotlight video for the Qualitative Knowledge Extraction (QuaKE) paper:      Citing   BibTeX for “Predicting DDIs”:   @inproceedings{dhami2021predictingdrug,   author=\"Dhami, Devendra Singh and Yan, Siwen and Kunapuli, Gautam and Page, David and Natarajan, Sriraam\",   editor=\"Tucker, Allan and Henriques Abreu, Pedro and Cardoso, Jaime and Pereira Rodrigues, Pedro and Ria{\\~{n}}o, David\",   title=\"Predicting Drug-Drug Interactions from Heterogeneous Data: An Embedding Approach\",   booktitle=\"Artificial Intelligence in Medicine\",   year=\"2021\",   publisher=\"Springer International Publishing\",   address=\"Cham\",   pages=\"252--257\",   isbn=\"978-3-030-77211-6\",   url=\"https://doi.org/10.1007/978-3-030-77211-6_28\" }   BibTeX for “Qualitative Knowledge Extraction”:   @inproceedings{karanam2021probabilistic,   author=\"Karanam, Athresh and Hayes, Alexander L. and Kokel, Harsha and Haas, David M. and Radivojac, Predrag and Natarajan, Sriraam\",   editor=\"Tucker, Allan and Henriques Abreu, Pedro and Cardoso, Jaime and Pereira Rodrigues, Pedro and Ria{\\~{n}}o, David\",   title=\"A Probabilistic Approach to Extract Qualitative Knowledge for Early Prediction of Gestational Diabetes\",   booktitle=\"Artificial Intelligence in Medicine\",   year=\"2021\",   publisher=\"Springer International Publishing\",   address=\"Cham\",   pages=\"497--502\",   isbn=\"978-3-030-77211-6\",   url=\"https://doi.org/10.1007/978-3-030-77211-6_59\" }  ","categories": ["news"],
        "tags": [],
        "url": "/news/2021-06-14-aime-accepted-papers/",
        "teaser": null
      },{
        "title": "Two publications at ICAPS 2021",
        "excerpt":"Our Relational Planning and RL framework (RePReL) was accepted in the Planning and Learning Track, and our Human-guided Collaborative Problem Solving system was accepted in Systems Demonstration Track.           RePReL: Integrating Relational Planning and Reinforcement Learning for Effective Abstraction  Harsha Kokel, Arjun Manoharan, Sriraam Natarajan, Balaraman Ravindran, Prasad Tadepalli.  Find more details in the blog here            Human-guided Collaborative Problem Solving: A Natural Language based Framework  Harsha Kokel, Mayukh Das, Rakibul Islam, Julia Bonn, Jon Cai, Soham Dan, Anjali Narayan-Chen, Prashant Jayannavar, Janardhan Rao Doppa, Julia Hockenmaier, Sriraam Natarajan,  Martha Palmer,  Dan Roth.  Checkout the video here      ","categories": ["news"],
        "tags": [],
        "url": "/news/2021-07-22-ICAPS21/",
        "teaser": null
      },{
        "title": "Interventional Sum-Product Networks: Causal Inference with Tractable Probabilistic Models wins the best poster award at EEML 2021",
        "excerpt":"The poster for Interventional Sum-Product Networks: Causal Inference with Tractable Probabilistic Models has won the best paper award at Eastern European Machine Learning Summer School (EEML) 2021. Congratulations to the authors!  ","categories": ["news"],
        "tags": [],
        "url": "/news/2021-08-11-best-poster-award-iSPNs/",
        "teaser": null
      },{
        "title": "Prof. Sriraam Natarajan talks about AI on The Tech Between Us Podcast",
        "excerpt":"Prof. Sriraam Natarajan talks about AI on The Tech Between Us Podcast. The full version of the podcast can be found here.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2021-08-25-AI-podcast/",
        "teaser": null
      },{
        "title": "Prof. Sriraam Natarajan gives a keynote talk at IIT Madras",
        "excerpt":"Prof. Sriraam Natarajan gives a keynote talk about the research we do at StARLinG Lab. The full version of the podcast can be found here.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2022-01-15-Keynote-talk/",
        "teaser": null
      },{
        "title": "Prof. Sriraam Natarajan's newest RBDSCAI Fellow! Workshop talk and colloquium is now available to watch on Youtube",
        "excerpt":"Sriraam Natarajan was felicitated as the newest fellow at IIT Madras’ Robert Bosch Center for Data Science and AI (RBDCSAI)      Prof. Natarajan gives a workshop talk on “Uncertainty, Bayesian Networks and Probabilistic Inference” and a colloquium on “Human-allied Learning of Symbolic Deep Models”. The links for the recordings can be found below.   Workshop day 1 Workshop day 2 Colloquium  ","categories": ["news"],
        "tags": [],
        "url": "/news/2022-04-05-Keynote-talk/",
        "teaser": null
      },{
        "title": "Our paper on extending RePReL for hybrid data is accepted at FUSION'22",
        "excerpt":"We recently extended the RePReL framework for domains with a combination of structured and unstructured data. This work is now accepted at IEEE 25th International Conference on Information Fusion (FUSION’22). Read more about it in a short post here.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2022-05-26-HDREPREL/",
        "teaser": null
      },{
        "title": "Our paper titled Association of Genetic Predisposition and Physical Activity With Risk of Gestational Diabetes in Nulliparous Women has been accepted at JAMA Network Open 2022",
        "excerpt":"Our paper on “Association of Genetic Predisposition and Physical Activity With Risk of Gestational Diabetes in Nulliparous Women” has been accepted at JAMA Network Open 2022. This paper attempts to answer the question “Are genetic predisposition to diabetes and physical activity in early pregnancy cooperatively associated with risk of gestational diabetes (GD) among nulliparous women?” and finds that physical activity in early pregnancy is associated with reduced risk of GD and reversal of excess risk in genetically predisposed individuals, and PRS may have utility in identifying women for targeted interventions. The paper can be found here.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2022-09-02-JAMA-2022/",
        "teaser": null
      },{
        "title": "Our paper titled Active Feature Acquisition via Human Interaction in Relational domains has been accepted to CODS-COMAD 2023",
        "excerpt":"Our paper titled “Active Feature Acquisition via Human Interaction in Relational domains” has been accepted to the 6th Joint International Conference on Data Science &amp; Management of Data!  ","categories": ["news"],
        "tags": [],
        "url": "/news/2022-09-16-CODS-2022/",
        "teaser": null
      },{
        "title": "Our paper on improving compute efficiency in domain adaptation tasks using data subset selection has been accepted to NeurIPS 2022",
        "excerpt":"Our paper titled “Orient: Submodular Mutual Information Measures for Data Subset Selection under Distribution Shift” has been accepted to NeurIPS 2022! We leverage submodular mutual information (SMI) measures to select source data similar to target data in order to improve compute efficiency.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2022-09-16-NeurIPS-2022/",
        "teaser": null
      },{
        "title": "Our paper titled Exploiting Domain Knowledge as Causal Independencies in Modeling Gestational Diabetes has been accepted to PSB 2023",
        "excerpt":"Our paper titled “Exploiting Domain Knowledge as Causal Independencies in Modeling Gestational Diabetes” has been accepted to the Pacific Symposium on Biocomputing (PSB) 2023! We leverage expert domain knowledge provided in the form of qualitative influences and causal independencies to learn an explainable and interpretable model for predicting gestational diabetes.  ","categories": ["news"],
        "tags": [],
        "url": "/news/2022-09-16-PSB-2022/",
        "teaser": null
      },{
        "title": "Congratulations to Dr. Kokel for successfully defending her thesis",
        "excerpt":"Dr. Harsha Kokel defended her dissertation on “Beyond Data: Efficient Knowledge-Guided Learning for Sparse and Structured Domains” on 24th February, 2023. Congratulations and we wish you all the best with your future endeavors!  ","categories": ["news"],
        "tags": [],
        "url": "/news/2023-02-25-harsha-dissertation/",
        "teaser": null
      },{
        "title": "Two Papers accepted to UAI 2023!",
        "excerpt":"We have two papers accepted to UAI 2023. Congrats to Sahil, Saurabh and our wonderful collaborators Kristian Kersting and Vibhav Gogate! Check out the papers here:      Probabilistic Flow Circuits: Towards Unified Deep Models for Tractable Probabilistic Inference            Sahil Sidheekh, Kristian Kersting, Sriraam Natarajan           Knowledge Intensive Learning of Cutset Networks            Saurabh Mathur, Vibhav Gogate, Sriraam Natarajan           ","categories": ["news"],
        "tags": [],
        "url": "/news/2023-05-08-uai-papers/",
        "teaser": null
      },{
        "title": "Prof. Sriraam Natarajan selected as Hessian.ai Fellow!",
        "excerpt":"On Thursday, July 20th (8AM CST), in his visit to TU Darmstadt, Dr. Sriraam Natarajan was felicitated as hessian.AI fellow, after his talk on Neurosymbolic Learning via the integration of (Relational) Planning and (Deep) RL.      Prof. Natarajan was also selected as a fellow at IIT Madras’ Robert Bosch Center for Data Science and AI in April of 2022. Check out that article here   To learn more on the challenges of constructing a two-level system and the importance of the interface between different components, click on the link below.   Zoom Link  ","categories": ["news"],
        "tags": [],
        "url": "/news/2023-07-20-Hessian-ai-Expert-Talk/",
        "teaser": null
      },{
        "title": "Alexander L. Hayes: \"Relation Extraction for Relational Learning\"",
        "excerpt":"My work is mostly centered on relational approaches to natural language processing. Approaches such as word2vec have increased interest in the context that words appear in beyond the bag-of-words model. The approach we focus on models the relations between words, document structure, and word attributes like part-of-speech; which can provide powerful insights for classification tasks.   This work has been applied to SEC Form S-1 Documents and known drug side effect information from the openFDA database. Work for the latter can be found on GitHub.   I am involved in the Natural Language Processing project.  ","categories": ["research-highlights"],
        "tags": [],
        "url": "/research-highlights/alexander-hayes/",
        "teaser": null
      },{
        "title": "Devendra Singh Dhami: \"Structure-based Discovery of Drug-Drug Interactions\"",
        "excerpt":"I am currently working on the problem of drug-drug interactions using structural data from the molecules and chemical reaction pathways which exist between interacting medications. Most of the work in this area has existed in the area of natural language processing where interactions are inferred through mining the medical literature. We take a non-conventional approach by considering the structure of the drugs themselves.   This method not only predicts interactions between different drugs but discovers new potential interactions, thereby exploiting deeper structures and drug features.   Relevant publications:      Devendra Singh Dhami, Gautam Kunapuli, Mayukh Das, David Page, and Sriraam Natarajan, Drug-Drug Interaction Discovery: Kernel Learning from Heterogeneous Similarities (Under review in IEEE Conference on Connected Health: Applications,  Systems, and Engineering Technologies (CHASE), 2018)  ","categories": ["research-highlights"],
        "tags": [],
        "url": "/research-highlights/devendra-dhami/",
        "teaser": null
      },{
        "title": "Mayukh Das: \"Human-Allied Problem Solving and Planning\"",
        "excerpt":"Knowledge-augmented approaches to sequential decision-making try to alleviate the limitations of data-driven techniques caused by noise, stochasticity and asymmetry of knowledge. They leverage rich knowledge assimilated by domain experts through years of experience to learn better behaviour. Recent years have witnessed a major research thrust in this direction and our group is an active contributor to this cause; focused on building human-in-the-loop frameworks for representation and elicitation of knowledge from, potentially multiple, expert(s) for sequential decision-making (RL), planning and prediction.   My research focuses on real-time/active elicitation of human knowledge, at varying levels of generality, for guided planning and problem-solving. Specifically, we address ‘asymmetry of knowledge’ where an AI planning system may have access to certain resources and vast computational power but may lack the necessary knowledge to prioritize among critical tasks. Human experts understand such priorities implicitly and we leverage that to generate better plans. Our DARPA-funded project “Communicating with Computers” has motivated research towards human-AI collaborative planning systems where both humans and AI agents solve problems together and learn from each other. Humans will teach new concepts, the agents will seek guidance when uncertain, and both will grow to augment one another. Part of my research also involves scaling Probabilistic Logic Models via approximation.   I am involved in the Communicating with Computers project.  ","categories": ["research-highlights"],
        "tags": [],
        "url": "/research-highlights/mayukh-das/",
        "teaser": null
      },{
        "title": "Nandini Ramanan: \"Precision Health\"",
        "excerpt":"We aim to bridge the gap between the machine learning community and the existing applications to healthcare. Our work involves developing efficient algorithms and probabilistic models using real-world data and expert knowledge. We employ state-of-the-art optimization techniques to understand the progression of disease symptoms and comorbidities over time. So far we have focused on customized probabilistic models in the context of Postpartum depression (PPD), cardiovascular disease, Alzheimer’s disease, and Parkinson’s disease; but our method is generalizable to summarizing patients with greater exactness to allow us to move toward personalized disease management strategies.   ","categories": ["research-highlights"],
        "tags": [],
        "url": "/research-highlights/nandini-ramanan/",
        "teaser": null
      },{
        "title": "Navdeep Kaur: \"Relational Connectionist Models\"",
        "excerpt":"Relational Connectionist Models bring together the complementary strengths of scalability and interpretability. We have considered learning Boltzmann machines for relational data; specifically generating features from lifted random walks that form the observed features of Boltzmann machines and produce efficient models when tested on six relational datasets.   Furthermore, we look at lifted relational connectionist models learned with the help of parameter tying and combining rules. This work is ongoing.   References:     Navdeep Kaur, Gautam Kunapuli, Tushar Khot, William Cohen, Kristian Kersting and Sriraam Natarajan, “Relational Restricted Boltzmann Machines: A Probabilistic Logic Learning Approach”, ILP 2017.  ","categories": ["research-highlights"],
        "tags": [],
        "url": "/research-highlights/navdeep-kaur/",
        "teaser": null
      },{
        "title": "Srijita Das: \"Active Learning from Minimum Information\"",
        "excerpt":"My research focus has been on learning robust machine learning models from small pool of complete instances. A practical application of this has been in finding potential recruits given the availability of a small number of patients with complete information from a clinical study or survey data. This has been applied to real world medical problems like Parkinson’s, Alzheimer’s disease,rare disease and Post partum depression where a small number of patients with the complete feature set remains available and the goal is to identify potential recruits having easily available partial feature set for better disease prediction. My big goal is to build an intelligent decision support agent that can assist physicians in making better healthcare decisions.  ","categories": ["research-highlights"],
        "tags": [],
        "url": "/research-highlights/srijita-das/",
        "teaser": null
      },{
        "title": "CS7301",
        "excerpt":"If you are having difficulty viewing this page on mobile, access the document here     ","categories": [],
        "tags": [],
        "url": "/courses/cs7301/",
        "teaser": null
      },{
        "title": "Siwen Yan: \"Efficient Learning of Fair Models using Human Guidance and Privileged Information\"",
        "excerpt":"My recent research is centered around developing tree-based ensemble models and utilizing large language models. To enhance the performance and fairness of our models at deployment, we leverage privileged information as guidance in XGBoost and incorporate human advice into relational regression trees using functional gradient boosting for more effective and efficient learning. Additionally, we employ LLMs to discover logical rules and generate missing data. In my previous works, I have focused on various areas including Statistical Relational AI and Healthcare, Graph Neural Networks, and Reinforcement Learning.   ","categories": ["research-highlights"],
        "tags": [],
        "url": "/research-highlights/siwen-yan/",
        "teaser": null
      },{
        "title": "Saurabh Mathur: \"Knowledge Intensive Learning of Cutset Networks\"",
        "excerpt":"My recent work deals with learning probabilistic models using expert advice. Specifically, we focus on a class of probabilistic models called cutset networks which allow fast inference. Our algorithm learns concise and accurate cutset networks from data using expert advice in the form of monotonic influence statements. Since cutset networks are interpretable, the learned models can help the expert understand relations between risk factors for medical conditions like Gestation Diabetes.  ","categories": ["research-highlights"],
        "tags": [],
        "url": "/research-highlights/saurabh-mathur/",
        "teaser": null
      },{
        "title": "Communicating with Computers",
        "excerpt":"The vision of this project (under the DARPA CwC grant) is to build smart machines that enable “Artificial Intelligence” agents and humans to interact seamlessly, make decisions and solve problems together and learn from each other as well as complement each others capabilities. An over-arching framework for human-machine collaboration involving multiple human (non)experts and machines through varied modalities and protocols of interaction. Concept learning/teaching, collaborative planning, domain transfer/extension and higher level knowledge induction are some of the avenues that we are presently investigating.      Publications:     Das, M., Odom, P., Islam, M.R., Doppa, J., Roth, D., &amp; Natarajan, S., “Preference- Guided Planning: An Active Elicitation Approach”, International Conference on Autonomous Agents and Multiagent Systems (AAMAS) 2018.   Das, M., Islam, M.R., Doppa, J.R., Roth, D., &amp; Natarajan, S., “Active Preference Elicitation for Planning”, Human-Machine Collaborative Learning workshop(@ AAAI) 2017.   Narayan-Chen A., Graber C., Das M., Islam M.R., Dan S., Natarajan S., Doppa J.R., Hockenmaier J., Palmer M., Roth D., “Towards Problem Solving Agents that Communicate and Learn.” Workshop on Language Grounding for Robotics at ACL 2017.   Das, M., &amp; Odom, P., &amp; Islam, M.R., &amp; Doppa, J., &amp; Roth, D., &amp; Natarajan, S., “Preference Guided Planning: An Active Elicitation Approach”, International Conference on Autonomous Agents and Multiagent Systems (AAMAS) 2018.   Das, M., &amp; Odom, P., &amp; Islam, M.R., &amp; Doppa, J., &amp; Roth, D., &amp; Natarajan, S., “Planning with actively eliciting preferences”, Knowledge-Based Systems 2019   Das, M., &amp; Ramanan, N., &amp; Doppa, J.R., &amp; Natarajan, S., “One-Shot Induction of Generalized Logical Concepts via Human Guidance”, Workshop on Statistical Relational AI (StarAI) 2020.   Das, M., &amp; Ramanan, N., &amp; Doppa, J.R., &amp; Natarajan, S., “Few-Shot Induction of Generalized Logical Concepts via Human Guidance”, Computational Intelligence in Robotics, Frontiers in Robotics and AI 2020.   Kumaraswamy, R., &amp; Ramanan, N., &amp; Odom, P., &amp; Natarajan, S., “Interactive Transfer Learning in Relational Domains”, KUIN Springer Journal 2020   Kokel, H., &amp; Das, M., &amp; Islam, R., &amp; Bonn, J., &amp; Cai, J., &amp; Dan, S., &amp; Narayan-Chen, A., &amp; Jayannavar, P., &amp; Doppa, J.R., &amp; Hockenmaier, J., &amp; Natarajan, S., &amp; Palmer, M., &amp; Roth, D., “Human-guided Collaborative Problem Solving: A Natural Language based Framework”, Thirty First International Conference on Automated Planning and Scheduling (ICAPS) 2021.     ","categories": ["HAAI"],
        "tags": [],
        "url": "/projects/communicating-with-computers/",
        "teaser": "/assets/images/project/cwc.png"
      },{
        "title": "Adverse <nobr>Drug-Events</nobr>",
        "excerpt":"With the increase in the number of drug discoveries and thus the data associated with each drug, detecting when these new drugs (and the old drugs) with react to something (can be other drugs, food etc.) adversely has become a very important problem inside bioinformatics. This problem of adverse drug events (ADE) is now being studied in the field of machine learning as due to the large amount of data available, it will be simpler for the machines to detect hidden patterns that might result in these ADEs.   Our previous work in this area was specifically in natural language processing (NLP) where we looked at PubMed articles and extracted relational features and constucted a model on these features. The model was supplemented by expert advice that was employed to identify text patterns in articles that might suggest the presence of an ADE(s). Recently we have focused on the problem of drug-drug interactions by looking at the molecular structure of the drugs and the interaction pathways associated with each drug. The goal is to create an adverse drug events network that can be easily mined in the future for detecting and discovering any ADEs.   Publications:      Devendra Singh Dhami, Gautam Kunapuli, Mayukh Das, David Page and Sriraam Natarajan. “Drug-Drug Interaction Discovery: Kernel Learning from Heterogeneous Similarities” IEEE Conference on Connected Health: Applications, Systems, and Engineering Technologies (CHASE), 2018.   Phillip Odom, Vishal Bangera, Tushar Khot, David Page and Sriraam Natarajan. “Extracting Adverse Drug Events from Text using Human Advice” Artificial Intelligence in Medicine (AIME) 2015.  ","categories": ["precision-health"],
        "tags": [],
        "url": "/projects/adverse-drug-events/",
        "teaser": "/assets/images/project/ade.jpg"
      },{
        "title": "Boosting",
        "excerpt":"Statistical Relational Learning models combine the representational power of  first-order logic with the ability of probability theory to handle uncertainity. While these models are  attractive from modelling perspective, learning them is computationally intensive. Existing approaches so far focused on the task of learning the so-called parameters where the rules are provided by the human expert and the data is merely used to learn the parameters. Our gradient boosted approach, instead relies on the intuition that learning a set of weak partial rules can be much easier than finding a single, highly accurate model. We learns both the rules and the parameters of the rules simultaneously. Our approach is capable of learning different types of models (Markov Logic Networks, Relational Dependency networks as well recently succesful Relational Logistic Regression), handling modeling of hidden data, learning with preferences from humans, scaling with large amounts of data by approximate counting and modeling temporal data.      Publications     Sriraam Natarajan, Tushar Khot, Kristian Kersting, Bernd Gutmann and Jude Shavlik. “Boosting Relational Dependency Networks”, International Conference on Inductive Logic Programming (ILP) 2010.   Tushar Khot, Sriraam Natarajan, Kristian Kersting, and Jude Shavlik. “Learning Markov Logic Networks via Functional Gradient Boosting”, International Conference in Data Mining (ICDM) 2011.   Sriraam Natarajan, Saket Joshi, Prasad Tadepalli, Kristian Kersting, and Jude Shavlik. “Imitation Learning in Relational Domains: A Functional-Gradient Boosting Approach”, International Joint Conference in AI (IJCAI) 2011.   Phillip Odom, Tushar Khot, Reid Porter, and Sriraam Natarajan, “Knowledge-Based Probabilistic Logic Learning”, Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI), 2015.   Shuo Yang, Tushar Khot, Kristian Kersting and Sriraam Natarajan, “Learning Continuous-Time Bayesian Networks in Relational Domains: A Non-Parametric Approach”, 30th AAAI Conference on Artificial Intelligence (AAAI), 2016.   Shuo Yang, Tushar Khot, Kristian Kersting, Gautam Kunapuli, Kris Hauser and Sriraam Natarajan, “Learning from Imbalanced Data in Relational Domains: A Soft Margin Approach”, International Conference on Data Mining (ICDM), 2014.   Nandini Ramanan, Gautam Kunapuli, Tushar Khot, Bahare Fatemi, Seyed Mehran Kazemi, David Poole, Kristian Kersting and Sriraam Natarajan, “Structure Learning for Relational Logistic Regression : An Ensemble Approach”, International Conference on Principles of Knowledge Representation and Reasoning (KR), 2018.  ","categories": ["efficient-starai"],
        "tags": [],
        "url": "/projects/boosting/",
        "teaser": "/assets/images/project/rfgb.png"
      },{
        "title": "Natural Language Processing",
        "excerpt":"Natural language processing is a hard task for traditional machine learning models, since they tend to ignore the relationships between the sentences, words, and text in documents. Because of this, they require careful feature construction and often make assumptions about conditional independences between all of the elements.   Even then, typically these features do not scale with large amounts of natural language data. Therefore, methods which can extract relations from text and exploit these relations during learning and inference will reduce the need for careful feature construction, and also scale better into larger data sets.   Models learned using relational machine learning methods can be used to create robust and interpretable predictions. In these projects, BoostSRL has been applied to extracting relations from clinical data, adverse drug event relations, and financial documents.   Publications:      Sriraam Natarjan, Ameet Soni, Anurag Wazalwar, Dileep Viswanathan, and Kristian Kersting. “Deep Distant Supervision: Learning Statistical Relational Models for Weak Supervision in Natural Language Processing.” Morik Festschrift, LNAI 9580 2016.   Sriraam Natarajan, Vishal Bangera, Tushar Khot, Jose Picado, Anurag Wazalwar, Vitor Santos Costa, David Page, and Michael Caldwell. “Markov Logic Networks for Adverse Drug Event Extraction from Text.” Knowledge and Information Systems (KAIS), 2016.   Ameet Soni, Dileep Viswanathan, Jude Shavlik, and Sriraam Natarajan. “Learning Relational Dependency Networks for Relation Extraction.” International Conference on Inductive Logic Programming (ILP), 2016.   Phillip Odom, Vishal Bangera, Tushar Khot, David Page, and Sriraam Natarajan. “Extracting Adverse Drug Events from Text using Human Advice.” Artificial Intelligence in Medicine (AIME), 2015.   Sriraam Natarajan, Jose Picado, Tushar Khot, Kristian Kersting, Christopher Re and Jude Shavlik. “Using Commonsense Knowledge to Automatically Create (Noisy) Training Examples from Text.” International Workshop on Statistical Relational AI, 2012.   Tushar Khot, Siddharth Srivastava, Sriraam Natarajan, and Jude Shavlik. “Learning Relational Structure for Temporal Relation Extraction.” International Workshop on Statistical Relational AI, 2012.      ","categories": ["applications"],
        "tags": [],
        "url": "/projects/nlp/",
        "teaser": "/assets/images/project/nlp.jpg"
      },{
        "title": "Closing the loop",
        "excerpt":"The crux of human-allied smart machines lie in their capability to communicate back to the human, be it for sharing knowledge, for querying additional guidance/knowledge or just for mere inconsequential conversation.   We develop AI agents that are able to infer dearth of knowledge in specific parts of a task (knowing-what-it-knows), communicate and explain to human (non-)expert(s) through various modalities, such as text, visuals, gestures etc., and actively seek advice/preference/guidance accordingly (knowing-when-to-ask). Queries and explanations from to the agent to the human are hierarchical and are at varying levels of generality, making it extremely intuitive and easy for the human ally to effectively help the agent make better choices and learn better models. We build such frameworks both for predictive modeling and sequential decision making.   Publications:     Das, M., Odom, P., Islam, M.R., Doppa, J., Roth, D., &amp; Natarajan, S., “Preference- Guided Planning: An Active Elicitation Approach”, International Conference on Autonomous Agents and Multiagent Systems (AAMAS) 2018.   Alexander L. Hayes, Mayukh Das, Phillip Odom, Sriraam Natarajan. “User Friendly Automatic Construction of Background Knowledge: Mode Construction from ER Diagrams.”, Knowledge Capture Conference 2017.   Odom, P., &amp; Natarajan, S., “Active Advice Seeking for Inverse Reinforcement Learning.”, International Conference on Autonomous Agents and Multiagent Systems (AAMAS) 2016.   Odom, P., &amp; Natarajan, S., “Actively Interacting with Experts: A Probabilistic Logic Approach.”, European Conference on Machine Learning and Principles of Knowledge Discovery in Databases (ECMLPKDD) 2016.   Odom, P., Kumaraswamy, R., Kersting, K., &amp; Natarajan, S., “Learning through Advice-Seeking via Transfer.”, International Conference on Inductive Logic Programming (ILP) 2016.  ","categories": ["HAAI"],
        "tags": [],
        "url": "/projects/closing-the-loop/",
        "teaser": "/assets/images/project/ctl3.jpg"
      },{
        "title": "Alzheimer's and Parkinson's Prediction",
        "excerpt":"Progressive neurological diseases are untreatable and thus the only way of preventing these disease is to detect them early. This early detection is difficult too since the signs of these disease can show up very late in any lab reports, MRIs and/or behaviour of the patient. The presence of large scale studies and databases such as the Alzheimer’s Disease Neuroimaging Initiative (ADNI) and the Parkinson’s Progression Markers Initiative (PPMI) has given hope to the efforts of machine learning researchers to contribute towards an early detection system. Our lab works with the different modalities of data in these studies with the aim of developing machine learning algorithms that can aid in both the classification and detection of patients with/or developing these diseases.   Publications:      Devendra Singh Dhami, Ameet Soni, David Page, Sriraam Natarajan, “Identifying Parkinson’s Patients : A Functional Gradient Boosting Approach”, Artificial Intelligence in Medicine (AIME) (2017).   Sriraam Natarajan, Baidya Saha, Saket Joshi, Adam Edwards, Tushar Khot, Elizabeth M. Davenport, Kristian Kersting, Christopher T. Whitlow and Joseph A. Maldjian. “Relational Learning helps in Three-way Classification of Alzheimer Patients from Structural Magnetic Resonance Images of the Brain” International Journal of Machine Learning and Cybernetics, Springer 2013.   Sriraam Natarajan, Saket Joshi, Baidya N. Saha, Adam Edwards, Elizabeth Moody, Tushar Khot, Kristian Kersting, Christopher T. Whitlow and Joseph A. Maldjian. “A Machine Learning Pipeline for Three-way Classification of Alzheimer Patients from Structural Magnetic Resonance Images of the Brain” IEEE Conference on Machine Learning and Applications (ICMLA), 2012.  ","categories": ["precision-health"],
        "tags": [],
        "url": "/projects/alzheimer-parkinson-prediction/",
        "teaser": "/assets/images/project/study.png"
      },{
        "title": "Cardia",
        "excerpt":"The Coronary Artery Risk Development in Young Adults (CARDIA) study is to identify the risk factors in early life that have influence on the development of clinical CVD in later life. Longitudinal study of 5115 subjects started in 1985-86 in 4 study centers in the US includes various clinical and physical measurements and in-depth questionnaires about sociodemographic background like psychosocial issues, smoking, drinking habits etc. We in our work used only basic socio-demographic information and health behaviour information for early prediction of incidence of CAC-levels using interpretative Dynamic Bayesian Networks. Our results demonstrate the superiority of behavioral features over other clinical measurements in the prediction task.   Publications:     Yang, S., Kersting, K., Terry, G., Carr, J., &amp; Natarajan, S., “Modeling Coronary Artery Calcification Levels From Behavioral Data in a Clinical Study”, Artificial Intelligence in Medicine (AIME) 2015.  ","categories": ["precision-health"],
        "tags": [],
        "url": "/projects/cardia/",
        "teaser": "/assets/images/project/cardia.PNG"
      },{
        "title": "Automatic Operation Notes",
        "excerpt":"With the explosion in the volume of medical information, there is the promise that patient care can be managed more precisely based on automatic extraction and evaluation of patients’ medical information available in the electronic health record (EHR).   One challenge is the limited ability to evaluate the rich information present in free text format, such as in notes written by health care providers.  We are starting to remedy this problem by examining a large set of operative notes describing a common surgical procedure, aiming to elicit a context free grammar that might be used to automatically generate such notes.   Research in this direction may help reduce the time doctors spend on such tasks, giving them more time to focus on their patients.  ","categories": ["precision-health"],
        "tags": [],
        "url": "/projects/automatic-operation-notes/",
        "teaser": "/assets/images/project/automaticOperationNotes.png"
      },{
        "title": "Active elicitation of lab tests",
        "excerpt":"The high level idea of this project is to actively elicit features for the most useful instances in order to build a sample efficient predictive model. We consider the problem of active feature elicitation in which, given some examples with all the features (say, the full Electronic Health Record), and many examples with some of the features (say, demographics), the goal is to identify the set of examples for whom more information (say, lab tests) needs to be collected. The assumption for this problem setting is that certain features like demographic data is cheap and easily available for most patients. However, expensive or cumbersome features like information about various lab tests are available for less number of patients and we want to elicit expensive features only for the patients who would give us maximum information about the target. We develop a novel unifying framework to solve this problem and demonstrate the efficacy of our approach on 4 real world prediction problems like Alzheimer’s disease, Parkinson’s disease, Postpartum depression and Rare disease identification. We are also exploring human-in-the-loop approaches within this setting and questions related to different feature acquisition for different subpopulation.     Publications:     Natarajan, S., Das, S., Ramaman, N., Kunapuli, G., &amp; Radivojac, P., “Whom Should I Perform the Lab Test on Next? An Active Feature Elicitation Approach”, International Joint Conference on Artificial Intelligence (IJCAI) 2018.  ","categories": ["precision-health"],
        "tags": [],
        "url": "/projects/active-elicitation/",
        "teaser": "/assets/images/project/AFE2.png"
      },{
        "title": "Survey data",
        "excerpt":"Survey data can be a rich source of information about non-clinical risk factors that could potentially have a considerable influence on the medical condition to be predicted. We aim to analyze and identify how these non-clinical risk factors interact and potentially influence the underlying condition to enable not just efficient diagnosis, but also treatment planning and behavior modification, ultimately leading to a positive impact on public health. We in our lab adapted the gradient boosting framework to learn from these non-clinical factors that are identified as useful in diagnosis of Post Partum Depression and Rare Disease. Empirical evaluation demonstrates the superiority of our proposed approach over existing machine learnings methods. To this account we also extended our approach to learn interpretable models. We developed a novel hybrid Bayesian network learning algorithm that can effectively learn Bayesian networks (possibly causal) from these survey data. Our approach scales to a large number of variables by learning an approximate joint distribution using dependency networks. This DN is turned into a BN by pruning edges as a result of mutual independency tests. We demonstrated, in two real-world problems, of predicting rare diseases and PPD from survey data, that our proposed algorithm learns interpretable and meaningful models.   Publications:      Haley MacLeod, Shuo Yang, Kim Oakes, Kay Connelly and Sriraam Natarajan, “Identifying Rare Diseases from Behavioural Data:A Machine Learning Approach”, First IEEE Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE), 2016.   Sriraam Natarajan, Annu Prabhakar, Nandini Ramanan, Anna Bagilone, Katie Siek, and Kay Connelly, “Boosting for Post Partum Depression Prediction”, IEEE Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE), 2017.   Sriraam Natarajan, Srijita Das, Nandini Ramaman, Gautam Kunapuli and Predrag Radivojac, “Whom Should I Perform the Lab Test on Next? An Active Feature Elicitation Approach”, International Joint Conference on AI (IJCAI) 2018.  ","categories": ["precision-health"],
        "tags": [],
        "url": "/projects/survey-data/",
        "teaser": "/assets/images/project/survey_PPD.png"
      },{
        "title": "Scaling",
        "excerpt":"We develop methodologies for fast approximate counting for scalable Statistical Relational AI. In the age of ‘Big Data’, scalability is one of the key challenges both for learning and (lifted) inference in the context of Statistical Relational models. A major bottleneck towards scalable SRL is counting - computing the number of satisfied instances of complex features (# satisfied groundings of First-Order Logic formulas). It is a key operation in SRL (be it for computing the coverage in structure search, for likelihood estimation in parameter learning or for computing cluster size in lifted inference) and is hard, specifically #P-complete, which poses increasingly greater challenges with large scale databases. However, exact computation of  counts are inconsequential in context of large/dense data sets. (Example: There will not be any significant change in the popularity of a professor based on whether the papers (s)he has published has 400 or 419 citations) Thus, we develop performance-preserving approximation strategies for such counts, paving the way for scalable and efficient SRL.   Publications     Das, M., Wu, Y., Khot, T., Kersting, K. &amp; Natarajan, S., “Scaling Lifted Probabilistic Inference and Learning Via Graph Databases”, SIAM International Conference on Data Mining (SDM) 2016.  ","categories": ["efficient-starai"],
        "tags": [],
        "url": "/projects/scaling/",
        "teaser": "/assets/images/project/scaling.png"
      },{
        "title": "Lifted Continuous Inference",
        "excerpt":"Continuous values are common in the real world, but in traditional graphical model, inference with continuous variable may require computing integals of potential functions without a tractable closed form. Because of this, lots of works about inference in continuous domains limit the potential to some special exponential family, which cannot fully represent the complexity of the real world. Some other works focus on using sampling-based methods for inference. These methods are powerful in the domain with both continuous and discrete values (Hybrid Model), but they are too computation-expensive to be applied to real-word applications when a large number of variables are involved.   In our work, we are working to combine continuous inference with lifted inference to improve the efficiency so that it can be applied into real-word tasks such as early detection of Alzheimer’s Disease through doing inference on brain fMRI features.   Publications     Kristian Kersting, Babak Ahmadi, Sriraam Natarajan. “Counting Lifted Belief Propagation” , International Conference on Uncertainty in AI (UAI) 2009.  ","categories": ["efficient-starai"],
        "tags": [],
        "url": "/projects/lifted-continuous-inference/",
        "teaser": "/assets/images/project/lifted_continuous_inference.jpg"
      },{
        "title": "Lifted Deep Models",
        "excerpt":"Relational Connectionist Models bring together the complementary strengths of scalability and explainability. Moreover, considering relational aspect of data allows us to handle data in its natural form. In past, we have considered learning Boltzmann machines for relational data; specifically generating features from lifted random walks that form the observed features of Boltzmann machines and produce efficient models which was tested on six relational datasets.Presently we are proposing a new lifted connectionist model that learns with the help of parameter tying and combining rules. This work is ongoing. Further, as another project, we are proposing a graph neural network for relational data that can learn in the presence of minimal labels provided to it.   Publications     Navdeep Kaur, Gautam Kunapuli, Tushar Khot, William Cohen, Kristian Kersting and Sriraam Natarajan, “Relational Restricted Boltzmann Machines: A Probabilistic Logic Learning Approach”, ILP 2017.  ","categories": ["efficient-starai"],
        "tags": [],
        "url": "/projects/lifted-deep-models/",
        "teaser": "/assets/images/project/lifted_deep_model.png"
      },{
        "title": "Logistics",
        "excerpt":"Machine Learning is a natural choice in the Logistics domain for various problems like predicting the mode of movements of goods i.e. rail vs. road, predicting trailer types, etc. StARLinG Lab along with Turvo is looking to create an automated statistician which will embed the relations in a knowledge graph from shipments. We are also building a regressor model which would provide an approximate cost of the shipments and hence aid brokers to bargain a fair price.    source: www.turvo.com  ","categories": ["applications"],
        "tags": [],
        "url": "/projects/logistics/",
        "teaser": "/assets/images/project/logistics.jpeg"
      },{
        "title": "Knowledge-intensive Gradient Boosting",
        "excerpt":"    Tree based gradient boosting methods are very powerful and are successfully used in various challenging problems. But they fail in region where data is absent or when majority of data is noisy.    \t       In real-world problems, data is often sparse and noisy. So, can we leverage qualitative domain knowledge for regions where data is noisy or absent? Yes, our Knowledge-intensive gradient boosting approach provides a way to do that.   Given a sparse/noisy dataset, similar to the dataset shown in figure below, and some qualitative knowledge of the domain, KiGB learns boosted trees. The qualitative knowledge or trends of a domain can be expressed as monotonic influences. For the dataset below, monotonic influence $a_{ &lt;}^{Q+}y$ indicated higher values of  variable \\(a\\) stochastically results in higher values of variable $y$.                          Dataset with X-Y axis represent features $a$ and $b$ resp.   and color/shape represent the target variable $y$                               Monotonic influence of feature $a$ on target variable $y$                    In order restricted statistics, monotonic influence implies following constraint: $a_1 &lt; a_2 \\implies \\psi (a_1) \\leq \\psi (a_2)$, i.e. given $a_1$ is less than $a_2$ the value of $y=\\psi(a_1)$ should also be less than $\\psi(a_2)$   Altendorf et al. UAI, 2005 and Yang and Natarajan, ECML-PKDD, 2013 converts monotonic influences to constraints on probability distribution as:  $a_1 &lt; a_2 \\implies  P(Y\\leq k \\vert \\boldsymbol{pa_{y}^{a_1}}) \\geq P(Y\\leq k \\vert \\boldsymbol{pa_{y}^{a_2}})$, i.e. given $a_1$ is less than $a_2$ the probability of $y$ less than any constant $k$ should be greater for $y$ given $a_1$ then $y$ given $a_2$ ceteris paribus.   We use a similar constraint for a tree, when a node split at variable with monotonicity constraint, the expectation of the left subtree should be less than that of right subtree. (Assuming that the split criteria is $\\leq$)                 for node A,    $$ \\mathbb E_{\\psi}[\\boldsymbol n_L] \\leq \\mathbb E_{\\psi}[\\boldsymbol n_R]   $$  allowing some margin/slack for overlap,  $$ \\mathbb E_{\\psi}[\\boldsymbol n_L] \\leq \\mathbb E_{\\psi}[\\boldsymbol n_R] + \\varepsilon $$     we obtain following constraint, which we name $\\zeta$,   \\[\\mathbb E_{\\psi}[\\boldsymbol n_L] - \\mathbb E_{\\psi}[\\boldsymbol n_R] - \\varepsilon &lt; 0,  \\quad \\quad  \\quad  \\Bigg\\} \\zeta_n\\]  When $\\zeta$ is greater than $0$ the constraint is violated. We modify the objective to include a loss function with $\\zeta$,   \\[\\underset{\\psi_{t}}{\\operatorname{argmin}} \\underbrace{\\sum_{i=1}^{N}\\left(\\tilde{y}_{i}-\\psi_{t}\\left(x_{i}\\right)\\right)^{2}}_{\\text {loss function w.r.t. data }}+\\underbrace{\\frac{\\lambda}{2} \\sum_{\\mathbf{n} \\in \\mathcal{N}\\left(\\mathbf{x}_{c}\\right)} \\max \\left(\\zeta_{\\mathbf{n}} \\cdot\\left|\\zeta_{\\mathbf{n}}\\right|, 0\\right)}_{\\text {loss function w.r.t. advice }}\\]  Here, parameter $\\lambda$ determines the importance of advice. Taking gradient of modified objective, we get a nice leaf update equation,   \\[\\psi_{t}^{\\boldsymbol\\ell}(\\mathbf{x}) \\, = \\underbrace{\\frac{1}{|\\boldsymbol\\ell |} \\sum_{i = 1}^N \\, \\tilde{y}_i \\cdot \\mathbb{I}({x}_i \\in {\\boldsymbol\\ell})}_\\textrm{mean}  + \\\\   \\underbrace{\\frac{\\lambda}{2} \\underset{\\mathbf{n} \\in \\mathcal{N}(\\mathbf{x}_c)}{\\sum} \\mathbb{I}(\\zeta_\\mathbf{n} &gt; 0) \\zeta_\\mathbf{n} \\cdot \\Big( \\frac{\\mathbb{I}({\\boldsymbol\\ell} \\in \\mathbf{n}_\\mathsf{R})}{ |\\mathbf{n}_\\mathsf{R}|}  - \\frac{\\mathbb{I}({\\boldsymbol\\ell} \\in \\mathbf{n}_\\mathsf{L})}{|\\mathbf{n}_\\mathsf{L}|} \\Big)}_\\textrm{penalty for advice violation}\\]  Notice that the penalty is directly proportional to the violation and inversely proportional to the number of data points. Thus, there is a trade-off between advice and data. Equilibrium can be obtained by tuning the hyperparameters – $\\varepsilon$ and $\\lambda$.              For more details on the empirical evaluations, background on other monotonicity based approaches, or gradient derivation refer our AAAI 2020 paper.   Code and the usage guide can be found here.   Citation   If you build on this code or the ideas of this paper, please use the following citation.   @inproceedings{kokelaaai20,   author = {Harsha Kokel and Phillip Odom and Shuo Yang and Sriraam Natarajan},   title  = {A Unified Framework for Knowledge Intensive Gradient Boosting: Leveraging Human Experts for Noisy Sparse Domains},   booktitle = {AAAI},   year   = {2020} }   Acknowledgements      Harsha Kokel and Sriraam Natarajan acknowledge the support of Turvo Inc. and CwC Program Contract W911NF-15-1-0461 with the US Defense Advanced Research Projects Agency (DARPA) and the Army Research Office (ARO).  ","categories": ["HAAI"],
        "tags": [],
        "url": "/papers/KiGB/",
        "teaser": "/assets/images/project/kigb/kigb.png"
      },{
        "title": "RePReL",
        "excerpt":"    State abstraction is necessary for better task transfer in complex reinforcement learning environments. Inspired by the benefit of state abstraction in MAXQ and building upon hybrid planner-RL architectures, we propose RePReL, a hierarchical framework that leverages a relational planner to provide useful state abstractions.   Since the benefit of using the state abstractions is critical in relational settings, where the number and/or types of objects are not fixed apriori. We propose Dynamic-FOCI statements, an adaptation of first-order conditional influence (FOCI) statements, to specify the bisimilarity conditions of MDP. This helps us justify the safety of the abstractions. Additionally, since the RL agent learns to optimize policies to achieve subgoal with abstract state representations, we see effective transfer of learned skill from one task to another. In fact our experiments show that RePReL framework not only achieves better performance and efficient learning on the task at hand but also demonstrates better generalization to unseen tasks.   Motivational Example   In many real world domains, e.g., driving, the state space of offline planning is rather different from the state space of online execution. Planning typically occurs at the level of deciding the route, while online execution needs to take into account dynamic conditions such as locations of other cars and traffic lights. Indeed, the agent typically does not have access to the dynamic part of the state at the planning time, e.g., future locations of other cars, nor does it have the computational resources to plan an optimal policy in advance that works for all possible traffic events. We motivate this with a toy domain of an extended version of taxi domain where the goal is to transport multiple passengers ($p1, p2, p3, …$) to their respective destination location.   Here, the high level planning does not see the exact map of the domain, just plans for the passenger pick up and drop subgoals. While the lower level RL agent learns to navigate in the grid and accomplish these subgoals. In this setting, task specific abstraction can boost the sample efficiency tremendously. For e.g., the RL policy that is performing pickup $p1$ subgoal, needs to know the location of $p1$ and whether the taxi is free, while passenger $p2$ and destination of $p1$ are irrelevant.                     Abstract representations for Taxi domain with multiple passengers; high level planner doesn't see the whole map and lower level RL agent doesn't see other passenger locations       It has been argued that for human-level general intelligence, the ability to detect compositional structure in the domain (Lake, Salakhutdinov, and Tenenbaum 2015) and form task-specific abstractions (Konidaris 2019) are necessary. Our RePReL framework propose a step in that direction by providing the D-FOCI formulation which enables domain expert to specify the task specific abstract representation.   D-FOCI   Since states are conjunctions of literals in Relational MDPs, we need to reason about how the actions influence the state predicates and how rewards are influenced by goal predicates and actions to decide which literals should be included and excluded in the abstraction. We capture this knowledge using First-Order Conditional Influence (FOCI) statements (Natarajan et al. 2008), one of the many variants of statistical relational learning languages (Getoor and Taskar 2007; Raedt et al. 2016).   Each FOCI statement is of the form: “if $\\text{condition}$ then $X1$ $\\text{influence}$ $X2$”, where, $\\text{condition}$ and $X1$ are a set of first-order literals and $X2$ is a single literal. It encodes the information that literal $X2$ is influenced only by the literals in $X1$ when the stated condition is satisfied.   For RePReL, we simplify the syntax and extend FOCI to dynamic FOCI (D-FOCI) statements. In addition to direct influences in the same time step, D-FOCI statements also describe the direct influences of the literals in the current time step on the literals in the next time step. To distinguish the two kinds of influences, we show a $+1$ on the arrow between the sets of literals to capture a temporal interaction, as shown below.   \\[\\text { operator }:\\left\\{\\mathrm{p}\\left(X_{1}\\right), q\\left(X_{1}\\right)\\right\\} \\stackrel{+1}{\\longrightarrow} \\mathrm{q}\\left(X_{1}\\right)\\]  It says that, for the given $\\text{operator}$, the literal $q(X1)$ in the next time step is directly influenced only by the literals ${p(X1), q(X1)}$. Following the standard DBN representation of MDP, we allow action variables and the reward variables in the two sets of literals. To represent unconditional influences between state predicates, we skip the $\\text{operator}$.   These D-FOCI statements can be viewed as relational versions of dynamic Bayesian networks (DBNs) and have a similar function of capturing the conditional independence relationships between domain predicates at different time steps. Hence, Q-learning with these abstractions result in an optimal policy for round MDP when the MDP satisfies the D-FOCI statements with fixed depth unrolling.   Experiments   Our evaluations try to answer three specific questions,      Sample Efficiency: Do the abstractions induced by RePReL improve sample efficiency?   Transfer: Do abstractions allow effective transfer across tasks?   Generalization: Does RePReL efficiently generalize to varying number of objects?   All three are answered positively. Refer our ICAPS 2021 paper for more details on optimality guarantees, learning algorithm, evaluations, and results.   A sample implementation is available on our lab github here.   Citation   If you build on this code or the ideas of this paper, please use the following citation.   @inproceedings{KokelMNBT21,   title={RePReL: Integrating Relational Planning and Reinforcement Learning for Effective Abstraction},   author={Kokel, Harsha and Manoharan, Arjun and Natarajan, Sriraam and Balaraman, Ravindran and Tadepalli, Prasad},   booktitle={Thirty First International Conference on Automated Planning and Scheduling ({ICAPS})},   year={2021}.   url={https://ojs.aaai.org/index.php/ICAPS/article/view/16001}, }   Acknowledgements   HK &amp; SN gratefully acknowledge the support of ARO award W911NF2010224. SN acknowledges AFOSR award FA9550-18-1-0462. PT acknowledges support of DARPA contract N66001-17-2-4030 and NSF grant IIS-1619433. AM gratefully acknowledge the travel grant from RBCDSAI. Any opinions, findings, conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the ARO, AFOSR, NSF, DARPA or the US government.   ","categories": ["efficient-starai"],
        "tags": [],
        "url": "/papers/RePReL/",
        "teaser": "/assets/images/project/RePReL/example.png"
      },{
        "title": "A Probabilistic Approach to Extract Qualitative Knowledge for Early Prediction of Gestational Diabetes",
        "excerpt":"Quick Overview   Accepted at the 19th International Conference on Artificial Intelligence in Medicine (AIME 2021), and published in Springer Lecture Notes in Artificial Intelligence.   In this work: (1) we developed an algorithm to extract qualitative rules from a causal/probabilistic model, (2) we applied our algorithm to a data set for predicting gestational diabetes from clinical observations, and (3) we verified the rules with the prior knowledge of a clinical expert in gynecology (Dr. Haas)—finding a precision of 0.923 and suggesting some mismatched cases could be future research directions.       Qualitative Knowledge in Machine Learning   This work combines two project categories our group investigates: “Human-Allied Artificial Intelligence” and “Precision Health.”   Consider the following statements:      Risk of lung cancer increases with the number of cigarettes smoked   Risk of high blood glucose decreases with each additional hour of sleep, up to eight   Risk of gestational diabetes (GDM) increases with BMI   Each statement describes how a change in one variable influences the change in another variable. Such statements are a common result of medical studies, and may be highly influential in medical decision making. These are statements about monotonic influence (MI) between variables, denoted by $X_{\\prec}^{M+}Y$. Previous work asked domain experts to provide these statements up front, and empirical results show that this inductive bias makes it possible to learn good models even in settings with little data.1   Now consider these statements:      Increase in BMI increases the risk of high blood pressure in patients with family history of hypertension more than in patients without such family history.   Increase in blood sugar level increases the risk of heart attack in high cholesterol level patients more than it does in patients with low cholesterol levels.2   These are statements about synergistic influnece (SI) between sets of variables ${A,B}_{\\prec}^{S+}Y$.   Both types of knowledge have been successfully applied as inductive bias prior to learning. Here we are interested in a reverse problem: given a causal/probabilistic world model, can we reverse-engineer the same kind of qualitative knowledge that experts have? This has potential to be an important component for getting machines to reason and explain themselves in ways that are amenable to how humans understand the world.   Standard rule mining doesn’t handle this case   Decision Rules are close in spirit.3 Assuming data is ordinal, categorical, or binned from continuous attributes; a rule such as the following could be observed:       $\\text{if}~(X_{0} = 0 \\land X_{1} = 0);~\\text{then}$   $\\quad Y = 0$    Substituting human-readable names for the variables:       $\\text{if}~(\\text{Systolic Blood Pressure Category} = 0 \\land \\text{Diastolic Blood Pressure Category} = 0);~\\text{then}$   $\\quad \\text{Blood Pressure Category} = 0$    Then converting the rule to natural language:       $\\text{If a person’s systolic blood pressure is less than 120 and their}$ $\\text{diastolic blood pressure is less than 80, then they have normal blood pressure.}$    But how should one write a statement from earlier: “Risk of gestational diabetes (GDM) increases with BMI” as a conjunctive rule? It is not obvious. Expressing this statement requires expressing all possible values of the body mass index (BMI) variable, expressing the possible values of gestational diabetes, weighing the rules by the probability of each outcome, and finally asserting that a monotonic increase in the former implies a monotonic increase in the latter.4   The qualitative influence statement:   \\[\\text{BMI}_{\\prec}^{M+}\\text{GDM}\\]  is a much more general and concerns all possible values of both variables. This single rule concisely expresses an idea that it would take multiple decision rules.   Learning Qualitative Influence Statements: The QuaKE Algorithm   We develop metrics to measure the degree of monotonicity and synergy, denoted by $\\delta_{a}$ for degree of monotonic influence and $\\delta_{a,b}$ for degree of synergistic influence. Both have slack $\\epsilon$ hyperparameters to allow some violation of the constraints, and threshold $T$ hyperparameters to tune the degree that should be considered a true QI statement.      Input:   $\\quad$ joint probability model $P(Y, \\boldsymbol{X})$,   $\\quad$ label vector $Y$, ordinal feature vector $\\boldsymbol{X}$,   $\\quad$ monotonic and synergistic slack parameters: $\\epsilon_{m}$, $\\epsilon_{s}$   $\\quad$ monotonic and synergistic threshold parameters $T_{m}$, $T_{s}$.   Initialize: $\\boldsymbol{R} \\leftarrow \\emptyset$     for $a \\leftarrow 0$ to $(|\\boldsymbol{X}| - 1)$ do   $\\quad$ compute $\\delta_{a}$ using (Degree of monotonic influence calculation)   $\\quad$ if $\\delta_{a} \\geq T_{m}$ then   $\\qquad$ $\\boldsymbol{R} \\leftarrow$ ($X_{a\\prec}^{M+}Y$) $\\cup$ $\\boldsymbol{R}$   $\\quad$ for $b \\leftarrow a + 1$ to $(|\\boldsymbol{X}| - 1)$ do   $\\qquad$ compute $\\delta_{a,b}$ using (Degree of synergistic influence calculation)   $\\qquad$ if $\\delta_{a,b} \\geq T_{s}$ then   $\\qquad \\quad$ $\\boldsymbol{R}$ $\\leftarrow$ (${X_a,X_b}_{\\prec}^{S+}Y$) $\\cup$ $\\boldsymbol{R}$   return $\\boldsymbol{R}$    Degree of monotonic influence $\\delta_{a}$ of a variable $X_{a} \\in \\boldsymbol{X}$ on $Y$ is defined as:   \\[\\delta_{a} = I_{(C_a&gt;0)} \\cdot \\sum_j\\sum_{j'&gt;j}\\sum_k \\frac{P(Y \\leq k|X_a=x_a^j) - P(Y \\leq k|X_a=x_a^{j'})}{|X_a|}\\]  where,   \\[C_{a} = \\prod_{j}\\prod_{j'&gt;j}\\prod_{k}{\\max(P(Y \\leq k|X_a=x_a^j) - P(Y \\leq k|X_a=x_a^{j'}) + \\epsilon_m, 0)}\\]  Degree of synergistic influence is defined in a similar way:   \\[\\delta_{a,b} = I_{(C_{a,b}&gt;0)} \\cdot \\sum_i\\sum_{i'&gt;i}\\sum_j\\sum_{j'&gt;j} \\frac{\\phi_{a,b}^{i,i',j,j'}}{|X_a| \\cdot |X_b|}\\]  where,   \\[C_{a,b} = \\prod_i\\prod_{i'&gt;i}\\prod_j\\prod_{j'&gt;j} \\max(\\phi_{a,b}^{i,i',j,j'} + \\epsilon_s,0)\\]  and   \\[\\begin{split} \\phi_{a,b}^{i,i',j,j'} = \\sum_k P(Y \\leq k|X_a=x_a^i,X_b=x_b^j) &amp;- P(Y \\leq k|X_a=x_a^{i'},X_b=x_b^j)~- \\\\     P(Y \\leq k|X_a=x_a^{i}, X_b=x_b^{j'}) &amp;+ P(Y \\leq k|X_a=x_a^{i'}, X_b=x_b^{j'}) \\end{split}\\]  The reasoning for each of these are explained further in Section 2 of the paper, but briefly: the degree of monotonic influence measures the difference in probability of $Y$ for a variable $X_{a}$ across combinations of its values $x_{a}^{j}$ and $x_{a}^{j^{\\prime}}$. The synergistic calculations are similar, but take into account the context specific influences of one variable in the presence of another.   Application to Gestational Diabetes   We applied this to a domain where the goal was to predict gestational diabetes from clinical observations.            We extracted rules using our algorithm, and asked David M. Haas to do a similar labeling over the rules. QuaKE scored a precision of 0.923 (over five cross validation folds) compared to Dr. Haas. We interpret this result as showing good agreement with clinical knowledge in most cases, but also speculated that cases where they disagree could be interesting directions for future study.   Presentation     Citation   If you build on or use portions of this work, please provide credit using the following reference or BibTeX.      Athresh Karanam, Alexander L. Hayes, Harsha Kokel, David M. Haas, Predrag Radivojac, and Sriraam Natarajan. (2021) A Probabilistic Approach to Extract Qualitative Knowledge for Early Prediction of Gestational Diabetes. In: Artificial Intelligence in Medicine. AIME 2021.    @inproceedings{karanam2021quake,   author = {Athresh Karanam and Alexander L. Hayes and Harsha Kokel and David M. Haas and Predrag Radivojac and Sriraam Natarajan},   title = {A Probabilistic Approach to Extract Qualitative Knowledge for Early Prediction of Gestational Diabetes},   year = {2021},   booktitle = {Artificial Intelligence in Medicine} }   Acknowledgements   We gratefully acknowledge the support of 1R01HD101246 from NICHD and Precision Health Initiative of Indiana University. Thanks to Rashika Ramola and Rafael Guerrero for advice during the data processing phase and their helpful discussions and feedback.   Footnotes                  Altendorf et al. presented results of monotonic constraints that included the UCI Breast Cancer Wisconsin data set as an extreme case of this, scoring 90% accuracy with a single example and qualitative background knowledge in the form of monotonicities—this showed that an informed learner with a single example could be equivalent to an uninformed learner given hundreds of examples. For full details, see learning curves in Figure 8 of “Learning from Sparse Data by Exploiting Monotonicity Constraints.” &#8617;                  From Yang and Natarajan (2013) “Knowledge Intensive Learning: Combining Qualitative Constraints with Causal Independence for Parameter Learning in Probabilistic Models” &#8617;                  The discussion presented here is overly concise and ignores differences in model learning. Decision rules and decisions trees often strike a good balance between effectiveness and interpretability. For a longer discussion on learning and interpreting decision rules, Christoph Molnar’s summary can be a good place to look: Interpretable Machine Learning &gt; Interpretable Models &gt; Decision Rules. &#8617;                  We speculated that this could still be a viable option. Gopalakrishnan [2010] studied a related problem and proposed a “Bayesian Rule Extraction” algorithm. This learned the structure and parameters of a Bayesian Network (BN) then extracted rules by comparing confidence factors between binary outcomes conditioned on the same evidence. It may be possible to extract qualitative rules by running the Gopalakrishnan algorithm on learned BNs and checking whether the confidence factors increase (decrease) as the factors increase (decrease). For full details, refer to their paper on “Bayesian rule learning for biomedical data mining.” Alexander L. Hayes implemented a version of this, an overview and example of applying the method is included in the README of this GitHub repository. &#8617;           ","categories": ["precision-health"],
        "tags": [],
        "url": "/papers/QuaKE/",
        "teaser": "/assets/images/project/gdm_bn.svg"
      },{
        "title": "Human-guided Collaborative Problem Solving",
        "excerpt":"   What?   We consider the problem of human-machine collaborative problem solving as a planning task coupled with natural language communication. For this, we propose a task of collaboratively building target structures in a Minecraft environment. Here, two players, an architect (played by human) and a builder (machine), collaborate and communicate using natural language via chat interface.    \t   The architect (shown as a human icon above) has access to the target structure and can see the current state in the build region. The builder (Steve from Minecraft) can move in the build region and place/remove blocks. The builder does not have access to the target structure. The architect has to describe the structure to the builder, via chat interface.       Why?   Human-machine collaborative planning and problem solving is quite challenging as it requires shared perception of the world, sophisticated language understanding, fluent execution, bi-directional communication and contextual understanding.   For a successful target structure construction, the architect must decompose the target structure to smaller structures that builder knows how to construct. The builder must interpet the instruction in context of the current world and plan the sequence of actions. The conundrums posed by the our building task are    the communication between the architect and the builder is inherently bi-directional, as seen in the image above the builder should be able to seek clarifications as required both players must share some initial structures in the vocabulary and expand the vocabulary with experience.   Our task highlights the key challenges of the collaborative planning problem: bi-directional communication, contextual understanding, composable vocabulary and ability to induce new, rich concepts based on limited interaction and experience.       How?   Our framework consists of three main components that interact with the Minecraft Simulator. A natural language engine that parses the language utterances to a formal representation and vice-versa, a concept learner that induces generalized concepts for plans based on limited interactions with the user, and a planner that solves the task based on human interaction. More details on each of this component can be found in the paper.    \t       The following video demonstrates our framework.          Few more demonstration videos                                  Demonstration 2                                Demonstration 3                                  Demonstration 4                  Citation   If you find this work useful, please provide consider using the following reference or BibTeX.      Harsha Kokel, Mayukh Das, Rakibul Islam, Julia Bonn, Jon Cai, Soham Dan, Anjali Narayan-Chen, Prashant Jayannavar, Janardhan Rao Doppa, Julia Hockenmaier, Sriraam Natarajan, Martha Palmer, and Dan Roth. (2022) Lara – Human-guided collaborative problem solver: Effective integration of learning, reasoning and communication. In Tenth Annual Conference on Advances in Cognitive Systems 2022.       Harsha Kokel, Mayukh Das, Rakibul Islam, Julia Bonn, Jon Cai, Soham Dan, Anjali Narayan-Chen, Prashant Jayannavar, Janardhan Rao Doppa, Julia Hockenmaier, Sriraam Natarajan, Martha Palmer, and Dan Roth. (2021) Human-guided Collaborative Problem Solving: A Natural Language based Framework. In ICAPS 2021.    @inproceedings{KokelDIBCDNJDHNPR22,   author = {Harsha Kokel, Mayukh Das, Rakibul Islam, Julia Bonn, Jon Cai, Soham Dan, Anjali Narayan-Chen, Prashant Jayannavar, Janardhan Rao Doppa, Julia Hockenmaier, Sriraam Natarajan, Martha Palmer, Dan Roth},   title = {Lara -- Human-guided collaborative problem solver: Effective integration of learning, reasoning and communication},   year = {2022},   booktitle = {Tenth Annual Conference on Advances in Cognitive Systems ({ACS})} }  @inproceedings{KokelDIBCDNJDHNPR21,   author = {Harsha Kokel, Mayukh Das, Rakibul Islam, Julia Bonn, Jon Cai, Soham Dan, Anjali Narayan-Chen, Prashant Jayannavar, Janardhan Rao Doppa, Julia Hockenmaier, Sriraam Natarajan, Martha Palmer, Dan Roth},   title = {Human-guided Collaborative Problem Solving: A Natural Language based Framework},   year = {2021},   booktitle = {Thirty First International Conference on Automated Planning and Scheduling ({ICAPS})} }   Acknowledgements   We gratefully acknowledge the support of CwC Program Contract W911NF-15-1-0461 with the US Defense Advanced Research Projects Agency (DARPA) and the Army Research Office (ARO). Any opinions, findings and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, ARO or the US government.  ","categories": ["HAAI"],
        "tags": [],
        "url": "/papers/collaborative-ps/",
        "teaser": "/assets/gifs/cwc.gif"
      },{
        "title": "Hybrid Deep RePReL",
        "excerpt":"Fusion of high-level symbolic reasoning with lower level signal-based reasoning has become a paramount research question with the recent progress of deep learning. We recently  developed a  combination of a higher-order symbolic reasoner, a planner, with a signal-based RL system, called RePReL, to effectively construct abstractions to accelerate learning in  structured domains (with several interacting objects that cannot be efficiently represented using a fixed-length vector). RL in structured domains is inherently a difficult task and only a small number of solutions exist1. The RePReL system takes a first step in the direction of combining (relational) planning and RL in solving structured problems by using the planner to define a smaller set of (abstract) state-action spaces to allow for efficient learning by the lower level RL agent. The key success of the RePReL method lies in its capability of generalization to a varying number of objects.   RePReL   The RePReL architecture, shown below, consists of three stacked modules:  Symbolic Planner, Abstraction Reasoner, and RL agents.                     RePReL Architecture          Symbolic Planner: The symbolic planner uses the high-level planning domain description to decompose the goal into a sequence of temporally extended actions. Essentially, the planner decomposes the GRMDP into small sub-goal RMDPs.   Abstraction Reasoner:   The abstraction reasoner generates a task-specific abstract state representation using the dynamic first-order conditional influence statements provided by a domain expert.   RL Agents: Finally, multiple reinforcement learners at the lowest level learn separate RL policies for each option in the abstract state space.   While RePReL was successful, it had an important assumption—the underlying features are discrete and homogeneous. The discrete assumption restricts the RePReL from exploiting the power of deep RL and the homogeneous assumption restricts the use of RePReL in scenarios where the data could arrive from multiple sources.  In this work, we propose an extention to RePReL architecture that can easily adapt to hybrid data, i.e. a conglomeration of many varied types of data coming from different sources.   Hybrid Deep RePReL   In the Hybrid Deep RePReL architecture, shown below, we introduce two additional modules, an input preprocessing module and a merge module, to handle the combination of structured and unstructured information.                     Hybrid Deep RePReL Architecture       The unstructured part of the state space is passed through the input pre-processing module., that generates a latent state embeddings for the unstructured data. The input pre-processing module can be a Convolutional Neural Network for image data, a transformer for text data, or a combination of both.  The relevant state variables obtained from the abstraction reasoner and the latent predicates obtained from the input preprocessing modules are combined by the merge module and provided to the deep RL agents for learning.   Implementation of this work is available our lab github here.   Citation   If you build on this code or the ideas of this paper, please use the following citation.   @inproceedings{KokelPRBTN22,   title={Hybrid Deep RePReL: Integrating Relational Planning and Reinforcement Learning for Information Fusion},   author={Harsha Kokel and Nikhilesh Prabhakar and Balaraman Ravindran and Eric Blasch and Prasad Tadepalli and Sriraam Natarajan},   booktitle={IEEE 25th International Conference on Information Fusion (FUSION)},    year={2022} }   Acknowledgements   HK, NP, and SN gratefully acknowledge the support of ARO award W911NF2010224 and AFOSR award FA9550-18-1-0462. PT acknowledges support of DARPA contract N66001-17-2-4030 and NSF &amp; USDA-NIFA under grant 2021-67021-35344. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the AFOSR, ARO, NSF, DARPA or the U.S. government.   ","categories": ["efficient-starai"],
        "tags": [],
        "url": "/papers/HybridDeepRePReL/",
        "teaser": "/assets/images/project/RePReL/RePReL.png"
      },{
        "title": "Explaining Deep Tractable Probabilistic Models: The sum-product network case",
        "excerpt":"Quick Overview   Accepted at the 11th International Conference on Probabilistic Graphical Models (PGM)  (PGM 2022).   In this work:  (1) We developed CSI-Trees, a tree structured representation for context-specific independence relations,  (2) we developed an algorithm, $\\mathcal{EXSPN}$, to explain sum-product networks in terms of the context-specific independences encoded in them, (3) we adapted our algorithm to a data set for predicting gestational diabetes from clinical observations       Sum-Product Networks   Sum-Product Networks (SPN) are a class of deep probabilistic models. Their structure is represented by a Directed Acyclic Graph (DAG). The DAG consists of Sum ($+$) and Product ($\\times$) nodes as internal nodes, and univariate distributions at the leaf nodes. Sum nodes represent a mixture of distributions and Product nodes represent a factorized distribution. While SPNs guarantee tractable inference, the presence of latent variables due to the Sum nodes makes them hard to explain to domain experts.            For example, the SPN shown in the figure above represents the joint distribution $P(Write, Study, Pass)$ over three variables $Write$ ($W$), $Study$ ($S$) and $Pass$ ($P$).       $ P(W, S, P) = 0.49 P_1(W)P_2(S)P_3(P) + 0.51 P_4(W) (0.5 P_5(S)P_6(P)+ 0.5 P_7(S)P_8(P)) $    Where $P_1, \\dots, P_8$ are the univariate probability distributions at the leaf nodes, from left to right.   Context-specific Independence  Context-Specific Independence (CSI) is a generalization of the notion of Conditional Independence. For example, consider the following statement      Passing the exam is independent of Studying if I don’t write my answers.    Here, the independence relation holds only in a particular region of the sample space where $Write$ is False. Formally, $Pass \\perp\\kern-5pt\\perp Study \\mid (\\neg Write)$   ExSPN: Explaining Sum-Product Networks  In this work, we propose the $\\mathcal{EXSPN}$ algorithm to explain SPNs using the Context-Specific Independences encoded in them. We use the notion of a CSI to define a CSI-tree that represents a hierarchy of contexts. In a CSI-Tree, the edge labels contain conditions that define the context and the partitions in each node represent groups of independent variables. As shown in the figure below, $\\mathcal{EXSPN}$ converts a given SPN to a CSI-tree by empirically approximating the context defined by each Sum node, and reading off the independences at each Product node.            Application to Gestational Diabetes   We applied this to a domain where the goal was to predict gestational diabetes from clinical observations. The figure belows shows the first two levels of the CSI-tree extracted from  an SPN fit on the data.            Here, $oDM$ is a boolean variable that represents whether or not the person has Gestational Diabetes. $Race$ is a categorical variable having 8 categories namely, Non-Hispanic White (1), Non-Hispanic Black (2), Hispanic (3), American Indian (4), Asian (5), Native Hawaiian (6), Other (7), and Multiracial (8). $Smoked3Months$ and $SmokedEver$ are boolean variables representing tobacco consumption.   While the $BMI$ variable is independent of other variables when $oDM \\neq 1$, it is dependent on $Age, Race, Education$ for the case when $oDM = 1$.   Presentation at UAI 2022 TPM Workshop     Citation   If you build on or use portions of this work, please provide credit using the following reference or BibTeX.      Athresh Karanam, Saurabh Mathur, Predrag Radivojac, David M. Haas, Kristian Kersting, Sriraam Natarajan. (2022) Explaining Deep Tractable Probabilistic Models: The sum-product network case. In: International Conference on Probabilistic Graphical Models. PGM 2022    @inproceedings{karanam2022exspn,   author = {Athresh Karanam, Saurabh Mathur, Predrag Radivojac, David M. Haas, Kristian Kersting and Sriraam Natarajan},   title = {Explaining Deep Tractable Probabilistic Models: The sum-product network case},   year = {2022},   booktitle = {International Conference on Probabilistic Graphical Models} }   Acknowledgements   The authors acknowledge the support by the NIH grant R01HD101246, AFOSR award FA9550-18-1- 0462 and ARO award W911NF2010224. KK acknowledges the support of the Hessian Ministry of Higher Education, Research, Science and the Arts (HMWK) in Germany, project “The Third Wave of AI”. DH acknowledges the support from the Eunice Kennedy Shriver National Institute of Child Health and Human Development (NICHD): U10 HD063037, Indiana University  ","categories": ["precision-health"],
        "tags": [],
        "url": "/papers/ExSPN/",
        "teaser": "/assets/images/project/exspn/SPN_to_CSI-tree.png"
      },{
        "title": "Knowledge-based Learning",
        "excerpt":"Knowledge-rich approaches to  learning  and sequential decision-making try to alleviate the limitations of data-driven techniques caused by noise, stochasticity and asymmetry of knowledge. They leverage rich knowledge assimilated by domain experts through years of experience to learn better models. Recent years have witnessed a major research thrust in this direction and our group is an active contributor to this cause; focused on building knowledge-augmented learning frameworks for representation and elicitation of knowledge from, potentially multiple, expert(s) for optimal learning both structure and parameters of compact models from noisy structured data both in context of prediction/regression tasks as well as sequential decision-making (RL, planning, Imitation Learning etc.). Specifically, we address ‘asymmetry of knowledge’ where an AI agent may have access to certain resources and vast computational power but may lack the necessary knowledge to prioritize choices. Human experts understand such priorities implicitly and we leverage that to generate better models.   Our research includes, but is not limited to, knowledge-augmented Statistical Relational Learning, human guided decision-making (esp. in stochastic, partially observable, semi-structured environments), various modalities of human guidance and knowledge elicitation and, finally, successful application of such systems to real-world tasks such as health, biomedicine and finance. Recently we have also been focusing on knowledge-guided deep models to alleviate one of the major bottlenecks of deep learning (sub-optimal learning in presence of sparse/noisy data).   Publications     Mathur, S., Karanam, A., Radivojac, P., Haas, D.M., &amp; Natarajan, S., “Exploiting Domain Knowledge as Causal Independencies in Modeling Gestational Diabetes”, Pacific Symposium on Biocomputing (PSB) 2023.   Kokel, H., Odom, P., Yang, S., &amp; Natarajan, S., “Knowledge-intensive Gradient Boosting”, Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI) 2020.   Odom, P., Khot, T., Porter, R., &amp; Natarajan, S., “Knowledge-Based Probabilistic Logic Learning”, Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI) 2015.   Odom, P., Bangera, V., Khot, T., Page, D., &amp; Natarajan, S., “Extracting Adverse Drug Events from Text using Human Advice”, Artificial Intelligence in Medicine (AIME) 2015.   Yang, S., Khot, T., Kersting, K., Kunapuli, G., Hauser, K., &amp; Natarajan, S., “Learning from Imbalanced Data in Relational Domains: A Soft Margin Approach”, International Conference on Data Mining (ICDM) 2014.   Yang, S., &amp; Natarajan, S., “Knowledge Intensive Learning: Combining Qualitative Constraints with Causal Independence for Parameter Learning in Probabilistic Models”, European Conference on Machine Learning, (ECMLPKDD) 2013.  ","categories": ["HAAI"],
        "tags": [],
        "url": "/projects/knowledge-based-learning/",
        "teaser": "/assets/images/project/kbl2.jpg"
      },{
        "title": "Lifted/Relational Reinforcement Learning",
        "excerpt":"The high level overview of this project is to build relational models for sequential decision making. The main bottleneck of approximating utility functions in decision making is that it requires sampling large number of states to reasonably approximate the function. However, most real world domains have large state space and are relational in nature. Learning rich relational representations in such domains can help in capturing useful interactions between entities and similar states can be clustered together to learn lifted relational models which can be useful and efficient utility function approximators in large state space. As part of this project, we focus on efficient relational representation learning and relational sequential decision making for hybrid domains. We also wish to combine structured symbolic representations with deep models to create powerful, efficient and interpretable deep utility function approximators. We are also working on knowledge augmented Imitation learning where the goal is to learn the demonstrator’s noisy policy using advice from multiple experts. Our recent work focuses on using task-specific abstractions to combine planning and reinforcement learning.   Publications     Kokel, H., Manoharan, A., Natarajan, S., Ravindran, B., Tadepalli, P., “RePReL: Integrating Relational Planning and Reinforcement Learning for Effective Abstraction”, Thirty First International Conference on Automated Planning and Scheduling (ICAPS), 2021.   Kokel, H., Natarajan, S., Ravindran, B., Tadepalli, P., “Dynamic probabilistic logic models for effective task-specific abstractions in RL”, The 5th Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM), 2022.   Kokel, H., Prabhakar, N., Ravindran, B., Blasch, E., Tadepalli, P., Natarajan, S., “Integrating Relational Planning and Reinforcement Learning for Information Fusion”, IEEE 25th International Conference on Information Fusion (FUSION), 2022.  ","categories": ["efficient-starai"],
        "tags": [],
        "url": "/projects/rl/",
        "teaser": "/assets/images/project/Relational_RL.PNG"
      },{
        "title": "BoostSRL: \"Boosting for Statistical Relational Learning\"",
        "excerpt":"Developed by Jude Shavlik, Tushar Khot, Sriraam Natarajan, and members of the StARLinG Lab.       As with the standard gradient-boosting approach, our approach turns the model-learning problem to learning a sequence of regression models. The key difference to the standard approaches is that we learn relational regression models (i.e. regression models that operate on relational data). We assume the data to be in predicate-logic format and the output are essentially first-order regression trees where the inner nodes contain conjunctions of logical predicates.   Getting Started   Prerequisites:      Java (tested with openjdk 1.8.0_144)   Installation:      Download stable jar file and auc.jar for measuring performance.   Download stable source with git.  git clone -b master https://github.com/boost-starai/BoostSRL.git   Nightly builds with git.  git clone -b development https://github.com/boost-starai/BoostSRL.git   Basic Usage      BoostSRL assumes that data are contained in files with data structured in predicate-logic format.   Positive Examples:   father(harrypotter,jamespotter). father(ginnyweasley,arthurweasley). father(ronweasley,arthurweasley). ...   Negative Examples:   father(harrypotter,mollyweasley). father(harrypotter,lilypotter). father(harrypotter,ronweasley). ...   Facts:   male(harrypotter). male(jamespotter). siblingof(ronweasley,fredweasley). siblingof(ronweasley,georgeweasley). childof(jamespotter,harrypotter). childof(lilypotter,harrypotter). ...   Learning a Relational Dependency Network:   [~/BoostSRL/]$ java -jar v1-0.jar -l -train train/ -target father -trees 10   Inference with the Relational Dependency Network:   [~/BoostSRL/]$ java -jar v1-0.jar -i -model train/models/ -test test/ -target father -aucJarPath . -trees 10   Acknowledgements   We would like to thank our users, our supporters, and Professor Natarajan.   ","categories": [],
        "tags": [],
        "url": "/software/boostsrl/",
        "teaser": null
      },{
        "title": "rfgb.py",
        "excerpt":"       Installation   We recommend using environments for managing Python packages, refer to venv or Conda for more information.   Tagged versions of the rfgb library are hosted on the Python Package Index (PyPi). The associated  Documentation is on readthedocs.   pip install rfgb   Development versions are on GitHub. These may contain partially-implemented features. The  Latest Documentation is on readthedocs.   git clone https://github.com/starling-lab/rfgb.py.git python setup.py develop   Quick-Start without Installation      git clone https://github.com/starling-lab/rfgb.py   cd rfgb.py   Relational Dependency Network classification on a logistics domain:   $ python -m rfgb \\ \tlearn rdn \\ \t-target unload \\ \t-train testDomains/Logistics/train/ \\ \t-test testDomains/Logistics/test/      Versioning and Development   We use SemVer (Major.Minor.Patch) for versioning.   This project is currently in early Alpha: some features available in BoostSRL are missing. The commandline interface (CLI) and application programming interface (API) are not finalized yet, and are subject to change.   Interested in contributing? Refer to the  Contributing section of the documentation.   License   Copyright © 2017-2019 StARLinG Lab. This program comes with absolutely no warranty. This is free software, available under the terms of the GPL-3.0. You are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law.  ","categories": [],
        "tags": [],
        "url": "/software/rfgb.py/",
        "teaser": null
      },{
        "title": "rnlp",
        "excerpt":"Installation   We recommend using environments for managing Python packages, refer to venv or Conda for more information.   Stable versions of the rnlp library are hosted on the Python Package Index (PyPi)   pip install rnlp   Development versions are on GitHub. These may contain partially-implemented features.   git clone https://github.com/starling-lab/rnlp.git python setup.py develop   Quick-Start   rnlp can either be used as a commandline tool or an imported Python Package.   Commandline:   $ python -m rnlp -f files/doi.txt Reading corpus from file(s)... Creating background file... 100%|████████| 18/18 [00:00&lt;00:00, 38it/s]   Imoprted:   from rnlp.corpus import declaration import rnlp  doi = declaration() rnlp.converter(doi)   A Relational View of Text   Text will be converted into relational facts, built around the basic building blocks of Words, Sentences, and Blocks.   Words are individual units of text, such as the words you are currently reading. Sentences are a collection of Words. Blocks are a collection of Sentences.   This package encodes text in such a format so that relational learning methods (such as BoostSRL) can learn its structure.   Encoded Facts           Sentence’s Relative Position in Block:              earlySentenceInBlock: Sentence occurs within the first third of a block’s length.       midWaySentenceInBlock: Sentence occurs between the first and last third of a block’s length.       lateSentenceInBlock: Sentence occurs within the last third of a block’s length.                Word’s Relative Position in Sentence:              earlyWordInSentence: Word occurs within the first third of a sentence.       midWayWordInSentence: Word occurs between a third and two-thirds of a sentence.       lateWordInSentence: Word occurs within the last third of a sentence.                Relative Position Between Items:              nextWordInSentence: Pointer from a word to its neighbor.       nextSentenceInBlock: Pointer from a sentence to its neighbor.                Existential Semantics:              sentenceInBlock: Sentence occurs in a particular block.       wordInSentence: Word occurs in a particular sentence.                Low-Level Information about words:              wordString: A string representation of a word.       partOfSpeechTag: The word’s part of speech.           Example      A toy classification task where the goal is to predict whether a sentence contains the word “you”.   At the root of the tree, we see that [wordString(b, \"you\")] occurring is the best predictor. More interestingly, the model also shows that if both “a” and “b” occur early in the sentence, and “anon12035” is “Thank”, then it is also likely to be true.   The model was able to learn that the word “you” often occurs with the word “Thank” in the same sentence when “Thank” appears early in that sentence.  ","categories": [],
        "tags": [],
        "url": "/software/rnlp/",
        "teaser": null
      },{
        "title": "Walk-ER",
        "excerpt":" \t    Source code and TeX for “User Friendly Automatic Construction of Background Knowledge: Mode Construction from ER Diagrams.”                  Latest Release       License                                         Citation   If you build on this code or the ideas of this paper, please use the following citation.   @inproceedings{kcap2017ermodeconstruction,   author = {Alexander Hayes and Mayukh Das and Phillip Odom and Sriraam Natarajan},   title  = {User Friendly Automatic Construction of Background Knowledge: Mode Construction from ER Diagrams},   booktitle = {KCAP},   year   = {2017} }   Getting Started      Modes are used to restrict/guide the search space and are a powerful tool in getting relational algorithms such as BoostSRL to work. If your algorithm does not learn anything useful, then the first debug point would be the modes (in the background.txt file).    Walk-ER is a system for defining background knowledge for use in relational learning algorithms by exploring entity/attribute/relationships in Entity-Relational Diagrams. Refer to the BoostSRL Basic Modes Guide for more information about modes.   NOTE: This code no longer supports the GUI. If you are looking for GUI, refer the JA-Walk-ER   Prerequisites      Java 1.8   Python (2.7, 3.5)   Installation           Download the latest version from the GitHub repository (including five datasets):       $ git clone https://github.com/batflyer/Walk-ER.git           Basic Usage   WalkER can either be invoked from a terminal or imported as a Python package. Examples of both follow:           Interactive version:              Options overview (output of python walker.py -h):           usage: WalkER_rewrite.py [-h] [-v] [--number NUMBER] [-w | -s | -e | -r | -rw] diagram_file  positional arguments:   diagram_file  optional arguments:   -h, --help         show this help message and exit   -v, --verbose      Increase verbosity to help with debugging.   --number NUMBER    Select number of features to walk to (assumes that                      Important features are ordered from most important to                      least important). Defaults to number_attributes +                      number_relations if chosen number is greater than both.   -w, --walk         [Default] Walk graph from target to features.   -s, --shortest     Walk the graph from target to features. If there are                      multiple paths, take the shortest. If the shortest are                      equal lengths, walk both.   -e, --exhaustive   Walk graph from every feature to every feature.   -r, --random       Ignore features the user selected and walk (-w) from the                      target to random features.   -rw, --randomwalk  Walk a random path from the target until reaching a depth                      limit (specified with --number).                           Examples:                                   $ python walker.py -w diagrams/imdb.mayukh                                    $ python walker.py -rw --number 10 diagrams/imdb.mayukh                                            Via GUI  Instructions for using the GUI module for creation/re-use of ER-Diagrams  GUI module can be initiated by running a jar file.  Jar file name : ERD-GUI.jar        Usage : java -jar ERD-GUI.jar [ | [-w | -s | -e | -r | -rw] | [--number number] | [--verbose] | [-m] | [--erout outputDiagramfile]]  All options have implicit default values ensuring that the GUI runs smoothly even when none of the options are specified   Optional arguments:    -m\t\t\t\t Calls the python mode construction module automatically                       after ER-Diagram is parsed.       The rest of the arguments are only required if \"-m\" is used.     --erout \t\t \t Indicator for specifying file name for parsed output                       (that the mode constructor will use) of ER-Diagram.    outputDiagramfile\t File name for the parsed output of ERD. This will be                       used as \"diagram_file\" argument when calling python                       mode construction module.    --verbose      \t Increase verbosity to help with debugging (for mode construction).    --number NUMBER    Select number of features to walk to (assumes that                       Important features are ordered from most important to                       least important). Defaults to number_attributes +                       number_relations if chosen number is greater than both.    -w, --walk         [Default] Walk graph from target to features.    -s, --shortest     Walk the graph from target to features. If there are                       multiple paths, take the shortest. If the shortest are                       equal lengths, walk both.    -e, --exhaustive   Walk graph from every feature to every feature.    -r, --random       Ignore features the user selected and walk (-w) from the                       target to random features.    -rw, --randomwalk  Walk a random path from the target until reaching a depth                       limit (specified with --number).                 Examples:                    For running with no arguments: java -jar ERD-GUI.jar           For running with arguments: java -jar ERD-GUI.jar -s --number 2 -m --erout ProfStudentCourse.out   (file extention of the output file is not important, can be anything or none)                           Once the jar file is executed, it will initiate a GUI window which allows one to create/save/upload and finally parse an ER-Diagram.  For a video tutorial on how to use the GUI please click  here .       Acknowledgements      Mayukh Das and Sriraam Natarajan gratefully acknowledge the support of the CwC Program Contract W911NF-15-1-0461 with the US Defense Advanced Research Projects Agency (DARPA) and the Army Research Office (ARO).   Phillip Odom and Sriraam Natarajan acknowledge the support of the Army Research Office (ARO) grant number W911NF-13-1-0432 under the Young Investigator Program.   Icon in the logo is “Trail” by Martina Krasnayová from the Noun Project, used under a Creative Commons (CC) Attribution 3.0 United States License.   ","categories": [],
        "tags": [],
        "url": "/software/walker/",
        "teaser": null
      },{
        "title": "BoostSRL: Python Wrappers",
        "excerpt":" \t                   License       Build Status       Codecov                                                  boostsrl   boostsrl is a Python wrapper for BoostSRL to help automate learning and inference tasks.   Getting Started   Prerequisites      Java 1.8   Python (2.7, 3.3, 3.4, 3.5, 3.6)   subprocess32 (if using Python 2.7: pip install subprocess32)   graphviz-0.8   Installation           The latest stable build can be installed with pip:       $ pip install git+git://github.com/batflyer/boostsrl-python-package.git           Basic Usage   &gt;&gt;&gt; from boostsrl import boostsrl  '''Step 1: Background Knowledge'''  # Sample data is built in from the 'Toy Cancer' Dataset, retrieve it with example_data &gt;&gt;&gt; bk = boostsrl.example_data('background')  # Create the background knowledge or 'Modes,' where 'cancer' is the target we want to predict. &gt;&gt;&gt; background = boostsrl.modes(bk, ['cancer'], useStdLogicVariables=True, treeDepth=4, nodeSize=2, numOfClauses=8)  '''Step 2: Training a Model'''  # Retrieve the positives, negatives, and facts. &gt;&gt;&gt; train_pos = boostsrl.example_data('train_pos') &gt;&gt;&gt; train_neg = boostsrl.example_data('train_neg') &gt;&gt;&gt; train_facts = boostsrl.example_data('train_facts')  # Train a model using this data: &gt;&gt;&gt; model = boostsrl.train(background, train_pos, train_neg, train_facts)  # How many seconds did training take? &gt;&gt;&gt; model.traintime() 0.705  '''Step 3: Test Model on New Data'''  # Retrieve the positives, negatives, and facts. &gt;&gt;&gt; test_pos = boostsrl.example_data('test_pos') &gt;&gt;&gt; test_neg = boostsrl.example_data('test_neg') &gt;&gt;&gt; test_facts = boostsrl.example_data('test_facts')  # Test the data &gt;&gt;&gt; results = boostsrl.test(model, test_pos, test_neg, test_facts)  '''Step 4: Observe Performance'''  # To see the overall performance of the model on test data: &gt;&gt;&gt; results.summarize_results() {'CLL': '-0.223184', 'F1': '1.000000', 'Recall': '1.000000', 'Precision': '1.000000,0.500', 'AUC ROC': '1.000000', 'AUC PR': '1.000000'}  # To see probabilities for individual test examples: &gt;&gt;&gt; results.inference_results('cancer') {'!cancer(Watson)': 0.6924179024024251, 'cancer(Xena)': 0.8807961917687174, '!cancer(Voldemort)': 0.6924179024024251, 'cancer(Yoda)': 0.8807961917687174, 'cancer(Zod)': 0.8807961917687174}    Contributing   Please refer to CONTRIBUTING.md for documentation on submitting issues and pull requests.   Versioning   We use SemVer for versioning. See Releases for all stable versions that are available.   Acknowledgements      Professor Sriraam Natarajan   Members of StARLinG Lab   ","categories": [],
        "tags": [],
        "url": "/software/boostsrl-python-wrappers/",
        "teaser": null
      },{
        "title": "JA-Walk-ER",
        "excerpt":" \t        This is Just-another implementation of Walk-ER. Because of dependencies which need license, the original code no longer supports a GUI. This code is built on top of the original code to provide a GUI.   The original source code and TeX for “User Friendly Automatic Construction of Background Knowledge: Mode Construction from ER Diagrams.” can be found here.   Citation   If you build on this code or the ideas of this paper, please use the following citation.   @inproceedings{kcap2017ermodeconstruction,   author = {Alexander Hayes and Mayukh Das and Phillip Odom and Sriraam Natarajan},   title  = {User Friendly Automatic Construction of Background Knowledge: Mode Construction from ER Diagrams},   booktitle = {KCAP},   year   = {2017} }   Acknowledgements      Mayukh Das and Sriraam Natarajan gratefully acknowledge the support of the CwC Program Contract W911NF-15-1-0461 with the US Defense Advanced Research Projects Agency (DARPA) and the Army Research Office (ARO).   Phillip Odom and Sriraam Natarajan acknowledge the support of the Army Research Office (ARO) grant number W911NF-13-1-0432 under the Young Investigator Program.   Icon in the logo is “Trail” by Martina Krasnayová from the Noun Project, used under a Creative Commons (CC) Attribution 3.0 United States License.   Getting Started   Clone a copy of the repository from here and refer the Docs.   ","categories": [],
        "tags": [],
        "url": "/software/jawalker/",
        "teaser": null
      },{
        "title": "KiGB",
        "excerpt":"KiGB is a unified framework for learning gradient boosted decision trees for regression and classification tasks while leveraging human advice for achieving better performance   This package contains two implementation of Knowledge-intensive Gradient Boosting framework:     with Gradient Boosted Decision Tree of Scikit-learn ( SKiGB )   with Gradient Boosted Decision Tree of LightGBM ( LKiGB )   Both these implementations are done in python.   Basic Usage   '''Step 1: Import the class''' from core.lgbm.lkigb import LKiGB as KiGB  '''Step 2: Import dataset''' train_data = pd.read_csv('train.csv') X_train = train_data.drop('target', axis=1) Y_train = train_data['target']  '''Step 3: Provide monotonic influence information''' advice  = np.array([1,0,1,1-1], dtype=int) # 0 for features with no influence, +1 for features with isotonic influence, -1 for antitonic influences  '''Step 4: Train the model''' kigb = KiGB(lamda=1, epsilon=0.1, advice=advice, objective='regression', trees=30) kigb.fit(X_train, y_train)  '''Step 5: Test the model''' kigb.predict(X_test)   To use Scikit version of KiGB, import from core.scikit.skigb import SKiGB   Citation   If you build on this code or the ideas of this paper, please use the following citation.   @inproceedings{kokelaaai20,   author = {Harsha Kokel and Phillip Odom and Shuo Yang and Sriraam Natarajan},   title  = {A Unified Framework for Knowledge Intensive Gradient Boosting: Leveraging Human Experts for Noisy Sparse Domains},   booktitle = {AAAI},   year   = {2020} }   Acknowledgements      Harsha Kokel and Sriraam Natarajan acknowledge the support of Turvo Inc. and CwC Program Contract W911NF-15-1-0461 with the US Defense Advanced Research Projects Agency (DARPA) and the Army Research Office (ARO).  ","categories": [],
        "tags": [],
        "url": "/software/KiGB/",
        "teaser": null
      }]
