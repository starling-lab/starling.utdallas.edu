<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>RePReL - StARLinG</title>
<meta name="description" content="Integrating Relational Planning and Reinforcement Learning for Effective Abstraction, by Harsha Kokel, Arjun Manoharan, Sriraam Natarajan, Balaraman Ravindran, Prasad Tadepalli, In ICAPS 2021">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="StARLinG">
<meta property="og:title" content="RePReL">
<meta property="og:url" content="https://starling.utdallas.edu/papers/RePReL/">


  <meta property="og:description" content="Integrating Relational Planning and Reinforcement Learning for Effective Abstraction, by Harsha Kokel, Arjun Manoharan, Sriraam Natarajan, Balaraman Ravindran, Prasad Tadepalli, In ICAPS 2021">



  <meta property="og:image" content="https://starling.utdallas.edu/assets/images/project/RePReL/example.png">



  <meta name="twitter:site" content="@StARLing_lab">
  <meta name="twitter:title" content="RePReL">
  <meta name="twitter:description" content="Integrating Relational Planning and Reinforcement Learning for Effective Abstraction, by Harsha Kokel, Arjun Manoharan, Sriraam Natarajan, Balaraman Ravindran, Prasad Tadepalli, In ICAPS 2021">
  <meta name="twitter:url" content="https://starling.utdallas.edu/papers/RePReL/">

  
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="https://starling.utdallas.edu/assets/images/project/RePReL/example.png">
  

  



  <meta property="article:published_time" content="2021-02-23T00:00:00-06:00">





  

  


<link rel="canonical" href="https://starling.utdallas.edu/papers/RePReL/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "StARLinG Lab",
      "url": "https://starling.utdallas.edu/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="StARLinG Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <link rel="shortcut icon" href="/assets/images/favicon.ico">

  </head>

  <body class="layout--splash narrow">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="https://starling.utdallas.edu/"><img style="max-height: 40px;" src="/assets/images/Logo.png"/></a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="https://starling.utdallas.edu/people/" >People</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="https://starling.utdallas.edu/publications/" >Publications</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="https://starling.utdallas.edu/projects/" >Projects</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="https://starling.utdallas.edu/software/" >Software</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="https://starling.utdallas.edu/datasets/" >Datasets</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="https://starling.utdallas.edu/gallery/" >Gallery</a>
            </li>
          
        </ul>
        
        <button class="search__toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero--overlay"
  style="background-color: SteelBlue; background-image: url('');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          RePReL

        
      </h1>
      
        <p class="page__lead">Integrating Relational Planning and Reinforcement Learning for Effective Abstraction <br /><i>Harsha Kokel, Arjun Manoharan, Sriraam Natarajan, Balaraman Ravindran, Prasad Tadepalli</i><br /><br />  <a href="/assets/pdfs/Kokel_ICAPS2021.pdf" class="btn btn--light-outline btn--large"><i class="fas fa-file-pdf"></i> Paper</a>  <a href="/assets/pdfs/Kokel_ICAPS21_sup.pdf" class="btn btn--light-outline btn--large"><i class="fas fa-paperclip"></i> Appendix</a>  <a href="https://github.com/starling-lab/RePReL" class="btn btn--light-outline btn--large"><i class="fas fa-code"></i> Code</a>   <a href="https://youtu.be/xNTdksqUQCM" target="_blank" class="btn btn--light-outline btn--large"><i class="fab fa-youtube"></i> Video</a>
</p>
      
      


      
      
    </div>
  
  
</div>



<div id="main" role="main">
  <article class="splash" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="RePReL">
    <meta itemprop="description" content="Integrating Relational Planning and Reinforcement Learning for Effective Abstraction Harsha Kokel, Arjun Manoharan, Sriraam Natarajan, Balaraman Ravindran, Prasad Tadepalli   Paper   Appendix   Code    Video">
    <meta itemprop="datePublished" content="2021-02-23T00:00:00-06:00">
    

    <section class="page__content" itemprop="text">
      <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]},
    extensions: [
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    jax: ["input/TeX", "output/CommonHTML"],
    TeX: {
      extensions: [
        "AMSmath.js",
        "AMSsymbols.js",
        "noErrors.js",
        "noUndefined.js",
      ]
    }
  });
</script>

<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>State abstraction is necessary for better task transfer in complex reinforcement learning environments. Inspired by the benefit of state abstraction in MAXQ and building upon hybrid planner-RL architectures, we propose RePReL, a hierarchical framework that leverages a relational planner to provide useful state abstractions.</p>

<p>Since the benefit of using the state abstractions is critical in relational settings, where the number and/or types of objects are not fixed apriori. We propose Dynamic-FOCI statements, an adaptation of first-order conditional influence (FOCI) statements, to specify the bisimilarity conditions of MDP. This helps us justify the safety of the abstractions. Additionally, since the RL agent learns to optimize policies to achieve subgoal with abstract state representations, we see effective transfer of learned skill from one task to another. In fact our experiments show that RePReL framework not only achieves better performance and efficient learning on the task at hand but also demonstrates better generalization to unseen tasks.</p>

<h3 id="motivational-example">Motivational Example</h3>

<p>In many real world domains, e.g., driving, the state space of offline planning is rather different from the state space of online execution. Planning typically occurs at the level of deciding the route, while online execution needs to take into account dynamic conditions such as locations of other cars and traffic lights. Indeed, the agent typically does not have access to the dynamic part of the state at the planning time, e.g., future locations of other cars, nor does it have the computational resources to plan an optimal policy in advance that works for all possible traffic events. We motivate this with a toy domain of an extended version of taxi domain where the goal is to transport multiple passengers ($p1, p2, p3, …$) to their respective destination location.</p>

<p>Here, the high level planning does not see the exact map of the domain, just plans for the passenger pick up and drop subgoals. While the lower level RL agent learns to navigate in the grid and accomplish these subgoals. In this setting, task specific abstraction can boost the sample efficiency tremendously. For e.g., the RL policy that is performing pickup $p1$ subgoal, needs to know the location of $p1$ and whether the taxi is free, while passenger $p2$ and destination of $p1$ are irrelevant.</p>

<div align="center">
        <img src="/assets/images/project/RePReL/example.png" width="300" />
        <p style="text-align:center;">Abstract representations for Taxi domain with multiple passengers; high level planner doesn't see the whole map and lower level RL agent doesn't see other passenger locations</p>
</div>
<p><br /></p>

<p>It has been argued that for human-level general intelligence, the ability to detect compositional structure in the domain (Lake, Salakhutdinov, and Tenenbaum 2015) and form task-specific abstractions (Konidaris 2019) are necessary. Our RePReL framework propose a step in that direction by providing the D-FOCI formulation which enables domain expert to specify the task specific abstract representation.</p>

<h3 id="d-foci">D-FOCI</h3>

<p>Since states are conjunctions of literals in Relational MDPs, we need to reason about how the actions influence the state predicates and how rewards are influenced by goal predicates and actions to decide which literals should be included and excluded in the abstraction. We capture this knowledge using First-Order Conditional Influence (FOCI) statements (Natarajan et al. 2008), one of the many variants of statistical relational learning languages (Getoor and Taskar 2007; Raedt et al. 2016).</p>

<p>Each FOCI statement is of the form: “if $\text{condition}$ then $X1$ $\text{influence}$ $X2$”, where, $\text{condition}$ and $X1$ are a set of first-order literals and $X2$ is a single literal. It encodes the information that literal $X2$ is influenced only by the literals in $X1$ when the stated condition is satisfied.</p>

<p>For RePReL, we simplify the syntax and extend FOCI to dynamic FOCI (D-FOCI) statements. In addition to direct influences in the same time step, D-FOCI statements also describe the direct influences of the literals in the current time step on the literals in the next time step. To distinguish the two kinds of influences, we show a $+1$ on the arrow between the sets of literals to capture a temporal interaction, as shown below.</p>

\[\text { operator }:\left\{\mathrm{p}\left(X_{1}\right), q\left(X_{1}\right)\right\} \stackrel{+1}{\longrightarrow} \mathrm{q}\left(X_{1}\right)\]

<p>It says that, for the given $\text{operator}$, the literal $q(X1)$ in the next time step is directly influenced only by the literals ${p(X1), q(X1)}$. Following the standard DBN representation of MDP, we allow action variables and the reward variables in the two sets of literals. To represent unconditional influences between state predicates, we skip the $\text{operator}$.</p>

<p>These D-FOCI statements can be viewed as relational versions of dynamic Bayesian networks (DBNs) and have a similar function of capturing the conditional independence relationships between domain predicates at different time steps. Hence, <em>Q-learning with these abstractions result in an optimal policy for round MDP when the MDP satisfies the D-FOCI statements with fixed depth unrolling</em>.</p>

<h3 id="experiments">Experiments</h3>

<p>Our evaluations try to answer three specific questions,</p>

<ol>
  <li><strong>Sample Efficiency:</strong> Do the abstractions induced by RePReL improve sample efficiency?</li>
  <li><strong>Transfer:</strong> Do abstractions allow effective transfer across tasks?</li>
  <li><strong>Generalization:</strong> Does RePReL efficiently generalize to varying number of objects?</li>
</ol>

<p>All three are answered positively. Refer our ICAPS 2021 <a href="/assets/pdfs/Kokel_ICAPS2021.pdf">paper</a> for more details on optimality guarantees, learning algorithm, evaluations, and results.</p>

<p>A sample implementation is available on our lab github <a href="https://github.com/starling-lab/RePReL">here</a>.</p>

<h2 id="citation">Citation</h2>

<p>If you build on this code or the ideas of this paper, please use the following citation.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{KokelMNBT21,
  title={RePReL: Integrating Relational Planning and Reinforcement Learning for Effective Abstraction},
  author={Kokel, Harsha and Manoharan, Arjun and Natarajan, Sriraam and Balaraman, Ravindran and Tadepalli, Prasad},
  booktitle={Thirty First International Conference on Automated Planning and Scheduling ({ICAPS})},
  year={2021}.
  url={https://ojs.aaai.org/index.php/ICAPS/article/view/16001},
}
</code></pre></div></div>

<h3 id="acknowledgements">Acknowledgements</h3>

<p>HK &amp; SN gratefully acknowledge the support of ARO award W911NF2010224. SN acknowledges AFOSR award FA9550-18-1-0462. PT acknowledges support of DARPA contract N66001-17-2-4030 and NSF grant IIS-1619433. AM gratefully acknowledge the travel grant from RBCDSAI. Any opinions, findings, conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the ARO, AFOSR, NSF, DARPA or the US government.</p>


    </section>
  </article>
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://github.com/starling-lab" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github-square" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://twitter.com/starling_lab" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://www.youtube.com/channel/UCtplIvykXStML8nLMak_02A" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-youtube-square" aria-hidden="true"></i> YouTube</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 StARLinG Lab. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <script>
  window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
  ga('create','UA-99296192-3','auto');
  ga('set', 'anonymizeIp', false);
  ga('send','pageview')
</script>
<script src="https://www.google-analytics.com/analytics.js" async></script>









  </body>
</html>
